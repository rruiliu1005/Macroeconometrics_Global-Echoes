---
title: "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies"
author: "Rui Liu"

execute:
  echo: false
#  eval: false
bibliography: references.bib
---

# 1. The Question Objective, and Motivation
**Objective:**
Construct a Bayesian Vector Autoregression model to forecast major macroeconomic indicators for the United States, Australia, Japan, and China to facilitate an investigation into the prospective interdependencies between the economies of these nations.

**Question:**
This research project will examine how trade relationships, investment flows, monetary policy environments, and economic performances within the United States, Australia, Japan and China mutually influence each other, and assess the implications of these interactions for predicting future values of these economic indicators.

**Motivation:**
Since the onset of the COVID-19 pandemic, the global economic landscape has witnessed a
series of unprecedented shifts in key macroeconomic indicators, spurred by governments'
adoption of varied expansionary monetary policies. Initially, to buffer their economies,
many nations implemented expansive monetary strategies, later swiftly transitioning to
interest rate hikes in a bid to manage surging inflation ratesâ€”a scenario not seen in
decades. The pandemic's disruption to trade further exacerbated inflationary pressures for
some economies, highlighting the intricate interdependencies among major economies
with significant trade and financial ties. This period recorded stark contrast in inflation
levels, with unprecedented highs in the US and Australia and notably low inflation in China
and Japan. Amidst this turmoil, a divergence in economic paths also became apparent, the
United States and Australia have witness robust economic rebounds, whereas China and
Japan saw more tepid recoveries. This research aims to dissect the nuanced web of
economic interdependencies between the United States, Australia, Japan, and China,
analyzing how their trade relationships, investment flows, and monetary policy
environments have mutually influenced their economic performances. Additionally, it
seeks to understand the ramifications of these dynamics for the predictive accuracy of
future economic indicators, offering insights into the evolving global economic order.

# 2. Data and their properties

**Proposed Dataset:**
This research project will utilize data from the International Monetary Fundâ€™s (IMF) extensive database, which offers a comprehensive collection of global economic information. The IMFâ€™s collection includes several key databases such as the World Economic Outlook Databases, International Financial Statistics (IFS), Government Finance Statistics. The analysis will predominantly focus on the IFS database, which encompasses an sizeable collection of financial and economic data from across the global, featuring 1,681 distinct indicators such as consumer price index, interest rates, exchange rates, national accounts, government finance statistics. The data is available in various frequencies â€“ annual, semi-annual, quarterly, monthly, daily, and weekly. As this research is primarily
focused on analyzing macroeconomic data that are published on a monthly or quarterly basis, quarterly data from Q1 2011 to Q3 2023 will be used. The analysis will examine key macroeconomic variables including consumer price indexes, foreign direct investments, exchange rates, balance of payments, interest rates and the national gross domestic product of the United States, Australia, Japan, and China.

**Variables and Motivation:**

| Variables                                              | Original Unit          | Final Unit | Mnemonic | Code           |
|--------------------------------------------------------|------------------------|------------|----------|----------------|
| Prices, Consumer Price Index, All items, Previous period | % Change               | % Change   | CPI      | PCPI_PC_PP_PT  |
| Exchange Rates, Domestic Currency per U.S. Dollar, Period Average | US dollar, millions           | US dollar, millions  | XCH      | ENDA_XDC_USD_R |
| Gross Domestic Product, Nominal, Seasonally Adjusted | Domestic Currency    |  log(US dollar)  | GDP      | NGDP_NSA_XDC   |

The variables included in this study were chosen with the objective to include key economic indicators that are susceptible to changes in other nations while also having experienced significant fluctuations over the past decade. These variables were chosen not only for their ability to provide insights into the trade relations, investment dynamics and monetary policy frameworks, but also for their roles as barometers of overall economic health and performance. Foreign direct investment (FDI) is a direct indicator of cross- border investment flows and serves as a proxy for economic confidence and integration between nations. Exchange rates directly impact trade balances and investment flows, influencing economic performances. By examining the volatility and trends in exchange rates, insights can be gleaned into how monetary policies and economic conditions in one country can affect its trade partners. Balance of payments is a comprehensive measure that captures the transactions between a country and the rest of the world, offering a holistic view of its economic interactions. For example, some view changes in balance of payments as largely a result of imports and exports, which can cause one country to import the inflation of another country and vice versa. GDP growth is the ultimate measure of economic performance, encapsulating the outcome of various economic activities and policies. Analyzing GDP in the form of percentage change allows for assessing economic momentum and comparing growth rates across countries and over time, offering a clear picture of economic health and trends.
Foreign direct investment, exchange rates, balance of payments and gross domestic products were transformed into percentage change from the previous period, aiming to standardize the data, facilitate temporal comparisons and enhancing the interpretability of trends over time. The presence of cyclical trends in the variables, alongside the observed impact of lagged values on future outcomes, highlights the suitability of the Bayesian Vector Autoregression model for our analysis. This model can well capture the temporal dynamics and interdependencies inherent in these economic indicators, offering a robust framework for understanding the nuanced interactions and feedback loops that characterize their behavior over time.

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
library(IMFData)
library(patchwork)
library(zoo)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tseries)
library(knitr)
library(MASS)
library(ggplot2)
library(reshape2)
library(progress)
library(psych)
library(truncnorm)
library(extraDistr)
library(forecast)
library(HDInterval)
library(purrr)
library(MCMCpack)
library(Rmpfr)
library(igraph)
```

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
#################################################################################
#################################################################################
#Download Data 
#################################################################################
#################################################################################
#devtools::install_github('mingjerli/IMFData')
# https://github.com/mingjerli/IMFData
#find out available dataset in IMF data
availableDB <- DataflowMethod()
#availableDB$DatabaseID

#Findout how many dimensions are available in a given dataset. 
# Available dimension code
IFS.available.codes <- DataStructureMethod("IFS")
#names(IFS.available.codes)
# Possible code in the first dimension
#IFS.available.codes[[1]]
IFS.data.category <-  IFS.available.codes[[3]]
#Search possible code to use in each dimension. 
#Here, we want to search code related to GDP in CL_INDICATOR_IFS dimension
#CodeSearch(IFS.available.codes, "CL_INDICATOR_IFS", "CPI")

#Make API call to get data
databaseID <- "IFS"
startdate = "1950-01-01"
enddate = "2024-05-01"
checkquery = FALSE

##################################################################
##################################################################
#Interest rate data 
##################################################################
##################################################################
#China
##################################################################
# queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'FIDR_PA') #Financial, Interest Rates, Deposit, Percent per annum
# CN.IR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
#                                   checkquery)
##################################################################
#US
##################################################################
#queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'FICPR_PA') #
#queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'FID_PA') #
#queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'FIGBY_SM_PA') #
#queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'FIGB_PA') #
#queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'FITB_3M_PA') #
#queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'FITB_PA') 
#queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'FIH_HH_L_CC_1Y_PA') 
#queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'FILR_PA') 
# Sys.sleep(0.5)
# queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'FPOLM_PA') #Financial, Interest Rates, Monetary Policy-Related Interest Rate, Percent per annum
# US.IR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate,  
#                                  checkquery)

##################################################################
#AU
##################################################################
# Sys.sleep(0.5)
# queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'FPOLM_PA') #Financial, Interest Rates, Monetary Policy-Related Interest Rate, Percent per annum
# AU.IR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate,  
#                                  checkquery)
##################################################################
#J{} #starts 2006 Q2
##################################################################
# Sys.sleep(0.5)
# queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'FPOLM_PA') #Financial, Interest Rates, Monetary Policy-Related Interest Rate, Percent per annum
# JP.IR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate,  
#                                  checkquery)

#International Investment Positions Data
#China
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'IAD_BP6_USD')
CN.FDI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#CN.FDI.query$Obs[[1]]
#Japan
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'IAD_BP6_USD')
JP.FDI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#JP.FDI.query$Obs[[1]]
#United States
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'IAD_BP6_USD')
US.FDI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#US.FDI.query$Obs[[1]]

#Australia
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'IAD_BP6_USD')
AU.FDI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)

#Exchange Rate
#China
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'ENDA_XDC_USD_RATE')
CN.XCH.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#CN.XCH.query$Obs[[1]]
#Japan
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'ENDA_XDC_USD_RATE')
JP.XCH.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#JP.XCH.query$Obs[[1]]
#United States
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'ENDA_XDC_USD_RATE')
US.XCH.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)

#Australia
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'ENDA_XDC_USD_RATE')
AU.XCH.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)


########################################################################
#Pause for 5 seconds before continue execution 
########################################################################

#Balance of Payments 
#China
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'BGS_BP6_USD')
CN.BOP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#CN.BOP.query$Obs[[1]]
#Japan
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'BGS_BP6_USD')
JP.BOP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#JP.BOP.query$Obs[[1]]
#United States
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'BGS_BP6_USD')
US.BOP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#US.BOP.query$Obs[[1]]

#Australia
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'BGS_BP6_USD')
AU.BOP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#AU.BOP.query$Obs[[1]]

#Bond Yield
#China
# queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'FITBBE_PA')
# CN.IR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
#                                  checkquery)
#CN.IR.query$Obs[[1]]
#Japan
########################################################################
#Pause for 5 seconds before continue execution 
########################################################################
# Sys.sleep(5)
# 
# queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'FIGB_PA')
# JP.IR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
#                                  checkquery)
#JP.IR.query$Obs[[1]]
#United States
# queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'FIGB_PA')
# US.IR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
#                                  checkquery)
# #US.IR.query$Obs[[1]]

#Australia
# queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'FIGB_PA')
# AU.IR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
#                                  checkquery)
#AU.IR.query$Obs[[1]]

#GDP
#China
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'NGDP_NSA_XDC')
CN.GDP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate,
                                  checkquery)
CN.GDP.query$Obs[[1]]
#Japan
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'NGDP_NSA_XDC')
JP.GDP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate,
                                  checkquery)
#JP.GDP.query$Obs[[1]]

########################################################################
#Pause for five seconds before continue execution 
########################################################################
Sys.sleep(0.5)
#United States
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'NGDP_NSA_XDC')
US.GDP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#US.GDP.query$Obs[[1]]

#Australia
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'NGDP_NSA_XDC')
AU.GDP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#AU.GDP.query$Obs[[1]]


#CPI
#China
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'PCPI_PC_PP_PT')
CN.CPI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#CN.CPI.query$Obs[[1]]
#Japan
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'PCPI_PC_PP_PT')
JP.CPI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#JP.CPI.query$Obs[[1]]
#United States
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'PCPI_PC_PP_PT')
US.CPI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#US.CPI.query$Obs[[1]]

#Australia
Sys.sleep(0.5)
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'PCPI_PC_PP_PT')
AU.CPI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#AU.CPI.query$Obs[[1]]


# #Unemployment Rate
# #China
# queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'LUR_PT')
# CN.UMR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
#                                   checkquery)
# #CN.UMR.query$Obs[[1]]
# #Japan
# queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'LUR_PT')
# JP.UMR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
#                                   checkquery)
# #JP.UMR.query$Obs[[1]]
# #United States
# queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'LUR_PT')
# US.UMR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
#                                   checkquery)
# #US.UMR.query$Obs[[1]]
# 
# #Australia
# queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'LUR_PT')
# AU.UMR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
#                                   checkquery)
# #AU.UMR.query$Obs[[1]]

```

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
#################################################################################
#################################################################################
#Plot Data
#################################################################################
#################################################################################

# print('earliest available interest rate data fo China: ')
# print(min(CN.IR.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available interest rate data fo Australia: ')
# print(min(AU.IR.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available interest rate data fo Japan: ')
# print(min(JP.IR.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available interest rate data fo US: ')
# print(min(US.IR.query$Obs[[1]]$`@TIME_PERIOD`))
# 
# print('newest available interest rate data fo China: ')
# print(max(CN.IR.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available interest rate data fo Australia: ')
# print(max(AU.IR.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available interest rate data fo Japan: ')
# print(max(JP.IR.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available interest rate data fo US: ')
# print(max(US.IR.query$Obs[[1]]$`@TIME_PERIOD`))

# years <- 2006:2023
# quarters <- paste(rep(years, each = 4), rep(c("-Q1", "-Q2", "-Q3", "-Q4"), times = length(years)), sep = "")
# all_quarters <- data.frame(Time = quarters[quarters >= "2006-Q2" & quarters <= "2023-Q4"])
# 
# IR_data = data.frame(JP.IR.query$Obs[[1]])
# IR_data = IR_data[,1:2]
# colnames(IR_data) = c('Time', 'JP_IR')
# #IR_data$Time <- as.character(as.yearqtr(IR_data$Time))
# IR_data = left_join(all_quarters,  IR_data, by = "Time")
# IR_data <- IR_data %>%
#   fill(JP_IR, .direction = "down")
# 
# tmp = data.frame(CN.IR.query$Obs[[1]])
# tmp = tmp[,1:2]
# colnames(tmp) = c('Time', 'CN_IR')
# IR_data = left_join(IR_data,  tmp, by = "Time")

# tmp = data.frame(US.IR.query$Obs[[1]])
# tmp = tmp[,1:2]
# colnames(tmp) = c('Time', 'US_IR')
# IR_data = left_join(IR_data,  tmp, by = "Time")
# 
# tmp = data.frame(AU.IR.query$Obs[[1]])
# tmp = tmp[,1:2]
# colnames(tmp) = c('Time', 'AU_IR')
# IR_data = left_join(IR_data,  tmp, by = "Time")
# 
# IR_data$Time = as.yearqtr(IR_data$Time, format = "%Y-Q%q")
# 
# IR_data <- IR_data %>%
#   mutate(
#     JP_IR = as.numeric(JP_IR),
#     CN_IR = as.numeric(CN_IR),
#     US_IR = as.numeric(US_IR),
#     AU_IR = as.numeric(AU_IR)
#   )
# IR_data_long <- pivot_longer(IR_data,
#                               cols = colnames(IR_data)[2:5],
#                               names_to = "Country",
#                               values_to = "IR")

# IR_plot <- ggplot(data = IR_data_long, aes(x = Time, y = IR, color = Country)) +
#   geom_line() +
#   scale_color_manual(values = c(
#     "CN_IR" = "#3498db",  # Blue
#     "JP_IR" = "#e74c3c",  # Red
#     "US_IR" = "#2ecc71",  # Green
#     "AU_IR" = "#f39c12"   # Orange
#   )) +
#   theme_minimal() +
#   labs(x = "Time", y = "IR (%)", title = "IR Trends", color = "Country") +
#   theme(
#     legend.title = element_blank(),  # Removes the legend title for a cleaner look
#     legend.position = "bottom"       # Moves the legend to the bottom
#   )

#################################################################################
###################################CPI##########################################
#################################################################################
# print('earliest available consumer price index data fo China: ')
# print(min(CN.CPI.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available consumer price index data fo Australia: ')
# print(min(AU.CPI.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available consumer price index data fo Japan: ')
# print(min(JP.CPI.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available consumer price index data fo US: ')
# print(min(US.CPI.query$Obs[[1]]$`@TIME_PERIOD`))

print('newest available consumer price index data fo China: ')
print(max(CN.CPI.query$Obs[[1]]$`@TIME_PERIOD`))
print('newest available consumer price index data fo Australia: ')
print(max(AU.CPI.query$Obs[[1]]$`@TIME_PERIOD`))
print('newest available consumer price index data fo Japan: ')
print(max(JP.CPI.query$Obs[[1]]$`@TIME_PERIOD`))
print('newest available consumer price index data fo US: ')
print(max(US.CPI.query$Obs[[1]]$`@TIME_PERIOD`))

years <- 1986:2024
quarters <- paste(rep(years, each = 4), rep(c("-Q1", "-Q2", "-Q3", "-Q4"), times = length(years)), sep = "")
all_quarters <- data.frame(Time = quarters[quarters >= "1986-Q2" & quarters <= "2024-Q1"])

CPI_data = data.frame(JP.CPI.query$Obs[[1]])
CPI_data = CPI_data[,1:2]
colnames(CPI_data) = c('Time', 'JP_CPI')
#CPI_data$Time <- as.character(as.yearqtr(CPI_data$Time))
CPI_data = left_join(all_quarters,  CPI_data, by = "Time")

tmp = data.frame(CN.CPI.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'CN_CPI')
CPI_data = left_join(CPI_data,  tmp, by = "Time")

tmp = data.frame(US.CPI.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'US_CPI')
CPI_data = left_join(CPI_data,  tmp, by = "Time")

tmp = data.frame(AU.CPI.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'AU_CPI')
CPI_data = left_join(CPI_data,  tmp, by = "Time")

CPI_data$Time = as.yearqtr(CPI_data$Time, format = "%Y-Q%q")

CPI_data <- CPI_data %>%
  mutate(
    JP_CPI = as.numeric(JP_CPI),
    CN_CPI = as.numeric(CN_CPI),
    US_CPI = as.numeric(US_CPI),
    AU_CPI = as.numeric(AU_CPI)
  )
CPI_data_long <- pivot_longer(CPI_data,
                              cols = colnames(CPI_data)[2:5],
                              names_to = "Country",
                              values_to = "CPI")

CPI_plot <- ggplot(data = CPI_data_long, aes(x = Time, y = CPI, color = Country)) +
  geom_line() +
  scale_color_manual(values = c(
    "CN_CPI" = "#3498db",  # Blue
    "JP_CPI" = "#e74c3c",  # Red
    "US_CPI" = "#2ecc71",  # Green
    "AU_CPI" = "#f39c12"   # Orange
  )) +
  theme_minimal() +
  labs(x = "Time", y = "CPI (%)", title = "CPI Trends", color = "Country") +
  theme(
    legend.title = element_blank(),  # Removes the legend title for a cleaner look
    legend.position = "bottom"       # Moves the legend to the bottom
  )
#################################################################################
###############################################FDI###############################
#################################################################################
# print('earliest available foreign direct investment data fo China: ')
# print(min(CN.FDI.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available foreign direct investment data fo Australia: ')
# print(min(AU.FDI.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available foreign direct investment data fo Japan: ')
# print(min(JP.FDI.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available foreign direct investment data fo US: ')
# print(min(US.FDI.query$Obs[[1]]$`@TIME_PERIOD`))
# 
# print('newest available foreign direct investment data fo China: ')
# print(max(CN.FDI.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available foreign direct investment data fo Australia: ')
# print(max(AU.FDI.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available foreign direct investment data fo Japan: ')
# print(max(JP.FDI.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available foreign direct investment data fo US: ')
# print(max(US.FDI.query$Obs[[1]]$`@TIME_PERIOD`))

years <- 2010:2023
quarters <- paste(rep(years, each = 4), rep(c("-Q1", "-Q2", "-Q3", "-Q4"), times = length(years)), sep = "")
all_quarters <- data.frame(Time = quarters[quarters >= "2010-Q4" & quarters <= "2023-Q3"])

FDI_data = data.frame(JP.FDI.query$Obs[[1]])
FDI_data = FDI_data[,1:2]
colnames(FDI_data) = c('Time', 'JP_FDI')
#FDI_data$Time <- as.character(as.yearqtr(FDI_data$Time))
FDI_data = left_join(all_quarters,  FDI_data, by = "Time")

tmp = data.frame(CN.FDI.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'CN_FDI')
FDI_data = left_join(FDI_data,  tmp, by = "Time")

tmp = data.frame(US.FDI.query$Obs[[1]])
tmp = tmp[,c(1, 3)]
colnames(tmp) = c('Time', 'US_FDI')
FDI_data = left_join(FDI_data,  tmp, by = "Time")

tmp = data.frame(AU.FDI.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'AU_FDI')
FDI_data = left_join(FDI_data,  tmp, by = "Time")

FDI_data$Time = as.yearqtr(FDI_data$Time, format = "%Y-Q%q")

FDI_data <- FDI_data %>%
  mutate(
    JP_FDI = as.numeric(JP_FDI),
    CN_FDI = as.numeric(CN_FDI),
    US_FDI = as.numeric(US_FDI),
    AU_FDI = as.numeric(AU_FDI)
  )
FDI_data_long <- pivot_longer(FDI_data,
                              cols = colnames(FDI_data)[2:5],
                              names_to = "Country",
                              values_to = "FDI")

FDI_plot <- ggplot(data = FDI_data_long, aes(x = Time, y = FDI, color = Country)) +
  geom_line() +
  scale_color_manual(values = c(
    "CN_FDI" = "#3498db",  # Blue
    "JP_FDI" = "#e74c3c",  # Red
    "US_FDI" = "#2ecc71",  # Green
    "AU_FDI" = "#f39c12"   # Orange
  )) +
  theme_minimal() +
  labs(x = "Time", y = "FDI (%)", title = "FDI Trends", color = "Country") +
  theme(
    legend.title = element_blank(),  # Removes the legend title for a cleaner look
    legend.position = "bottom"       # Moves the legend to the bottom
  )

#################################################################################
###########################################XCH###################################
#################################################################################

# print('earliest available exchange rate data fo China: ')
# print(min(CN.XCH.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available exchange rate data fo Australia: ')
# print(min(AU.XCH.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available exchange rate data fo Japan: ')
# print(min(JP.XCH.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available exchange rate data fo US: ')
# print(min(US.XCH.query$Obs[[1]]$`@TIME_PERIOD`))
# 
# print('newest available exchange rate data fo China: ')
# print(max(CN.XCH.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available exchange rate data fo Australia: ')
# print(max(AU.XCH.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available exchange rate data fo Japan: ')
# print(max(JP.XCH.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available exchange rate data fo US: ')
# print(max(US.XCH.query$Obs[[1]]$`@TIME_PERIOD`))

years <- 1980:2024
quarters <- paste(rep(years, each = 4), rep(c("-Q1", "-Q2", "-Q3", "-Q4"), times = length(years)), sep = "")
all_quarters <- data.frame(Time = quarters[quarters >= "1980-Q1" & quarters <= "2024-Q1"])

XCH_data = data.frame(JP.XCH.query$Obs[[1]])
XCH_data = XCH_data[,1:2]
colnames(XCH_data) = c('Time', 'JP_XCH')
#XCH_data$Time <- as.character(as.yearqtr(XCH_data$Time))
XCH_data = left_join(all_quarters,  XCH_data, by = "Time")

tmp = data.frame(CN.XCH.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'CN_XCH')
XCH_data = left_join(XCH_data,  tmp, by = "Time")

tmp = data.frame(US.XCH.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'US_XCH')
XCH_data = left_join(XCH_data,  tmp, by = "Time")

tmp = data.frame(AU.XCH.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'AU_XCH')
XCH_data = left_join(XCH_data,  tmp, by = "Time")

XCH_data$Time = as.yearqtr(XCH_data$Time, format = "%Y-Q%q")

XCH_data <- XCH_data %>%
  mutate(
    JP_XCH = as.numeric(JP_XCH),
    CN_XCH = as.numeric(CN_XCH),
    US_XCH = as.numeric(US_XCH),
    AU_XCH = as.numeric(AU_XCH)
  )

XCH_data_percent <- XCH_data %>%
  mutate(across(c(JP_XCH, CN_XCH, AU_XCH),
                ~ (./lag(.) - 1) * 100,
                .names = "{.col}"))

XCH_data_long <- pivot_longer(XCH_data,
                              cols = colnames(XCH_data)[2:5],
                              names_to = "Country",
                              values_to = "XCH")
XCH_data_percent_long <- pivot_longer(XCH_data_percent,
                              cols = colnames(XCH_data)[2:5],
                              names_to = "Country",
                              values_to = "XCH")
XCH_data_long <- XCH_data_long %>%
 filter(Country != "US_XCH")
XCH_data_long <- XCH_data_long %>%
  mutate(JP_Scaled_XCH = ifelse(Country == "JP_XCH", XCH * 0.1, XCH))
# XCH_data_percent_long <- XCH_data_percent_long %>%
#  filter(Country != "US_XCH")
XCH_plot <- ggplot(data = XCH_data_long, aes(x = Time, color = Country)) +
  geom_line(aes(y =  JP_Scaled_XCH))+  # Plot AU and CN on the primary axis Plot JP on the secondary axis
  scale_y_continuous(
    name = "Exchange rate against the dollar per AUD and CNY",
    sec.axis = sec_axis(~ . * 10, name = "Exchange rate against the dollar per 10 JPY")  # Adjust the transformation to match the scale factor used above
  ) +
  scale_color_manual(values = c(
    "CN_XCH" = "#3498db",  # Blue
    "JP_XCH" = "#e74c3c",  # Red
    "AU_XCH" = "#f39c12"   # Orange
  )) +
  theme_minimal() +
  labs(
    x = "Time",
    title = "Exchange Rate Against the Dollar",
    color = "Country"
  ) +
  theme(
    legend.title = element_blank(),
    legend.position = "bottom"
  )

#################################################################################
###########################################BOP###################################
#################################################################################
# print('earliest available balance of payments data fo China: ')
# print(min(CN.BOP.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available balance of payments data fo Australia: ')
# print(min(AU.BOP.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available balance of payments data fo Japan: ')
# print(min(JP.BOP.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available balance of payments data fo US: ')
# print(min(US.BOP.query$Obs[[1]]$`@TIME_PERIOD`))
# 
# print('newest available balance of payments data fo China: ')
# print(max(CN.BOP.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available balance of payments data fo Australia: ')
# print(max(AU.BOP.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available balance of payments data fo Japan: ')
# print(max(JP.BOP.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available balance of payments data fo US: ')
# print(max(US.BOP.query$Obs[[1]]$`@TIME_PERIOD`))

years <- 2005:2024
quarters <- paste(rep(years, each = 4), rep(c("-Q1", "-Q2", "-Q3", "-Q4"), times = length(years)), sep = "")
all_quarters <- data.frame(Time = quarters[quarters >= "2005-Q1" & quarters <= "2023-Q4"])

BOP_data = data.frame(JP.BOP.query$Obs[[1]])
BOP_data = BOP_data[,1:2]
colnames(BOP_data) = c('Time', 'JP_BOP')
#BOP_data$Time <- as.character(as.yearqtr(BOP_data$Time))
BOP_data = left_join(all_quarters,  BOP_data, by = "Time")

tmp = data.frame(CN.BOP.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'CN_BOP')
BOP_data = left_join(BOP_data,  tmp, by = "Time")

tmp = data.frame(US.BOP.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'US_BOP')
BOP_data = left_join(BOP_data,  tmp, by = "Time")

tmp = data.frame(AU.BOP.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'AU_BOP')
BOP_data = left_join(BOP_data,  tmp, by = "Time")

BOP_data$Time = as.yearqtr(BOP_data$Time, format = "%Y-Q%q")

BOP_data <- BOP_data %>%
  mutate(
    JP_BOP = as.numeric(JP_BOP),
    CN_BOP = as.numeric(CN_BOP),
    US_BOP = as.numeric(US_BOP),
    AU_BOP = as.numeric(AU_BOP)
  )
BOP_data_long <- pivot_longer(BOP_data,
                              cols = colnames(BOP_data)[2:5],
                              names_to = "Country",
                              values_to = "BOP")

BOP_plot <- ggplot(data = BOP_data_long, aes(x = Time, y = BOP, color = Country)) +
  geom_line() +
  scale_color_manual(values = c(
    "CN_BOP" = "#3498db",  # Blue
    "JP_BOP" = "#e74c3c",  # Red
    "US_BOP" = "#2ecc71",  # Green
    "AU_BOP" = "#f39c12"   # Orange
  )) +
  theme_minimal() +
  labs(x = "Time", y = "BOP (%)", title = "BOP Trends", color = "Country") +
  theme(
    legend.title = element_blank(),  # Removes the legend title for a cleaner look
    legend.position = "bottom"       # Moves the legend to the bottom
  )

#################################################################################
###########################################GDP###################################
#################################################################################
# print('earliest available gross domestic product data fo China: ')
# print(min(CN.GDP.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available gross domestic product data fo Australia: ')
# print(min(AU.GDP.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available gross domestic product data fo Japan: ')
# print(min(JP.GDP.query$Obs[[1]]$`@TIME_PERIOD`))
# print('earliest available gross domestic product data fo US: ')
# print(min(US.GDP.query$Obs[[1]]$`@TIME_PERIOD`))
# 
# print('newest available gross domestic product data fo China: ')
# print(max(CN.GDP.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available gross domestic product data fo Australia: ')
# print(max(AU.GDP.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available gross domestic product data fo Japan: ')
# print(max(JP.GDP.query$Obs[[1]]$`@TIME_PERIOD`))
# print('newest available gross domestic product data fo US: ')
# print(max(US.GDP.query$Obs[[1]]$`@TIME_PERIOD`))

years <- 1994:2024
quarters <- paste(rep(years, each = 4), rep(c("-Q1", "-Q2", "-Q3", "-Q4"), times = length(years)), sep = "")
all_quarters <- data.frame(Time = quarters[quarters >= "1994-Q1" & quarters <= "2023-Q3"])

GDP_data = data.frame(JP.GDP.query$Obs[[1]])
GDP_data = GDP_data[,1:2]
colnames(GDP_data) = c('Time', 'JP_GDP')
#GDP_data$Time <- as.character(as.yearqtr(GDP_data$Time))
GDP_data = left_join(all_quarters,  GDP_data, by = "Time")

tmp = data.frame(CN.GDP.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'CN_GDP')
GDP_data = left_join(GDP_data,  tmp, by = "Time")

tmp = data.frame(US.GDP.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'US_GDP')
GDP_data = left_join(GDP_data,  tmp, by = "Time")

tmp = data.frame(AU.GDP.query$Obs[[1]])
tmp = tmp[,1:2]
colnames(tmp) = c('Time', 'AU_GDP')
GDP_data = left_join(GDP_data,  tmp, by = "Time")
GDP_data$Time = as.yearqtr(GDP_data$Time, format = "%Y-Q%q")
GDP_data <- GDP_data %>%
  mutate(
    JP_GDP = as.numeric(JP_GDP),
    CN_GDP = as.numeric(CN_GDP),
    US_GDP = as.numeric(US_GDP),
    AU_GDP = as.numeric(AU_GDP)
  )

tmp = left_join(GDP_data, XCH_data, by = 'Time')
tmp$JP_GDP = tmp$JP_GDP/tmp$JP_XCH
tmp$CN_GDP = tmp$CN_GDP/tmp$CN_XCH
tmp$AU_GDP = tmp$AU_GDP/tmp$AU_XCH
tmp$US_GDP = tmp$US_GDP/tmp$US_XCH
GDP_data = tmp[colnames(GDP_data)[2:5]]

JP_GDP_ts <- ts(GDP_data$JP_GDP, start=c(1994, 1), frequency=4)
CN_GDP_ts <- ts(GDP_data$CN_GDP, start=c(1994, 1), frequency=4)
US_GDP_ts <- ts(GDP_data$US_GDP, start=c(1994, 1), frequency=4)
AU_GDP_ts <- ts(GDP_data$AU_GDP, start=c(1994, 1), frequency=4)

adjust_series <- function(series) {
  decomposed <- stl(series, s.window="periodic")
  seasadj(decomposed)  # Return the seasonally adjusted series
}

JP_GDP_adj <- adjust_series(JP_GDP_ts)
CN_GDP_adj <- adjust_series(CN_GDP_ts)
US_GDP_adj <- adjust_series(US_GDP_ts)
AU_GDP_adj <- adjust_series(AU_GDP_ts)

adjusted_GDP_data <- data.frame(
  Time = as.Date(time(JP_GDP_adj), origin = "1970-01-01"),
  JP_GDP = JP_GDP_adj,
  CN_GDP = CN_GDP_adj,
  US_GDP = US_GDP_adj,
  AU_GDP = AU_GDP_adj
)
adjusted_GDP_data$Time = as.yearqtr(adjusted_GDP_data$Time, format = "%Y-Q%q")

adjusted_GDP_data_long <- pivot_longer(adjusted_GDP_data,
                              cols = colnames(adjusted_GDP_data)[2:5],
                              names_to = "Country",
                              values_to = "GDP")

GDP_plot <- ggplot(data = adjusted_GDP_data_long, aes(x = Time, y = GDP, color = Country)) +
  geom_line() +
  scale_color_manual(values = c(
    "CN_GDP" = "#3498db",  # Blue
    "JP_GDP" = "#e74c3c",  # Red
    "US_GDP" = "#2ecc71",  # Green
    "AU_GDP" = "#f39c12"   # Orange
  )) +
  theme_minimal() +
  labs(x = "Time", y = "GDP", title = "Seasonally Adjustd GDP Trends", color = "Country") +
  theme(
    legend.title = element_blank(),  # Removes the legend title for a cleaner look
    legend.position = "bottom"       # Moves the legend to the bottom
  )
```

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
final_df = inner_join(CPI_data, adjusted_GDP_data, by = 'Time')
final_df = inner_join(final_df, XCH_data, by = 'Time')
final_df = final_df %>% dplyr::select(-c('US_XCH'))
final_df[c('JP_GDP', 'CN_GDP', 'US_GDP', 'AU_GDP')] = log(final_df[c('JP_GDP', 'CN_GDP', 'US_GDP', 'AU_GDP')])
#final_df[,2:ncol(final_df)] = scale(final_df[,2:ncol(final_df)])
##############################################scaling data###############################
print('Start time of analysis: ')
print(min(final_df$Time))
print('End time of analysis: ')
print(max(final_df$Time))
```


## Data Plots 
**Original Variables** 
```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
layout <-   (XCH_plot) / (GDP_plot)/ ( CPI_plot)
#layout <-   (BOP_plot | XCH_plot) / (GDP_plot | FDI_plot) / (CPI_plot)
#FDI_original_plot | BOP_original_plot) / (XCH_original_plot | GDP_original_plot)
layout
```
## Stationarity Check 
**Stationary Tests** 

The Augmented Dickey-Fuller Test is used in this section to test the null hypothesis that a unit root is present in the time series and the time series is non-stationary. 


```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
#install.packages("tseries")

perform_adf_test <- function(series) {
  adf_test <- adf.test(series, alternative = "stationary", k = 4)
  list(
    statistic = adf_test$statistic,
    lag.order = as.numeric(adf_test$parameter),
    p.value = adf_test$p.value
  )
}

results <- sapply(CPI_data[,2:5], perform_adf_test)
results_df <- as.data.frame(t(results), stringsAsFactors = FALSE)
names(results_df) <- c("Dickey-Fuller Statistic", "Lag Order", "P-value")
results_df$Country <- rownames(results_df)
rownames(results_df) <- NULL
kable(results_df, caption = "ADF Test Results for CPI Data by Country")
```
The lag order chosen in the ADF test is 4, which is appropriate given our data is of quarterly frequency. The result of the ADF test on the CPI data shows that we do not have enough evidence to reject the null hypothesis that the CPI series is unit root non-stationary at 1% significance level, except for China. 
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
results <- sapply(FDI_data[2:nrow(FDI_data),2:5], perform_adf_test)
results_df <- as.data.frame(t(results), stringsAsFactors = FALSE)
names(results_df) <- c("Dickey-Fuller Statistic", "Lag Order", "P-value")
results_df$Country <- rownames(results_df)
rownames(results_df) <- NULL
kable(results_df, caption = "ADF Test Results for Foreign Direct Investment (% change) Data by Country")
```
The ADF results shows that we do not have enough evidence to reject the null hypothesis that the foreign direct investment data is not unit-root stationary for Australia, the United States and Japan at 1% significance level and we do have enough evidence to reject the null hypothesis for China at 1% significance level. 


```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
results <- sapply(XCH_data[2:nrow(XCH_data),c(2,3,5)], perform_adf_test)
results_df <- as.data.frame(t(results), stringsAsFactors = FALSE)
names(results_df) <- c("Dickey-Fuller Statistic", "Lag Order", "P-value")
results_df$Country <- rownames(results_df)
rownames(results_df) <- NULL
kable(results_df, caption = "ADF Test Results for Exchange Rate (% change) Data by Country")
```

The result of the ADF test on the exchange rate data shows that we do not have enough evidence to reject the null hypothesis that the exchange rate against the dollar series is unit root non-stationary at 1% significance level. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
results <- sapply(BOP_data[2:nrow(BOP_data),2:5], perform_adf_test)
results_df <- as.data.frame(t(results), stringsAsFactors = FALSE)
names(results_df) <- c("Dickey-Fuller Statistic", "Lag Order", "P-value")
results_df$Country <- rownames(results_df)
rownames(results_df) <- NULL
kable(results_df, caption = "ADF Test Results for Balance of Payments (% change) Data by Country")
```
The result of the ADF test on the balance of payments data shows that we do not have enough evidence to reject the null hypothesis that the balance of payments series is unit root non-stationary at 1% significance level for all countries. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}

results <- sapply(adjusted_GDP_data[2:nrow(adjusted_GDP_data),2:5], perform_adf_test)
results_df <- as.data.frame(t(results), stringsAsFactors = FALSE)
names(results_df) <- c("Dickey-Fuller Statistic", "Lag Order", "P-value")
results_df$Country <- rownames(results_df)
rownames(results_df) <- NULL
kable(results_df, caption = "ADF Test Results for GDP (% change) Data by Country")
```

The result of the ADF test on the GDP data shows that we do not have enough evidence to reject the null hypothesis that the GDP series is unit root non-stationary at 5% significance level.

# 3. The Model and Hypothesis
We will employ four models to address our proposed problem. Firstly, we will use a standard Bayesian Vector Autoregressive (BVAR) model with independently and identically distributed innovations, as outlined in details in @wozniakBsvarsBayesianEstimation2022. Additionally, we will investigate a large BVAR model with flexible error covariance structures, following the methodology proposed by @Chan_2015. Specifically, the second model will incorporate MA(1) Gaussian innovations to better account for potential model misspecifications such as omitted variable bias and to facilitate shrinkage in VAR coefficients. Our third model will be a BVAR incorporating common stochastic volatilities to allow for time varying distribution of volatility terms, which will be useful to handle the impact of specific events like the Covid-19 Global Pandemic. The final model will combined a common stochastic volatility frame work with MA(1) Gaussian innovations, offering a robust approach to volatility modelling. 

## 3.1 Model 1: Standard BVAR(p) Model

### 3.1.1 Model Specification
$$ Y = XA+E$$
$$E|X \sim \mathcal{MN}_{T \times N}(0_{T \times N},\Sigma_{N \times N}, I_T)$$
$$
A = \begin{bmatrix}
\mu_0^T \\
A_1^T \\
\vdots \\
A_p^T
\end{bmatrix}
, \quad
Y = \begin{bmatrix}
y_1^T \\
y_2^T \\
\vdots \\
y_T^T
\end{bmatrix}
, \quad
x_t = \begin{bmatrix}
1 \\
y_{t-1}^T \\
\vdots \\
y_{t-p}^T
\end{bmatrix}
, \quad
X = \begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_T^T
\end{bmatrix}
, \quad
E = \begin{bmatrix}
e_1^T \\
e_2^T \\
\vdots \\
e_T^T
\end{bmatrix}
$$

where 

- $T$ is the number of time periods under consideration
- $N$ is the number of variables, in our case, N = 20
- $P$ is the number of lags
- $Y$ is a $T \times N$ matrix of variables of response variables we aim to model.
- $A$ is a $K \times N$ matrix of coefficients, $K = (1+ð‘\times N)$.
- $E$ is a $T \times N$ matrix of the error terms
- $X$ is a $T \times (1+ð‘\times N)$ matrix of covariates
- $\Sigma$ is a $N \times N$ matrix representing the row-specific covariance matrix
- $I_T$ is a $T \times T$ identity matrix representing the column specific covariance matrix
- $E|X$ follows a matrix-variate normal distribution with mean $0_{T \times N}$, row specific covariance matrix $\Sigma_{N \times N}$ and column specific covariance matrix $I_T$
- $x_{t}^T = \left( \begin{array}{cccc}1  & y_{t-1} & y_{t-2} & \cdots & y_{t-p} \end{array} \right)$



The Bayesian Vector Autoregression model as formulated above provides a robust framework for investigating the relationships among selected economic indicators across different nations. By employing this model, this research aims to quantitatively measure the influence of one countryâ€™s economic indicators on another, such as how lagged changes in Chinaâ€™s inflation rate, may influence the GDP growth rate of the United States and vice versa. The BVAR model, with its estimation of coefficients across various lags, offers a deep understanding of both immediate and more delayed economic interactions, which is crucial to analyzing the cyclical nature of trade relationships, investment flows, monetary policy environments, and economic performances and the transmission of these metrics across borders.

The strength of this BVAR model lies in its ability to incorporate prior economic knowledge and beliefs into the estimation process. By setting prior distributions for the matrix of coefficients A and the covariance matrix $\Sigma$, the model can be tailors to reflect established economic theories regarding international economic linkages and the time it takes for policy changes in one country to affect another. By calibrating the prior variances, particularly for the autoregressive coefficients, we can integrate prior knowledge or hypotheses, such as the presence of unit roots or the diminishing influence of distant lags on current values, into the analysis. When interpreting the estimation output, attention will be given to the posterior means and variances of the coefficients, which represent the modelâ€™s â€œlearntâ€ understanding of the underlying economic structure. The analysis will be supplemented by forecast error variance decompositions to better understand the proportion of the movements in economic indicators that can be accounted for by their own shocks versus shocks to other variables.

The economic context underscoring this analysis is the increased globalization over the past decade, marking an era where economies are more intertwined than ever through trade, capital flows, and policy decisions. This period has witnessed not only the strengthening of global economic ties but also recent calls from political leaders advocating for a reduction in globalization. These contrasting dynamics highlight the complexity of the current global economic landscape, where the push for deeper integration coexists with growing sentiments for retrenchment. This dual trend sets the stage for our investigation, providing a rich context to explore how economic variables across nations influence each other amidst fluctuating levels of global interconnectedness. In this environment, understanding the cross-country spillover effects is vital for policymakers and businesses alike, as decisions made in one country can have far-reaching implications. By addressing these aspects, this research will contribute to the discourse on economic policy formulation, risk assessment, and strategic planning. 


### 3.1.2 Prior Settings 
We will employ a Normal-Inverse Wishart distribution for the joint distribution of coefficient matrices A and the row-specific variance matrix $\Sigma$, and a Minnesota prior on the coefficients A. Specifically, we have:

$$\Sigma \sim \mathcal{IW}(S_0, \nu_0) $$


$$p(\Sigma) \propto |\Sigma|^{-\frac{\nu_0+N+1}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}S_0))$$

$$A|\Sigma \sim \mathcal{MN}_{K \times N}(A_0, \Sigma, V_A)$$

$$p(A|\Sigma) \propto |\Sigma|^{-\frac{K}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^T(A-A_0)))$$

$$(A,\Sigma) \sim \mathcal{NIW}_{K \times N} (A_0, V_A, S_0, \nu_0)$$

$$p(A,\Sigma) \propto |\Sigma|^{-\frac{K+\nu_0+N+1}{2}} \times exp(-\frac{1}{2}tr(\Sigma^{-1}S_0))\times exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^T(A-A_0)))$$

where 

- $$V_A = diag(\kappa_2 \quad \kappa_1(\mathbf{p} \otimes I_N^T))$$

- $$\mathbf{p} = [1 \quad 2 \quad ... \quad p]$$
- $$I_N = [1 \quad 1 \quad ... \quad 1] \in \mathbb{R}^N$$
- $$\kappa_1 \text{ is the overall shrinkage level for autoregressive slopes}$$ 
- $$\kappa_2 \text{ is the overall shrinkage lvel for the constant term }$$ 


Additionally, we adopt commonly used values for the hyperparameters as established in the literature.

$$A_0 = 0$$

$$v_0 = N+3$$

$$S_0 = I_N$$

$$\kappa_1 = 0.2^2 \quad \kappa_2 = 10^2$$

The hyperparameters $\kappa_1$ and $\kappa_2$ are specified in a way such that the coefficient associated with a lag l variable is shrunk more heavily to - as lag length increases whereas the intercepts are not shrunk to 0. 


### 3.1.3 Posterior Distributions

The posterior distribution specified above has the form 

$$p(Y|A, \Sigma) = (2\pi)^{-\frac{TN}{2}}|\Sigma|^{-\frac{T}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T(Y-XA))$$

and the joint posterior distribution

$$p(A, \Sigma \mid Y)  = \frac{p(A,\Sigma,Y)}{p(Y)}\propto p(A, \Sigma, Y) \propto p(Y|A,\Sigma)\times p(A,\Sigma) =  p(Y|A,\Sigma) p(A \mid \Sigma) p(\Sigma)$$


$$\propto |\Sigma|^{-\frac{T}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T(Y-XA))) \times$$

$$\mid \Sigma \mid^{-\frac{\nu_0+N+K}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}S_o))exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))$$


$$\propto |\Sigma|^{-\frac{T+N+K+\nu_0+1}{2}} \times \exp (-\frac{1}{2}tr(\Sigma^{-1}S_0)) \times $$

$$exp(-\frac{1}{2}tr(\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^TY-\bar{A}^T\bar{V}^{-1}\bar{A})\times$$
$$exp(-\frac{1}{2}tr(\Sigma^{-1}(A-\bar{A})^T\bar{V}^{-1}(A-\bar{A})))$$
Hence, 

$$p(A, \Sigma \mid Y, X) = p(A \mid Y, X, \Sigma) p(\Sigma \mid Y, X)$$


$$p(A \mid Y, X, \Sigma) = \mathcal{MN}_{k \times N}(\bar{A}, \Sigma, \bar{V})$$

$$p(\Sigma \mid Y, X) = \mathcal{IW}_N(\bar{S}, \bar{\nu})$$

where

$$\bar{V} = (X^TX + V_A^{-1})^{-1}$$



$$\bar{A} = \bar{V}(X^TY + V_A^{-1}A_0)$$

$$\bar{\nu} = T + \nu_0$$

$$\bar{S} = S_0 + Y^TY + A_0^TV_A^{-1}A_0 - \bar{A}^T\bar{V}^{-1}\bar{A}$$


### 3.1.4 Estimation Procedure 


In this setting, we have 

- $(A,\Sigma) \sim \mathcal{NIW}_{K \times N} (A_0, V_A, S_0, \nu_0)$

then, prior draws can be sampled from 

- $\Sigma \sim \mathcal{IW}(S_0, \nu_0)$
- $A|\Sigma \sim \mathcal{MN}_{K \times N}(A_0, \Sigma, V_A)$


we use the following Gibb's Sampler algorithm to sample from the posterior distribution: 

- Initialize $\Sigma$ at $\Sigma^0$ 
- For $s = 1,...,S_1+S_2$
    1. Draw a sample $A^{(s)}$ from $p(A \mid Y, X, \Sigma^{(s-1)}) \sim \mathcal{MN}_{K \times N}(\bar{A}, \Sigma, \bar{V})$
    2. Draw a sample $\Sigma^{(s)}$ from $p(\Sigma \mid Y, X, A^{(s)}) = \mathcal{IW}_N(\bar{S}, \bar{\nu})$
    
We discard the first $S_1$ sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain $S_2$ sampled draws from the joint posterior distribution. 

$$\left\{A^{(s)}, \Sigma^{(s)}\right\}_{s=S_1+1}^{S_1+S_2}$$

The draws from joint predictive density can then be obtained using the following algorithm: 

1. Sample S draws $\left\{ A^{(s)}, \Sigma^{(s)} \right\}_{s=1}^{S}$ from $p(A,\Sigma|Y, X)$ 
2. Sample S draws $\left\{ Y_{t+h}^{(s)} \right\}_{s=1}^S$ from $Y_{t+h}^{(s)} \sim \mathcal{N}_{hN}(Y_{t+h|t}(A^{(s)}), \mathbb{V}ar[Y_{t+h|t}|A^{(s)}, \Sigma^{(s)}])$

where 
$$Y_{t+h} = \begin{pmatrix}
    Y_{t+1} \\
    Y_{t+2} \\
    \vdots    \\
    Y_{t+h} 
\end{pmatrix}
$$

To derive the posterior predictive density for $Y_{t+h}$, we first note that: 

$$p(Y_{t+h}|Y_{t}) = \int \int p(Y_{t+h}|Y_{t},A, \Sigma)\times p(A,\Sigma|Y,X)dAd\Sigma$$
where 

$$p(y_{t+h}|Y, X, A, \Sigma, 1) \sim \mathcal{N}_{hN} (Y_{t+h|t}(A), Var(Y_{t+h}|A, \Sigma))$$

$$Y_{t+h|t}(A) =\begin{pmatrix}
    Y_{t+1|t} \\
    Y_{t+2|t} \\
    \vdots    \\
    Y_{t+h|t} 
\end{pmatrix} = \begin{pmatrix}  
\mu_0 + A_1 Y_t + \cdots + A_pY_{t-p+1|t} \\ 
\mu_0 + A_1 Y_{t+1|t} + \cdots + A_p Y_{t-p+2|t}\\ 
\vdots \\ 
\mu_0 + A_1 Y_{t+1|t} + \cdots + A_p Y_{t-p+h|t}
\end{pmatrix}$$


$$\mathbb{V}ar(Y_{t+h|t}|A, \Sigma) = \begin{pmatrix}
    \Sigma & \Sigma \phi_1^T & \cdots & \Sigma \phi_{h-1}^T \\
    \phi_1\Sigma &\Sigma + \phi_1 \Sigma \phi_1^T  & \cdots & \Sigma \phi_1^T + \phi_1 \Sigma \phi_2^T+ \cdots + \phi_{h-2}\Sigma \phi_{h-1}^T + \phi_{h-1} \Sigma \phi_{h}^T \\
    \vdots & \vdots & \ddots & \vdots \\
    \phi_{h-1}\Sigma\ & \phi_1\Sigma  + \phi_2 \Sigma \Phi_1^T + \cdots + \phi_{h} \Sigma \phi_{h-1}^T & \cdots & \Sigma + \phi_1 \Sigma \phi_1^T + \cdots+\phi_{h-1}\Sigma\phi_{h-1}^T+\phi_h\Sigma\phi_h^T
\end{pmatrix}
$$
where $\phi_i = J A^i J'$ are the parameters of the VMA($\infty$) representation of VAR($p$) and $A$ is the parameter matrix of VAR(1) representation of VAR($p$).

$$p(A|Y, X, \Sigma) \sim \mathcal{MN}_{K \times N}(\bar{A}, \Sigma, \bar{V})$$
In this approach, we are incorporating into our prediction the uncertainties in the estimates of the parameters by integrating over A and $\Sigma$, the is solved using numerical integration. 



### 3.1.5 Test for Granger Causality 
Granger causality testing, introduced in @Granger_1969, is a way to measure whether the past and current values of one time series contains useful information for predicting future values of another. The idea behind granger causality testing is that if a time series does contain predictive information about another, incorporating it into the model should reduce the error variance and improve the precision of the forecasts. The classical approach for testing causality is the Wald test, but it fails to quantify the degree of evidence in the data in favour of or against the causality hypothesis. The Bayesian approach to testing Granger causality based on Baye's factor, on the other hand, has the ability to quantify the evidence for and against the hypothesis with a single number. The Baye's factor is defined as:

$$B_H = \frac{p[H_0|Y]}{p[H_1|Y]}$$ 
In the context of testing for VAR(p) models, we have the marginal likelihood under the assumption of no Granger causality is:

$$p_0(Y) = \int p_1(Y| A_{ij} = 0, A_{-(ij)})dA_{-(ij)} = p_1(Y|A_{ij}=0)$$ 
using Baye's rule, we have: 
$$p_0(Y) = \frac{p_1(A_{ij} = 0|Y) \times p_1(Y)}{p_1(A_{ij} = 0)}$$
We divide the above equation by $p(y_t|H_1)$ to get Baye's factor: 

$$B_H = \frac{p_0(Y)}{p_1(Y)} = \frac{p_1(A_{ij} = 0|Y)}{p_1(A_{ij} = 0)}=\frac{\int p_1(A_{ij=0}|Y, \Sigma)d \Sigma}{\int p(A_{ij}=0, \Sigma)d\Sigma}$$

and the integration is performed using numerical integration methods. The second part is also known as the Savage-Dickey density ratio estimator, which is the ratio of the posterior over the prior density evaluated at $A_{ij} = 0$. Indicative values for interpreting Bayes factors is provided below: 

To test for Granger causality, we use the equation:
$$B_H = \frac{p_1(A_{ij=0}|Y)}{p(A_{ij}=0)} $$
and the integral is evaluated using numerical integration. 

**Descriptive labels for certain Bayes factors.**

| Label                          | $B_H$ | $p(H_1 \mid X)$ |
|--------------------------------|-----------|-----------------|
| Data strongly support $H_1$    | 10        | 91%             |
| Data weakly support $H_1$      | 3         | 75%             |
| Data provide ambiguous information | 1     | 50%             |
| Data weakly support $H_0$      | 1/3       | 25%             |
| Data strongly support $H_0$    | 1/10      | 9%              |


## 3.2 Model 2: Large BVAR model with MA(1) Gaussian Innovations 
Incorporating MA(1) Gaussian innovations in a large BVARs model lead to a significant enhancement over traditional BVAR models, especially when forecasting macroeconomic variables for several reasons. Firstly, the variances of economic shocks is rarely constant over time. For example, volatility tends to cluster during periods of economic crisis and is more tranquil during stable times. Incorporating stochastic volatility allows the model to adapt to changing volatility in the data, improving forecasting performance especially in the presence of financial market instability or economic policy shifts. In addition, by allowing for serial correlation in the innovations term, we will be able to capture the momentum or persistence in economic variables that is often observed in real-world data. Recognizing that shocks may have a lasting impact over several periods can enhance the model's ability to predict future values by considering the path-dependent nature of the economy. Our BVAR model with common stochastic volatility is a natural extension to the standard BVAR model and formulated as below: 

### 3.2.1 Model Specification 

$$ Y = XA+E$$
$$E|X \sim \mathcal{MN}_{T \times N}(0_{T \times N},\Sigma_{N \times N}, \Omega_{T \times T})$$
$$
A = \begin{bmatrix}
\mu_0^T \\
A_1^T \\
\vdots \\
A_p^T
\end{bmatrix}
, \quad
Y = \begin{bmatrix}
y_1^T \\
y_2^T \\
\vdots \\
y_T^T
\end{bmatrix}
, \quad
x_t = \begin{bmatrix}
1 \\
y_{t-1}^T \\
\vdots \\
y_{t-p}^T
\end{bmatrix}
, \quad
X = \begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_T^T
\end{bmatrix}
, \quad
E = \begin{bmatrix}
e_1^T \\
e_2^T \\
\vdots \\
e_T^T
\end{bmatrix}
$$

where 

- $T$ is the number of time periods under consideration
- $N$ is the number of variables, in our case, N = 20
- $P$ is the number of lags
- $Y$ is a $T \times N$ matrix of variables of response variables we aim to model.
- $A$ is a $K \times N$ matrix of coefficients, $K = (1+ð‘\times N)$.
- $E$ is a $T \times N$ matrix of the error terms
- $X$ is a $T \times (1+ð‘\times N)$ matrix of covariates
- $\Sigma$ is a $N \times N$ matrix representing the row-specific covariance matrix
- $I_T$ is a $T \times T$ identity matrix representing the column specific covariance matrix
- $E|X$ follows a matrix-variate normal distribution with mean $0_{T \times N}$, row specific covariance matrix $\Sigma_{N \times N}$ and column specific covariance matrix $I_T$

with MA(1) innovations, we have, for i = 1,....,N and t = 1,...,T, $$e_{it} = \eta_{it} + \psi \eta_{i,t-1}$$ where $|\psi|<1$ and $\eta_{it} \sim \mathcal{N}(0,1)$

In matrix notation, we have: 

$$e_i = H_{\psi} \eta_i$$ 

where 


$$
e_i = \begin{bmatrix}
e_{i1} \\
e_{i2} \\
\vdots \\
e_{iT}
\end{bmatrix} \qquad 
\eta_i = \begin{bmatrix}
\eta_{i1} \\
\eta_{i2} \\
\vdots \\
\eta_{iT}
\end{bmatrix} \qquad 
H_{\psi} = \left(
\begin{array}{cccc}
1 & 0  & \cdots & 0 \\
\psi & 1  & \cdots & 0 \\
\vdots & \ddots & \ddots & \vdots \\
0 & \cdots & \psi & 1
\end{array}
\right) \qquad
O_{\psi} = diag(1+\psi^2, 1, ..., 1)
$$

 
Hence, we have 

$$e \sim \mathcal{N}(0, H_{\psi}O_{\psi}H_{\psi}^T) \qquad \qquad \Omega = H_{\psi}O_{\psi}H_{\psi}^T$$
Note that the covariance matrix $\Omega$ depends on $\psi$ only. 

### 3.2.2 Prior Specification 

We consider a prior independent distributions for $(A, \Sigma, \Omega)$, specifically, we have: $$P(A, \Sigma, \Omega) = P(A, \Sigma) \times P(\Omega)$$

We will employ a Normal-Inverse Wishart distribution for the joint distribution of A and $\Sigma$ and before, $$(A,\Sigma) \sim \mathcal{NIW}_{K \times N}(A_0, V_A, S_0, \nu_0)$$

We apply a truncated normal prior on $\psi$, $$\psi \sim \mathcal{N}(\psi_0, V_{\psi})\mathbb{1}_{\{|\psi|<1\}}$$

For estimation purposes, we initialize with the following setting: 

- $e_{i1} \sim \mathcal{N}(0, 1+\psi^2)$
- $\psi_0 = 0$ and $V_{\psi} = 1$ so that the prior centers around 0 with a relatively large variance and has support within (-1, 1)

### 3.2.3 Posterior Distributions 

1. The posterior distribution of Y specified above has the form: 

$$p(Y|A, \Sigma) = (2\pi)^{-\frac{Tn}{2}}|\Sigma|^{-\frac{T}{2}}|\Omega|^{-\frac{N}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T\Omega^{-1}(Y-XA))$$

$$= (2\pi)^{-\frac{Tn}{2}}|\Sigma|^{-\frac{T}{2}}(1+\psi^2)^{-\frac{N}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T(H_{\psi}O_{\psi}H_{\psi}^T)^{-1}(Y-XA))$$

2. The joint posterior distribution of A and $\Sigma$ can be derived as follows:

$$p(A, \Sigma \mid Y, \Omega)  = \frac{p(A,\Sigma,Y, \Omega)}{p(Y, \Omega)}$$
$$\propto p(A, \Sigma, Y, \Omega) \propto p(Y \mid A,\Sigma, \Omega)\times p(A,\Sigma, \Omega)$$
$$=  p(Y \mid A,\Sigma, \Omega) p(A, \Sigma) p(\Omega) = p(Y \mid A,\Sigma, \Omega) p(A \mid \Sigma) p(\Sigma) p(\Omega)$$

$$\propto p(Y \mid A,\Sigma, \Omega) p(A \mid \Sigma) p(\Sigma)$$



$$\propto |\Sigma|^{-\frac{T}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T\Omega^{-1}(Y-XA)) \times$$

$$\mid \Sigma \mid^{-\frac{\nu_0+N+K}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}S_o))exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))$$


$$\propto |\Sigma|^{-\frac{T+N+K+\nu_0+1}{2}} \times \exp (-\frac{1}{2}tr(\Sigma^{-1}S_0)) \times $$
$$exp(-\frac{1}{2}tr(\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\Omega^{-1}Y-\bar{A}^T\bar{V}^{-1}\bar{A})\times$$
$$exp(-\frac{1}{2}tr(\Sigma^{-1}(A-\bar{A})^T\bar{V}^{-1}(A-\bar{A})))$$

Hence, 

$$p(A, \Sigma \mid Y, X) = p(A \mid Y, X, \Sigma) p(\Sigma \mid Y, X)$$


$$p(A \mid Y, X, \Sigma, \Omega) = \mathcal{MN}_{K \times N}(\bar{A}, \Sigma, \bar{V})$$

$$p(\Sigma \mid Y, X) = \mathcal{IW}_N(\bar{S}, \bar{\nu})$$

where

$$\bar{V} = (X^T\Omega^{-1}X + V_A^{-1})^{-1}$$



$$\bar{A} = \bar{V}(X^T\Omega^{-1}Y + V_A^{-1}A_0)$$

$$\bar{\nu} = T + \nu_0$$

$$\bar{S} = S_0 + Y^T\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \bar{A}^T\bar{V}^{-1}\bar{A}$$
3. The posterior distribution for the parameter $\psi$ can be obtained as follows: 
$$P(\psi|Y, A, \Sigma) = \frac{P(\psi, Y, A, \Sigma)}{P(Y, A, \Sigma)} \propto P(\psi, Y, A, \Sigma) = P(Y|A, \Sigma, \psi) \times P(A,\Sigma, \psi) $$

$$= P(Y|A, \Sigma, \psi) \times P(A, \Sigma) \times P(\psi) \propto P(Y|A, \Sigma, \psi) \times P(\psi)$$

we can sample from the posterior distribution of $\psi$ using an independence-chain Metropolis-Hastings algorithm. 

### 3.2.4 Estimation Procedure 

We obtain posterior estimates of $A, \Sigma, \psi$ using a Gibbs sampler, specifically, we initialize $\psi^{(0)}$ and for s = 1,...,S1+S2, we sequentially sample: 

- $\Sigma^{(s)} | Y, X, \psi^{(s-1)} \sim \mathcal{IW}_N(\bar{S}, \bar{\nu})$
- $A^{(s)} | Y, X, \Sigma^{(s)}, \psi^{(s-1)} \sim \mathcal{MN}_{k \times N}(\bar{A}, \Sigma^{(s)}, \bar{V})$
- $\psi^{(s)} | Y, X, A^{(s)}, \Sigma^{(s)} \propto P(Y|A^{(s)}, \Sigma^{(s)}, \psi^{s-1})\times p(\psi)$

The Metropolis-Hastings Algorithm for Sampling $\psi$ is given as follows: 



**Initialization:**

- Choose an initial value $\psi^{(0)}$ within the bounds $(-1, 1)$. 

**Proposal Distribution:**

- Select the proposal distribution $q(\psi' | \psi^{(t)}) \sim N(\psi^{(t)}, \tau^2)$. $\tau^2$ is a tuning parameter that controls the step size.
    
**Sampling Loop:**

For t = 1,...,$T_1+T_2$

- Generate a candidate $\psi'^{(t)}$ from $q(\psi' | \psi^{(t-1)})$.

- Check if $\psi'$ is within the bounds $(-1, 1)$. If not, reject $\psi'$ (set $\alpha = 0$).

- Compute the acceptance ratio $\alpha$:
            $$ \alpha(\psi^{(t-1)}, \psi') = \min\left(1, \frac{p(Y | A, \Sigma, \psi') p(\psi') q(\psi^{(t-1)} | \psi')}{p(Y | A, \Sigma, \psi^{(t-1)}) p(\psi^{(t-1)}) q(\psi' | \psi^{(t-1)})}\right)$$

**Decide to accept or reject:**

- Generate a random number $u$ from $U[0,1]$.

- If $u \leq \alpha$, accept $\psi'$ and set $\psi^{(t)} = \psi'$.

- Otherwise, reject $\psi'$ and set $\psi^{(t)} = \psi^{(t-1)}$.
  
**Burn in period**

- Discard the first $T_1$ samples to allow the algorithm to converge to the true distribution 

**Obtain one sample of $\psi$**

- Randomly draw a $\psi^*$ from the $T_2$ draws and set $\psi^{(s)} = \psi^*$

We monitor the acceptance rate and adjust $\tau^2$ as necessary to achieve an optimal rate of about 20-40\%.

We note that since $\Omega$ is a band matrix, we do not need compute $\Omega^{-1}$, instead, we obtain the Cholesky decomposition $C_{\Omega}$ of $\Omega$, which has a time complexity of $O(T)$. Terms involving $\Omega^{-1}$ such as $X^T\Omega^{-1}X$ can be obtained by: $$X^T\Omega^{-1}X = X^T(C_{\Omega}^{-1})^{T}C_{\Omega}^{-1}X=(C_{\Omega}^{-1}X)^T(C_{\Omega}^{-1}X) = \tilde{X}^T\tilde{X}$$

## 3.3 Model 3: Large BVAR model with ommon Stochastic Volatility
## 3.4 Model 4: Large BVAR model with MA(1) Gaussian innovations and Common Stochastic Volatility


### 3.4.1 Model Specification


$$ Y = XA+E$$ 

where

$$
A = \begin{bmatrix}
\mu_0^T \\
A_1^T \\
\vdots \\
A_p^T
\end{bmatrix}
, \quad
Y = \begin{bmatrix}
y_1^T \\
y_2^T \\
\vdots \\
y_T^T
\end{bmatrix}
, \quad
x_t = \begin{bmatrix}
1 \\
y_{t-1}^T \\
\vdots \\
y_{t-p}^T
\end{bmatrix}
, \quad
X = \begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_T^T
\end{bmatrix}
, \quad
E = \begin{bmatrix}
e_1^T \\
e_2^T \\
\vdots \\
e_T^T
\end{bmatrix}
$$
where 

- $T$ is the number of time periods under consideration
- $N$ is the number of variables, in our case, N = 20
- $P$ is the number of lags
- $Y$ is a $T \times N$ matrix of variables of response variables we aim to model.
- $A$ is a $K \times N$ matrix of coefficients, $K = (1+ð‘\times N)$
- $E$ is a $T \times N$ matrix of the error terms
- $X$ is a $T \times (1+ð‘\times N)$ matrix of covariates
- $\Sigma$ is a $N \times N$ matrix representing the row-specific covariance matrix


the same as before. But, instead of the column specific matrix as the identity matrix, we specify the column specific matrix as a diagonal matrix $\Omega$. Specifically, we have: 

$$\epsilon_t = u_t + \psi_1 u_{t-1}$$

$$u_t \sim \mathcal{N}(0,e^{h_t} \Sigma)$$

$$h_t = \rho h_{t-1} + u_t^h \quad \text{ follows an Autoregressive process of lag 1 AR(1), and}$$

$$u_t^h \sim N(0,\sigma_h^2)$$

$$\Omega = \left(
\begin{array}{cccc}
(1 + \psi_1^2) e^{h_1} & \psi_1 e^{h_1} & \cdots & 0 \\
\psi_1 e^{h_1} & \psi_1^2 e^{h_1} + e^{h_2} & \cdots & \vdots \\
0 & \cdots & \ddots & \vdots \\
\vdots & \cdots & \psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} & \psi_1 e^{h_{T-1}} \\
0 & \cdots & \psi_1 e^{h_{T-1}} & \psi_1^2 e^{h_{T-1}} + e^{h_T}
\end{array}
\right)$$

In this specification, each element of $e_t$ may have distinct variances, and the variances of all innovations can be scaled by a common factor. This approach is economically intuitive, as the volatility of macroeconomic variables often exhibit co-movement. It is important to emphasize that each component of $e_t$ must adhere to the same univariate time series model. 

### 3.4.2 Prior Specification


Here, we consider a priori independent distributions for $(A, \Sigma, \Omega)$, namely: 

$$p(A, \Sigma, \Omega) = p(A, \Sigma) \times p(\Omega)$$
Given this structure, we can sample from the posterior distribution by sequentially sampling from: 

- $P(A, \Sigma | Y, X, \Omega)$
- $P(\Omega | Y, X, A, \Sigma)$

The prior distribution of $(A,\Sigma)$ follow the same normal inverse Wishart prior distribution as outlined in model one. But the variance matrix $V_A$ for A is different: 

$$V_A = diag(v_{A,ii})$$

$$v_{A,ii} = \begin{cases} 
\kappa_1(\frac{l^2}{\hat{s}_r}) & \text{for a coefficient associated to lag l of variable r} \\ 
\kappa_2& \text{for an intercept}
\end{cases}$$

where $\hat{s}_r$ is the sample variance of an AR(4) model for the variable r. 


$$E|X \sim \mathcal{MN}_{T \times N}(0_{T \times N},\Sigma_{N \times N}, \Omega_{T \times T})$$

and, 


For the moving average coefficients, we adopt an uninformative truncated normal prior for the MA coefficient $\psi$:

$$
\psi \sim \mathcal{N}(\psi_0, V_\psi) \mathbb{1}(|\psi|<1)
$$

and we set $\psi_0 = 0$ and $V_\psi = 1$ so that the prior centers around 0 with a relatively large variance and has support within (-1,1). Further more, we assume independent priors for $\sigma^2_h$ and $\rho$:

$$\sigma_h^2 \sim \mathcal{IG}(\nu_{h_0}, s_{h_0})$$

$$\rho \sim \mathcal{N}(\rho_0, V_\rho) \mathbb{1}(|\rho|<1)$$

We set the hyperparameters $\nu_{h_0} = 5$, $s_{h_0} = 0.04$, $\rho_0 = 0.9$ and $V_\rho = 0.04$ so that the prior mean of $\sigma_h^2$ is 0.01 and $\rho$ is centered at 0.9. 

### 3.4.3 Posterior Distribution

The posterior distribution specified above has the following form: 

$$p(Y|A, \Sigma) = (2\pi)^{-\frac{Tn}{2}}|\Sigma|^{-\frac{T}{2}}|\Omega|^{-\frac{N}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T\Omega^{-1}(Y-XA))$$

and the joint posterior distribution

$$p(A, \Sigma \mid Y, \Omega)  = \frac{p(A,\Sigma,Y, \Omega)}{p(Y, \Omega)}$$
$$\propto p(A, \Sigma, Y, \Omega) \propto p(Y \mid A,\Sigma, \Omega)\times p(A,\Sigma, \Omega)$$
$$=  p(Y \mid A,\Sigma, \Omega) p(A, \Sigma) p(\Omega) = p(Y \mid A,\Sigma, \Omega) p(A \mid \Sigma) p(\Sigma) p(\Omega)$$

$$\propto p(Y \mid A,\Sigma, \Omega) p(A \mid \Sigma) p(\Sigma)$$



$$\propto |\Sigma|^{-\frac{T}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T\Omega^{-1}(Y-XA)) \times$$

$$\mid \Sigma \mid^{-\frac{\nu_0+N+K}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}S_o))exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))$$


$$\propto |\Sigma|^{-\frac{T+N+K+\nu_0+1}{2}} \times \exp (-\frac{1}{2}tr(\Sigma^{-1}S_0)) \times $$
$$exp(-\frac{1}{2}tr(\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\Omega^{-1}Y-\bar{A}^T\bar{V}^{-1}\bar{A})\times$$
$$exp(-\frac{1}{2}tr(\Sigma^{-1}(A-\bar{A})^T\bar{V}^{-1}(A-\bar{A})))$$

Hence, 

$$p(A, \Sigma \mid Y, X) = p(A \mid Y, X, \Sigma) p(\Sigma \mid Y, X)$$


$$p(A \mid Y, X, \Sigma, \Omega) = \mathcal{MN}_{k \times N}(\bar{A}, \Sigma, \bar{V})$$

$$p(\Sigma \mid Y, X) = \mathcal{IW}_N(\bar{S}, \bar{\nu})$$

where

$$\bar{V} = (X^T\Omega^{-1}X + V_A^{-1})^{-1}$$



$$\bar{A} = \bar{V}(X^T\Omega^{-1}Y + V_A^{-1}A_0)$$

$$\bar{\nu} = T + \nu_0$$

$$\bar{S} = S_0 + Y^T\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \bar{A}^T\bar{V}^{-1}\bar{A}$$

### 3.4.4 Estimation Procedure 


In this setting, we have 

- $(A,\Sigma) \sim \mathcal{NIW}_{K \times N} (A_0, V_A, S_0, \nu_0)$
- $p(A, \Sigma, \Omega) = p(A, \Sigma) \times p(\Omega)$

then, prior draws can be sampled from 

- $\Sigma \sim \mathcal{IW}(S_0, \nu_0)$
- $A|\Sigma \sim \mathcal{MN}_{K \times N}(A_0, \Sigma, V_A)$
- $\Omega = \left(
\begin{array}{cccc}
(1 + \psi_1^2) e^{h_1} & \psi_1 e^{h_1} & \cdots & 0 \\
\psi_1 e^{h_1} & \psi_1^2 e^{h_1} + e^{h_2} & \cdots & \vdots \\
0 & \cdots & \ddots & \vdots \\
\vdots & \cdots & \psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} & \psi_1 e^{h_{T-1}} \\
0 & \cdots & \psi_1 e^{h_{T-1}} & \psi_1^2 e^{h_{T-1}} + e^{h_T}
\end{array}
\right)$
- $\epsilon_t = u_t + \psi_1 u_{t-1}$

- $\psi \sim \mathcal{N}(\psi_0, V_\psi) \mathbb{1}(|\psi|<1)$

- $u_t \sim \mathcal{N}(0,e^{h_t} \Sigma)$

- $h_t = \rho h_{t-1} + u_t^h$

- $\rho \sim \mathcal{N}(\rho_0, V_\rho) \mathbb{1}(|\rho|<1)$

- $u_t^h \sim N(0,\sigma_h^2)$

- $\sigma_h^2 \sim \mathcal{IG}(\nu_{h_0}, s_{h_0})$



To sample $S_1+S_2$ draws of $\left\{\Omega^{(s)}\right\}_{s=1}^{S_1+S_2}$

1. Sample $S_1+S_2$ draws of $\left\{\sigma^{2,(s)}_h\right\}_{s=1}^{S_1+S_2}$ from $\mathcal{IG}(v_{h_0}, s_{h_0})$
2. Sample $S_1+S_2$ draws of $\left\{\rho^{(s)}\right\}_{s=1}^{S_1+S_2}$ from $\mathcal{N}(\rho_0, V_\rho) \mathbb{1}(|\rho|<1)$
3. For each $\sigma^{2,(s)}_h$, sample $\left\{u_t^{h,(s)}\right\}_{t=1}^T$ from $N(0,\sigma_h^{2,(s)})$
4. For t = 1,...,T and s = 1,..., $S_1+S_2$, compute $h_t^{(s)} = \rho h_{t-1}^{(s)} + u_t^{h,(s)}$
5. Sample $S_1+S_2$ draws of $\left\{u_t^{(s)}\right\}_{s=1}^{S_1+S_2}$ from $u_t \sim \mathcal{N}(0,e^{h_t^{(s)}} \Sigma)$ for t = 1,...,T
6. Sample $S_1+S_2$ draws of $\left\{\psi^{(s)}\right\}_{s=1}^{S_1+S_2}$ from $\mathcal{N}(\psi_0, V_\psi) \mathbb{1}(|\psi|<1)$
7. for each t = 1,...,T and s = 1,...$S_1+S_2$, compute $\epsilon_t^{(s)} = u_t^{(s)} + \psi^{(s)}u_{t-1}^{(s)}$

After we have obtained $\left\{\Omega^{(s)}\right\}_{s=1}^{S_1+S_2}$, we can use the following Gibb's Sampler algorithm to sample from the posterior distribution $p(A \mid Y, X, \Sigma, \Omega)$: 


- Initialize $\Sigma$ at $\Sigma^0$ 
- For $s = 1,...,S_1+S_2$
    1. Draw a sample $A^{(s)}$ from $p(A \mid Y, X, \Omega^{(s)}, \Sigma^{(s-1)}) \sim \mathcal{MN}_{k \times N}(\bar{A}, \Sigma, \bar{V})$
    2. Draw a sample $\Sigma^{(s)}$ from $p(\Sigma \mid Y, X, A^{(s)}, \Omega^{(s)}) = \mathcal{IW}_N(\bar{S}, \bar{\nu})$
    
We discard the first $S_1$ sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain $S_2$ sampled draws from the joint posterior distribution. 

$$\left\{A^{(s)}, \Sigma^{(s)}\right\}_{s=S_1+1}^{S_1+S_2}$$

Sampling from the joint predictive density is the same as before. 

# 4. Model Estimation
## 4.1 Standard Bayesian VAR
### 4.1.1 Model Building Code and Validation 
We verify that our model can replicate the true parameter of the data generate process by: 
1. Generate artificial data containing 1000 observations simulated from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2. That is, 

$$\mathbf{y_t} = \begin{pmatrix} y_t,1 \\ y_t,2\end{pmatrix} = \mathbf{y_{t-1}} + \mathbf{\epsilon_t} = \begin{pmatrix}y_{t-1,1}\\y_{t-1, 2}\end{pmatrix} + \begin{pmatrix}\epsilon_{t,1}\\ \epsilon_{t, 2}\end{pmatrix}$$ and 

$$\mathbf{\epsilon} \sim iid \mathcal{N}(\mathbf{0}, \mathbf{I_2} )$$


```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
set.seed(123)

n = 1000  
cov_matrix = diag(2)  

random_walk_sample = matrix(nrow = n, ncol = 2)
#initialize the random walk at 0,0
random_walk_sample[1, ] = c(0, 0)

for (i in 2:n) {
  random_walk_sample[i, ] = random_walk_sample[i - 1, ] + mvrnorm(n = 1, mu = c(0, 0), Sigma = cov_matrix)
  }
mat_plot = random_walk_sample 
colnames(mat_plot) = c("Series1", "Series2")
mat_plot = as.data.frame(mat_plot)
mat_plot$Index = 1:nrow(mat_plot) 
mat_plot = melt(mat_plot, id.vars = "Index")

ggplot(mat_plot, aes(x = Index, y = value, color = variable)) +
    geom_line() +
    labs(title = "1000 observations simulated from a bi-variate Gaussian random walk process", x = "", y = "") +
    theme_minimal()
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
#if (!require(matrixNormal)) {install_packages('matrixNormal')}
BVAR_fit <- function(data, p, S1, S2, kappa1, kappa2, A_prior, V_prior, S_prior, nu_prior, N){
  Y = (data[(p+1):nrow(data),])
  N = ncol(Y)
  Ty = nrow(Y)
  S = S1 + S2
  Y = (data[(p+1):nrow(data),])
  X = matrix(1,nrow(Y),1) 
  for (i in 1:p){
    X     = cbind(X, (data[(p+1):nrow(data)-i,]))
  }
  X = as.matrix(X)
  Y = as.matrix(Y)
  lambda <- 1e-5
  V_bar = solve(t(X)%*%X + solve(V_prior) + diag(lambda, nrow = ncol(X)))
  #V_bar = round(V_bar, 5)
  A_bar = V_bar%*%(t(X)%*%Y + solve(V_prior)%*%A_prior)
  nu_bar = Ty + nu_prior
  #S_bar  = S_prior + t(Y)%*%Y + t(A_prior)%*%diag(1/diag(V_prior))%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
  S_bar = S_prior +  t(Y)%*%Y + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
  S_bar_inv = solve(S_bar)
  #Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)
  Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)
  Sigma_posterior = apply(Sigma_posterior,3,solve)
  Sigma_posterior  = array(Sigma_posterior,c(N,N,S))
  A_posterior = array(rnorm(prod(c(dim(A_bar),S))),c(dim(A_bar),S))
  L = t(chol(V_bar))
  for (s in 1:S){
    A_posterior[,,s]= A_bar + L%*%A_posterior[,,s]%*%chol(Sigma_posterior[,,s])
    }
  A_posterior = A_posterior[,,(S1+1):(S1+S2)]
  Sigma_posterior = Sigma_posterior[,,(S1+1):(S1+S2)]
  return(list(A_posterior, Sigma_posterior))
}
```
Then we estimate a model with a constant term and 1 lag using the simulated data, show that the posterior mean of the autoregressive and the covariance matrices are close to an identity matrix and that the posterior mean of the constant term is close to a vector of zeros.
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
p = 1
kappa1 = 0.02^2
kappa2 = 10^2
N = ncol(random_walk_sample)
A_prior             = matrix(0,1+p*N,N)
A_prior[2:(N + 1),] = diag(N)
V_prior     = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))
S_prior     = diag(1, N)
nu_prior    = ncol(random_walk_sample)+1
S1 = 1000 
S2 = 2000

posterior_samples = BVAR_fit(data = random_walk_sample, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)

posterior_samples_A = posterior_samples[[1]]
posterior_samples_Sigma = posterior_samples[[2]]
print('Posterior mean of autoregressive parameter:')
round(apply(posterior_samples_A,1:2,mean),3)
print('Posterior standard deviation of autoregressive parameter:')
round(apply(posterior_samples_A,1:2,sd),3)
print('Posterior mean of covariance matrix:')
round(apply(posterior_samples_Sigma,1:2,mean),3)
print('Posterior standard deviation of covariance matrix:')
round(apply(posterior_samples_Sigma,1:2,sd),3)
```

### 4.1.2 Empirical Result

**1. Fitted Model Parameters **
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
p = 5
kappa1 = 0.2^2
kappa2 = 10^2
data = final_df[, 2:ncol(final_df)]
N = ncol(data)
A_prior = matrix(0,1+p*N,N)
A_prior[2:(N + 1),] = diag(N)
V_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))
S_prior = diag(1, N)
nu_prior  = ncol(data)+1
data = final_df[,2:ncol(final_df)]
#data = final_df[,2:19]
S1 = 100
S2 = 200

posterior_samples = BVAR_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)

posterior_samples_A = posterior_samples[[1]]
posterior_samples_Sigma = posterior_samples[[2]]
# print('Posterior mean of autoregressive parameter:')
# round(apply(posterior_samples_A,1:2,mean),3)
# print('Posterior standard deviation of autoregressive parameter:')
# round(apply(posterior_samples_A,1:2,sd),3)
# print('Posterior mean of covariance matrix:')
# round(apply(posterior_samples_Sigma,1:2,mean),3)
# print('Posterior standard deviation of covariance matrix:')
# round(apply(posterior_samples_Sigma,1:2,sd),3)
```
**2. Prediction Plots**
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
BVAR_predict = function(posterior_samples_A, posterior_samples_Sigma, data, h){
  S = dim(posterior_samples_A)[3]
  N =dim(posterior_samples_A)[2]
  p = (dim(posterior_samples_A)[1] -1) / N
  Y = (data[(p+1):nrow(data),])
  #last p observations 
  x = Y[(nrow(Y)-p+1):nrow(Y),]
  # reverse the order t-p t-(p-1)...
  x = x[p:1,]
  Y_h   = array(NA,c(h,N,S))
  for (s in 1:S){
    A     = posterior_samples_A[,,s]
    Sigma = posterior_samples_Sigma[,,s]
    N = ncol(A)
    p = (nrow(A) -1) / N
    Y = (data[(p+1):nrow(data),])
    x  = Y[(nrow(Y)-p+1):nrow(Y),]
    x = x[p:1,]
    
    for (i in 1:h){
      #x_{{t+1}= (1 y_t y_{t-1},... y_{t-p+1})
      x_T               = c(1,purrr::as_vector(t(x)))
      #y_t+h = mvnnorm(1, x_t%*%A)
      Period_Y               = mvtnorm::rmvnorm(1, mean = x_T%*%A, sigma=Sigma)
      colnames(Period_Y) = colnames(x)
      x            = rbind(Period_Y,x[1:(p-1),])
      Y_h[i,,s]   = Period_Y[1:N]
      }
  }
  return(Y_h)
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
data = final_df[,2:ncol(final_df)]
h = 8
BVAR_prediction = BVAR_predict(posterior_samples_A = posterior_samples_A, posterior_samples_Sigma = posterior_samples_Sigma, data = data, h = h)
```

```{r}
# plots of forecasts
plot_CI = function(prediction, data, final_df, var_index){
  forecast_mean  = apply(prediction,1,mean) 
  forecast_interval = apply(prediction,1,hdi,credMass=0.90)
  forecast_range = range(data[,i],forecast_interval)
  mcxs1  = "#05386B"
  mcxs2  = "#379683"
  mcxs3  = "#5CDB95"
  mcxs4  = "#8EE4AF"
  mcxs5  = "#EDF5E1"
  mcxs1.rgb   = col2rgb(mcxs1)
  mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
  mcxs2.rgb   = col2rgb(mcxs2)
  mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)
    
  par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1)
  plot(1:(length(data[,i])+h),c(data[,i],forecast_mean), type="l", ylim=forecast_range, axes=FALSE, xlab="", ylab="", lwd=2)
  # specifying x axis
  x_axis_position = c(1,11,21,31,41,51,61,71, 81, 91, 101, 111, nrow(final_df),nrow(final_df)+h)
  x_axis_tick = final_df$Time[x_axis_position[1:(length(x_axis_position)-1)]]
  #x_axis_tick[length(x_axis_tick) + 1] = max(final_df$Time)+1/4
  x_axis_tick[length(x_axis_tick) + 1] = max(final_df$Time)+1/4*h
  #x_axis_tick[length(x_axis_position)] =""
  axis(1, x_axis_position, x_axis_tick, col=mcxs1)
  # specifying y axis
  axis(2,c(forecast_range[1],mean(forecast_range),forecast_range[2]),c("",colnames(data)[var_index],""), cex.axis = 1.5, col=mcxs1)
  #vline for begin forecast period
  text(x=nrow(final_df)+2, y=0.8*forecast_range[2], srt=90, final_df$Time[dim(final_df)[1]]+1/4)
  abline(v=nrow(final_df)+1, col=mcxs1)
  #Confidence interval for forecasts
  lower_bound <- forecast_interval[1,]
  upper_bound <- forecast_interval[2,]
  polygon_x = ( dim(data)[1]+1):( dim(data)[1] + h)
  polygon(x = c(polygon_x, rev(polygon_x)), 
          y = c(lower_bound, rev(upper_bound)),
          col=mcxs1.shade1, border=mcxs1.shade1)
}

nrow_plot = ceiling(ncol(data)/2)
par(mfrow=c(nrow_plot,2), mar=rep(3,4),cex.axis=1.5)
for (i in 1:ncol(data)){
  plot_CI(prediction = BVAR_prediction[,i,], data = data, final_df = final_df, var_index = i)
}
```

**3. Test for Granger Causality**
We first perform Granger Causality test for each country, we test whether the macroeconomic factor of one country has some predictive power on the macroeconomic factors of another. Specifically, we test the following hypothesis: 

1. $H_0: $ The consumer price index, interest rate, foreign direct investment, balance of payments, gross domestic product, exchange rate against the dollar of country A has no impact on these variables in the country B$

The Baye's factor in this case is defined as: 

$$B_H = \frac{p_0(Y)}{p_1(Y)} = \frac{p_1(A_{ij} = 0|Y)}{p_1(A_{ij} = 0)}=\frac{\int p_1(A_{ij} = 0|Y, X,\Sigma)d \Sigma}{\int p_1(A_{ij}=0, \Sigma)d\Sigma}$$
$$p_1(A_{ij} = 0|Y, X,\Sigma) \sim \mathcal{MN}_{K \times N}(\bar{A_{ij}}, \Sigma_{ij}, \bar{V}_{ij})$$
$$p_1(A_{ij} = 0, \Sigma) \sim \mathcal{NIW}_{K \times N}(A_{0, ij}, V_{A, ij}, S_{0, ij}, \nu_{0, ij})$$
$$p_1(A_{ij} = 0, \Sigma) = p(A_{ij}|\Sigma)\times p(\Sigma)$$
$$A_{ij}|\Sigma \sim \mathcal{MN}_{K \times N} (A_{0,ij}, \Sigma, V_{A, ij}) $$
$$\Sigma \sim \mathcal{IW}(S_0, \nu_0)$$
and we test the bi-lateral relationship between each of these countries. 


```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
Y = (data[(p+1):nrow(data),])
X = matrix(1,nrow(Y),1) 
for (i in 1:p){
  X     = cbind(X, (data[(p+1):nrow(data)-i,]))
}
X = as.matrix(X)
N = ncol(data)
K = 1+p*N
kappa1 = 0.2^2
kappa2 = 10^2
V_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))
lambda = 1e-4
posterior_V = solve(t(X)%*%X + solve(V_prior) + diag(lambda, nrow = ncol(X)))

calc_bayes_factor = function(country1, country2){
  ratio_list = list(dim = S2)
  A_0_mat = apply(posterior_samples_A,1:2,mean)
  dimnames(A_0_mat) = list(c("constant", rep(colnames(data),p)), colnames(data))
  var_list = c('_GDP', '_CPI',  '_XCH')
  
  A_0_df <- as.data.frame(A_0_mat)
  A_0_df$row_names <- rownames(A_0_mat)
  
  if (country1 != 'US'){
    country1 = paste0(country1, var_list)
  }else{country1 = paste0(country1, var_list[1:(length(var_list)-1)])}
  
  if (country2 != 'US'){
    country2 = paste0(country2, var_list)
  }else{country2 = paste0(country2, var_list[1:(length(var_list)-1)])}
  
  
  for (c1 in country1){
    for (c2 in country2){
      A_0_df[A_0_df$row_names == c2, c1] = 0
    }
  }
  
  row_names <- A_0_df$row_names
  A_0_mat <- as.matrix(A_0_df[, 1:ncol(A_0_df)-1])
  rownames(A_0_mat) <- row_names
  
  for (s in 1:S2){
    prior_Sigma = MCMCpack::riwish(v = nu_prior, S = S_prior)
    A_0 = c(A_0_mat)
    M = c(A_prior)
    Q = kronecker(prior_Sigma, V_prior)
    prior_prob =  mvtnorm::dmvnorm(x = A_0, mean = M, sigma = Q, log = TRUE)
    M = c(posterior_samples_A[,,s])
    Q = kronecker(posterior_samples_Sigma[,,s], posterior_V)
    posterior_prob = mvtnorm::dmvnorm(x = A_0, mean = M, sigma = Q, log = TRUE)
    ratio_list[s] = posterior_prob - prior_prob
  }
  return(ratio_list)
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
#aa = calc_bayes_factor('CN', "US")
countries_list = c('CN', 'US', "JP", "AU")
pb <- progress_bar$new(
  format = "[:bar] :percent :etas",
  total = 100, clear = FALSE, width = 60
)


bayesfactor = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))
for (country_a in countries_list){
  for (country_b in countries_list){
    pb$tick()
    bayesfactor[country_a, country_b] = mean(unlist(calc_bayes_factor(country_a, country_b)))
    gc()
  }
}
```


```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
# 
# countries = rownames(bayesfactor)
# 
# # Define starting and ending pairs for edges in both directions
# starting <- c(rep(countries, each = length(countries) - 1), rep(countries, each = length(countries) - 1))
# ending <- c(
#   unlist(sapply(countries, function(x) setdiff(countries, x))),
#   unlist(sapply(countries, function(x) setdiff(countries, x)))
# )
# nodes <- data.frame(name = countries)
# 
# # Create labels and colors for the edges
# edge_labels <- c(
#   paste(starting[1:(length(starting)/2)], "to", ending[1:(length(ending)/2)]),
#   paste(starting[(length(starting)/2 + 1):length(starting)], "to", ending[(length(ending)/2 + 1):length(ending)])
# )
# #edge_colors <- rep(c("red", "blue", "green", "purple", "orange"), length.out = length(starting))
# edge_colors = c()
# i = 1
# for (s in starting){
#   for (e in ending){
#     if (bayesfactor[s,e] > 0){
#       edge_colors[i] = 'salmon'
#     }
#     else{edge_colors[i] = 'gray'}
#   }
#   i = i+1
# }
# # Create the edges data frame
# edges <- data.frame(
#   from = starting,
#   to = ending,
#   #label = edge_labels,
#   color = edge_colors
# )
# 
# library(igraph)
# 
# # Create the graph object
# g <- graph_from_data_frame(d = edges, vertices = nodes, directed = TRUE)
# 
# # Set the layout for the graph
# layout <- layout_with_fr(g)
# 
# # Scale the layout coordinates
# scale_factor <- 10  # Adjust this factor to make edges longer or shorter
# layout_scaled <- layout * scale_factor
# i = 1
# vertex_color = c()
# for (c in countries){
#   if (bayesfactor[c, c] < 0){
#     vertex_color[i] = 'salmon'
#   }
#   else{vertex_color[i] = 'gray'}
#   i = i+1
# }
# # Plot the graph with the scaled layout and different labels for each direction
# plot(g, layout = layout_scaled,
#      vertex.label = V(g)$name,
#      #edge.label = E(g)$label,
#      edge.color = E(g)$color,
#      edge.arrow.size = 0.5,
#      vertex.size = 30,
#      vertex.label.cex = 1,
#      vertex.label.dist = 1.5,
#      vertex.color = 'salmon')

```

|              | China | United States | Japan           | Australia     |
|--------------|---------------|---------------|-----------------|---------------|
| **China**    | -     | Inf           | 5.466142e+215   | Inf           |
| **US**       | Inf   | -             | Inf             | Inf           |
| **Japan**    | Inf   | Inf           | -    | Inf           |
| **Australia**| Inf   | 1.697063e-58  | Inf             | 0             |



|              | China | United States | Japan           | Australia     |
|--------------|---------------|---------------|-----------------|----------------|
| **China**    |       -         | Strong Evidence for Granger Causality| SStrong Evidence for Granger Causality| Strong Evidence for Granger Causality
| **US**       | Strong Evidence for Granger Causality |   -  | Strong Evidence for Granger Causality | Strong Evidence for Granger Causality |
| **Japan**    | Strong Evidence for Granger Causality   | Strong Evidence for Granger Causality           | -    | Strong Evidence for Granger Causality           |
| **Australia**| Strong Evidence for Granger Causality   | Strong Evidence for Granger Causality  | Strong Evidence **against** Granger Causality             |  -            |



## 4.2 Large BVAR model with MA(1) Gaussian Innovations 
### 4.2.1 Model Building Code and Validation 
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
BVAR_MA_fit <- function(data, p, S1, S2, kappa1, kappa2, A_prior, V_prior, S_prior, nu_prior, N){
  Y = (data[(p+1):nrow(data),])
  N = ncol(Y)
  Ty = nrow(Y)
  S = S1 + S2
  X = matrix(1,nrow(Y),1) 
  K = 1+p*N
  tau = 0.15
  for (i in 1:p){
    X     = cbind(X, (data[(p+1):nrow(data)-i,]))
  }
  X = as.matrix(X)
  Y = as.matrix(Y)
  
  nu_bar = Ty + nu_prior
  psi = 0
  H_psi = matrix(0, nrow = Ty, ncol = Ty)
  O_psi = diag(rep(1, Ty))
  O_psi[1,1] = 1+psi^2
  for (t in 1:Ty){
    H_psi[t,t] = 1
    if (t < Ty){
      H_psi[t+1, t] = psi
    }
  }
  A_posterior = array(rnorm(prod(c(c(K,N),S))),c(c(K,N),S))
  Sigma_posterior = array(dim = c(N,N,S))
  Omega_posterior = array(dim = c(Ty, Ty, S+1))
  Omega_posterior[,,1] = H_psi %*% O_psi %*% t(H_psi)
  
  accept = 0
  pb <- progress_bar$new(total = S)
  
  for (s in 1:S){
    pb$tick()

    chol_Omega = chol(Omega_posterior[,,s]) #########change 
    chol_Omega_inv = solve(chol_Omega)
    #V_bar = solve(t(X)%*%X + solve(V_prior))
    V_bar = solve(t(chol_Omega_inv%*%X)%*%(chol_Omega_inv%*%X) + solve(V_prior))
    L = t(chol(V_bar))
    #A_bar = V_bar%*%(t(X)%*%Y + solve(V_prior)%*%A_prior)
    A_bar = V_bar%*%(t(chol_Omega_inv%*%X)%*%(chol_Omega_inv%*%Y) + solve(V_prior)%*%A_prior)
    #S_bar = S_prior +  t(Y)%*%Y + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
    S_bar = S_prior +  t(chol_Omega_inv%*%Y)%*%(chol_Omega_inv%*%Y) + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
    S_bar_inv = solve(S_bar)
    Sigma_posterior[,,s] = solve(rWishart(1, df=nu_bar, Sigma=S_bar_inv)[,,1])
    A_posterior[,,s]= A_bar + L%*%A_posterior[,,s]%*%chol(Sigma_posterior[,,1]) ############change 
    ###############MH Step    ##################################
    psi_candidate = rnorm(n = 1, mean = psi, sd = tau)
    if(abs(psi_candidate) > 1){
      alpha = 0
    }
    else{
      H_psi = matrix(0, nrow = Ty, ncol = Ty)
      O_psi = diag(rep(1, Ty))
      O_psi[1,1] = 1+psi_candidate^2
      for (i in 1:Ty){
        H_psi[i,i] = 1
        if (i < Ty){
          H_psi[i+1, i] = psi_candidate
        }
      }
      
      
      Omega_draw = H_psi %*% O_psi %*% t(H_psi)
      chol_Omega_draw = chol(Omega_draw)
      chol_Omega_draw_inv = solve(chol_Omega_draw)
      Sigma_posterior_inv = solve(Sigma_posterior[,,s])
      #p_y_numerator = ((1 + psi_candidate^2)/( 1 + psi^2))^(-N/2)*exp(-1/2*tr(solve(Sigma_posterior[,,s])%*%t(chol_Omega_inv%*%(Y-X%*%A_posterior[,,s]))%*%(chol_Omega_inv%*%(Y-X%*%A_posterior[,,s])))
      #                                                                +1/2*tr(solve(Sigma_posterior[,,s])%*%t(chol_Omega_draw_inv%*%(Y-X%*%A_posterior[,,s]))%*%(chol_Omega_inv%*%(Y-X%*%A_posterior[,,s]))\
      
      Likelihood_ratio = ((1+psi_candidate^2)/(1+psi^2))^(N/2) * exp(-1/2*(tr(
        Sigma_posterior_inv%*%t(chol_Omega_draw_inv%*%(Y-X%*%A_posterior[,,s]))%*%(chol_Omega_draw_inv%*%(Y-X%*%A_posterior[,,s]))
        ) - tr(
          Sigma_posterior_inv%*%t(chol_Omega_inv%*%(Y-X%*%A_posterior[,,s]))%*%(chol_Omega_inv%*%(Y-X%*%A_posterior[,,s]))))
        )
      prior_ratio = dtruncnorm(x = psi_candidate, mean = psi_prior, sd = Vpsi_prior)/dtruncnorm(x = psi, mean = psi_prior, sd = Vpsi_prior)
      proposal_ratio = dnorm(x = psi, mean = psi_candidate, sd = tau)/dnorm(x=psi_candidate, mean = psi, sd = tau)
      #numerator = p_y_numerator * dtruncnorm(x=psi_candidate, mean=psi_prior, sd = Vpsi_prior, a = -1, b = 1) *dnorm(x = psi, mean = psi_candidate, sd = tau)
      #denominator =  dtruncnorm(x=psi, mean=psi_prior, sd = Vpsi_prior, a = -1, b = 1) *dnorm(x = psi_candidate, mean = psi, sd = tau)
      #alpha = min(1, numerator/denominator)
      alpha = min(1, Likelihood_ratio, prior_ratio, proposal_ratio)
    }
    u = runif(1, min = 0, max = 1)
      #print(numerator/denominator)
    if (u <= alpha){
      accept = accept + 1
      psi = psi_candidate
      Omega_posterior[,,s+1] = Omega_draw
      }
    else{
      Omega_posterior[,,s+1] = Omega_posterior[,,s]}
    ##################################################    #####
  }
  A_posterior = A_posterior[,,(S1+1):(S1+S2)]
  Sigma_posterior = Sigma_posterior[,,(S1+1):(S1+S2)]
  Omega_posterior = Omega_posterior[,,(S1+2):(S1+S2+1)]
  print("acceptance rate of the Metropolis Hasting Step: ")
  print(accept/S)
  return(list(A_posterior, Sigma_posterior, Omega_posterior))
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}

p = 1
kappa1 = 0.1^2
kappa2 = 10^2

N = ncol(random_walk_sample)
A_prior     = matrix(0, 1+p*N, ncol = N)
V_prior     = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))
S_prior     = diag(1, N)
nu_prior    = ncol(random_walk_sample)+1
S1 = 30
S2 = 50
vh0_prior = 5
sh0_prior = 0.04
rho_prior = 0.9
Vrho_prior = 0.04
psi_prior = 0
Vpsi_prior = 1

posterior_samples_MA = BVAR_MA_fit(data = random_walk_sample, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)

posterior_samples_A_MA = posterior_samples_MA[[1]]
posterior_samples_Sigma_MA = posterior_samples_MA[[2]]
posterior_samples_Omega_MA = posterior_samples_MA[[3]]
print('Posterior mean of autoregressive parameter:')
round(apply(posterior_samples_A_MA,1:2,mean),3)
print('Posterior standard deviation of autoregressive parameter:')
round(apply(posterior_samples_A_MA,1:2,sd),3)
print('Posterior mean of row specific covariance matrix:')
round(apply(posterior_samples_Sigma_MA,1:2,mean),3)
print('Posterior standard deviation of row specific covariance matrix:')
round(apply(posterior_samples_Sigma_MA,1:2,sd),3)
print('Posterior mean of column specific covariance matrix (first 10 rows and columns):')
round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,mean),3)
print('Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):')
round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,sd),3)
```



### 4.2.1 Empirical Results
**1. Fitted Model Parameters**
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
p = 5
kappa1 = 0.02^2
kappa2 = 10^2
data = final_df[,2:ncol(final_df)]
N = ncol(data)
A_prior = matrix(0,1+p*N,N)
A_prior[2:(N + 1),] = diag(N)
V_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))
S_prior = diag(1, N)
nu_prior  = ncol(data)+1
data = final_df[,2:ncol(final_df)]
#data = final_df[,2:19]
S1 = 100
S2 = 200

vh0_prior = 5
sh0_prior = 0.04
rho_prior = 0.9
Vrho_prior = 0.04
psi_prior = 0
Vpsi_prior = 1
posterior_samples_MA = BVAR_MA_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)

posterior_samples_A_MA = posterior_samples_MA[[1]]
posterior_samples_Sigma_MA = posterior_samples_MA[[2]]
posterior_samples_Omega_MA = posterior_samples_MA[[3]]
print('Posterior mean of autoregressive parameter:')
round(apply(posterior_samples_A_MA,1:2,mean),3)
print('Posterior standard deviation of autoregressive parameter:')
round(apply(posterior_samples_A_MA,1:2,sd),3)
print('Posterior mean of row specific covariance matrix:')
round(apply(posterior_samples_Sigma_MA,1:2,mean),3)
print('Posterior standard deviation of row specific covariance matrix:')
round(apply(posterior_samples_Sigma_MA,1:2,sd),3)
print('Posterior mean of column specific covariance matrix (first 10 rows and columns):')
round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,mean),3)
print('Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):')
round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,sd),3)
```

**2. Prediction Plots ** 
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
# BVAR_MA_predict = function(posterior_samples_A, posterior_samples_Sigma, data, h){
#   S = dim(posterior_samples_A)[3]
#   N =dim(posterior_samples_A)[2]
#   p = (dim(posterior_samples_A)[1] -1) / N
#   Y = (data[(p+1):nrow(data),])
#   x               = Y[(nrow(Y)-p+1):nrow(Y),]
#   x               = x[p:1,]
#   Y_h   = array(NA,c(h,N,S))
#   for (s in 1:S){
#     A     = posterior_samples_A[,,s]
#     Sigma = posterior_samples_Sigma[,,s]
#     N = ncol(A)
#     p = (nrow(A) -1) / N
#     Y = (data[(p+1):nrow(data),])
#     x  = Y[(nrow(Y)-p+1):nrow(Y),]
#     x = x[p:1,]
#     
#     for (i in 1:h){
#       x_T  = c(1,purrr::as_vector(t(x)))
#       Period_Y   = mvtnorm::rmvnorm(1, mean = x_T%*%A, sigma=Sigma)
#       colnames(Period_Y) = colnames(x)
#       x            = rbind(Period_Y,x[1:(p-1),])
#       Y_h[i,,s]   = Period_Y[1:N]
#       }
#   }
#   return(Y_h)
# }
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
# nrow_plot = ceiling(ncol(data)/2)
# par(mfrow=c(nrow_plot,2), mar=rep(3,4),cex.axis=1.5)
# for (i in 1:ncol(data)){
#   plot_CI(prediction = BVAR_prediction[,i,], data = data, final_df = final_df, var_index = i)
# }
```

**2. Forecasts Model Parameters**

**3. Test for Granger Causality**
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
Y = (data[(p+1):nrow(data),])
X = matrix(1,nrow(Y),1) 
for (i in 1:p){
  X     = cbind(X, (data[(p+1):nrow(data)-i,]))
}
X = as.matrix(X)
N = ncol(data)
K = 1+p*N
kappa1 = 0.2^2
kappa2 = 10^2
V_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))
lambda = 1e-4

calc_bayes_factor_MA = function(country1, country2){
  ratio_list = list(dim = S2)
  A_0_mat = apply(posterior_samples_A_MA,1:2,mean)
  dimnames(A_0_mat) = list(c("constant", rep(colnames(data),p)), colnames(data))
  var_list = c('_GDP', '_CPI',  '_XCH')
  
  A_0_df <- as.data.frame(A_0_mat)
  A_0_df$row_names <- rownames(A_0_mat)
  
  if (country1 != 'US'){
    country1 = paste0(country1, var_list)
  }else{country1 = paste0(country1, var_list[1:(length(var_list)-1)])}
  
  if (country2 != 'US'){
    country2 = paste0(country2, var_list)
  }else{country2 = paste0(country2, var_list[1:(length(var_list)-1)])}


for (c1 in country1){
  for (c2 in country2){
    A_0_df[A_0_df$row_names == c2, c1] = 0
  }
}

row_names <- A_0_df$row_names
A_0_mat <- as.matrix(A_0_df[, 1:ncol(A_0_df)-1])
rownames(A_0_mat) <- row_names
  
  for (s in 1:S2){
    posterior_V = solve(t(X)%*%posterior_samples_Omega_MA[,,s]%*%X + solve(V_prior) + diag(lambda, nrow = ncol(X)))
    prior_Sigma = MCMCpack::riwish(v = nu_prior, S = S_prior)
    M = c(A_prior)
    Q = kronecker(prior_Sigma, V_prior)
    A_0 = c(A_0_mat)
    prior_prob =  mvtnorm::dmvnorm(x = A_0, mean = M, sigma = Q, log = TRUE)
    
    M = c(posterior_samples_A_MA[,,s])
    Q = kronecker(posterior_samples_Sigma_MA[,,s], posterior_V)
    posterior_prob = mvtnorm::dmvnorm(x = A_0, mean = M, sigma = Q, log = TRUE)
    ratio_list[s] = posterior_prob - prior_prob
  }
  return(ratio_list)
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
#aa = calc_bayes_factor_MA('CN', 'US')
bf_ma = list()
i = 1
bayesfactor_MA = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))
for (country_a in countries_list){
  for (country_b in countries_list){
    pb$tick()
    #bf_ma[i] = calc_bayes_factor_MA(country_a, country_b)
    i = i+1
    bayesfactor_MA[country_a, country_b] = mean(unlist(calc_bayes_factor_MA(country_a, country_b)))
    gc()
  }
}
```

**4. Test Time Change ** 
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
p = 5
kappa1 = 0.02^2
kappa2 = 10^2
data = final_df[final_df$Time >= '2010 Q1', 2:ncol(final_df)]
N = ncol(data)
A_prior = matrix(0,1+p*N,N)
A_prior[2:(N + 1),] = diag(N)
V_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))
S_prior = diag(1, N)
nu_prior  = ncol(data)+1
data = final_df[,2:ncol(final_df)]
#data = final_df[,2:19]
S1 = 100
S2 = 200

vh0_prior = 5
sh0_prior = 0.04
rho_prior = 0.9
Vrho_prior = 0.04
psi_prior = 0
Vpsi_prior = 1
posterior_samples_MA = BVAR_MA_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)

# posterior_samples_A_MA = posterior_samples_MA[[1]]
# posterior_samples_Sigma_MA = posterior_samples_MA[[2]]
# posterior_samples_Omega_MA = posterior_samples_MA[[3]]
# print('Posterior mean of autoregressive parameter:')
# round(apply(posterior_samples_A_MA,1:2,mean),3)
# print('Posterior standard deviation of autoregressive parameter:')
# round(apply(posterior_samples_A_MA,1:2,sd),3)
# print('Posterior mean of row specific covariance matrix:')
# round(apply(posterior_samples_Sigma_MA,1:2,mean),3)
# print('Posterior standard deviation of row specific covariance matrix:')
# round(apply(posterior_samples_Sigma_MA,1:2,sd),3)
# print('Posterior mean of column specific covariance matrix (first 10 rows and columns):')
# round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,mean),3)
# print('Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):')
# round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,sd),3)

## 4.4  Large BVAR model with MA(1) Gaussian Innovations and Common Stochastic Volatility
### 4.4.1 Model Building Code and Validation 
```
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
bf_ma = list()
i = 1
bayesfactor_MA_new = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))
for (country_a in countries_list){
  for (country_b in countries_list){
    pb$tick()
    #bf_ma[i] = calc_bayes_factor_MA(country_a, country_b)
    i = i+1
    bayesfactor_MA_new[country_a, country_b] = mean(unlist(calc_bayes_factor_MA(country_a, country_b)))
    gc()
  }
}
```
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
p = 5
kappa1 = 0.02^2
kappa2 = 10^2
data = final_df[final_df$Time >= '2010 Q1', 2:ncol(final_df)]
N = ncol(data)
A_prior = matrix(0,1+p*N,N)
A_prior[2:(N + 1),] = diag(N)
V_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))
S_prior = diag(1, N)
nu_prior  = ncol(data)+1
data = final_df[,2:ncol(final_df)]
#data = final_df[,2:19]
S1 = 100
S2 = 200

vh0_prior = 5
sh0_prior = 0.04
rho_prior = 0.9
Vrho_prior = 0.04
psi_prior = 0
Vpsi_prior = 1
posterior_samples_MA = BVAR_MA_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)

posterior_samples_A_MA = posterior_samples_MA[[1]]
posterior_samples_Sigma_MA = posterior_samples_MA[[2]]
posterior_samples_Omega_MA = posterior_samples_MA[[3]]
print('Posterior mean of autoregressive parameter:')
round(apply(posterior_samples_A_MA,1:2,mean),3)
print('Posterior standard deviation of autoregressive parameter:')
round(apply(posterior_samples_A_MA,1:2,sd),3)
print('Posterior mean of row specific covariance matrix:')
round(apply(posterior_samples_Sigma_MA,1:2,mean),3)
print('Posterior standard deviation of row specific covariance matrix:')
round(apply(posterior_samples_Sigma_MA,1:2,sd),3)
print('Posterior mean of column specific covariance matrix (first 10 rows and columns):')
round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,mean),3)
print('Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):')
round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,sd),3)

```
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
bf_ma = list()
i = 1
bayesfactor_MA_new1 = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))
for (country_a in countries_list){
  for (country_b in countries_list){
    pb$tick()
    #bf_ma[i] = calc_bayes_factor_MA(country_a, country_b)
    i = i+1
    bayesfactor_MA_new1[country_a, country_b] = mean(unlist(calc_bayes_factor_MA(country_a, country_b)))
    gc()
  }
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
#if (!require(matrixNormal)) {install_packages('matrixNormal')}
# BVAR_MA_fit <- function(data, p, S1, S2, kappa1, kappa2, A_prior, V_prior, S_prior, nu_prior, N, T, vho_prior, sh0_prior, rho_prior, Vrho_prior, psi_prior, Vpsi_prior){
#   N = ncol(data)
#   Ty = nrow(data)
#   X = matrix(1,Ty,1)
#   for (i in 1:p){
#     X     = cbind(X,data[p+1:Ty-i,])
#   }
#   S = S1 + S2
#   K = 1+p*N
#  
#   nu_bar = Ty + nu_prior
#   #S_bar  = S_prior + t(data)%*%data + t(A_prior)%*%diag(1/diag(V_prior))%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
#  
#   #Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)
# 
#  
#   sigma_h_squared = rinvgamma(n = 1, alpha = vh0_prior, beta = sh0_prior)
#   rho = rtnorm(n = 1, mean = rho_prior, sqrt(Vrho_prior), a = -1, b = 1)
#   u_h = rnorm(n = Ty, mean = 0, sd = sqrt(sigma_h_squared))
#   h = array(dim = Ty)
#   u = array(dim = c(N,1,Ty))
#   h [1] = rho * 0 + u_h[1]
#   psi = rtnorm(n = 1, mean = psi_prior, sd = sqrt(Vpsi_prior), a = -1, b = 1)
#   epsilon = array(dim = c(N,1,Ty))
#   Omega = array(0, dim = c(Ty,Ty))
#     
#   for (t in 2:Ty){
#     h[t] = rho*h[t-1] + u_h[t]
#   } 
#   
#   Sigma_prior = rWishart(n = 1, df = nu_prior, Sigma = S_prior)[,,1]
#   Sigma_prior = solve(Sigma_prior)
#   
#   for (t in 1:Ty){
#     u_var = exp(h[t])*Sigma_prior
#     u[,,t] = mvrnorm(n = 1, mu = rep(0,N), Sigma = u_var)
#     if (t == 1){
#       epsilon[,,t] = u[,,t]
#       Omega[t,t] = (1+psi^2)*exp(h[t])
#     }
#     else{
#       epsilon[,,t] = u[,,t] + psi*u[,,t-1]
#       Omega[t,t] = psi^2*exp(h[t-1]) + exp(h[t])
#       Omega[t-1,t] = psi * exp(h[t-1])
#       Omega[t,t-1] = psi * exp(h[t-1]) 
#     }
#   }
#   
#   V_bar = solve(t(X)%*%solve(Omega)%*%X + solve(V_prior))
#   A_bar = V_bar%*%(t(X)%*%solve(Omega)%*%data + solve(V_prior)%*%A_prior)
#   nu_bar = Ty + nu_prior
#   #S_bar  = S_prior + t(data)%*%data + t(A_prior)%*%diag(1/diag(V_prior))%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
#   S_bar = S_prior +  t(data)%*%solve(Omega)%*%data + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
#   S_bar_inv = solve(S_bar)
#   #Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)
#   Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)
#   Sigma_posterior = apply(Sigma_posterior,3,solve)
#   Sigma_posterior  = array(Sigma_posterior,c(N,N,S))
#   A_posterior = array(rnorm(prod(c(dim(A_bar),S))),c(dim(A_bar),S))
#   L = t(chol(V_bar))
#   
#   for (s in 1:S){
#     A_posterior[,,s]= A_bar + L%*%A_posterior[,,s]%*%chol(Sigma_posterior[,,s])
#     }
#   
#   A_posterior = A_posterior[,,(S1+1):(S1+S2)]
#   Sigma_posterior = Sigma_posterior[,,(S1+1):(S1+S2)]
#   return(list(A_posterior, Sigma_posterior))
# }
```


```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
# #Code for S draws f Omega
# 
# p = 1
# kappa1 = 0.1^2
# kappa2 = 10^2
# N = ncol(random_walk_sample)
# A_prior     = matrix(0, 1+p*N, ncol = N)
# V_prior     = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))
# S_prior     = diag(1, N)
# nu_prior    = ncol(random_walk_sample)+1
# vh0_prior = 5
# sh0_prior = 0.04
# rho_prior = 0
# Vrho_prior = 0.04
# psi_prior = 0
# Vpsi_prior = 1
# 
# data = random_walk_sample
# N = ncol(data)
# Ty = nrow(data)
# X = matrix(1,Ty,1)
# S1 = 50
# S2 = 50
# S = S1 + S2
# set.seed(123)
# for (i in 1:p){
#   X     = cbind(X,data[p+1:Ty-i,])
# }
# 
# #dim 1 variables
# sigma_h_squared = rinvgamma(n = S, alpha = vh0_prior, beta = sh0_prior)
# rho = rtnorm(n = S, mean = rho_prior, sqrt(Vrho_prior), a = -1, b = 1)
# psi = rtnorm(n = S, mean = psi_prior, sd = sqrt(Vpsi_prior), a = -1, b = 1)
# 
# #dim NxN variables
# Sigma_prior = array(dim = c(N,N,S))
# 
# #dim Tx1  variables
# u_h = array(dim = c(Ty,1,S))
# h = array(dim = c(Ty,1,S))
# 
# # dim T by N variables
# u = array(dim = c(N,1,Ty, S))
# epsilon = array(dim = c(N,1,Ty, S))
# uvar = array(dim = c(N,N,Ty,S))
# #dim TxT variables
# Omega = array(0, dim = c(Ty,Ty,S))
# 
# for (s in 1:S){
#   u_h[,,s] = rnorm(n = Ty, mean = 0, sd = sqrt(sigma_h_squared[s]))
#   h[1,1,s] = rho[s] * 0 + u_h[1,1,s]
#   Sigma_prior[,,s] = rWishart(n = 1, df = nu_prior, Sigma = S_prior)[,,1]
#   Sigma_prior[,,s] = solve(Sigma_prior[,,s])
#   for (t in 2:Ty){
#     h[t,1,s] = rho[s]*h[t-1,1,s] + u_h[t,1,s]
#   }
#   for (t in 1:Ty){
#     uvar[,,t,s] = exp(h[t,1,s])*Sigma_prior[,,s]
#     u[,,t,s] = mvrnorm(n=1,mu = rep(0,N), Sigma = uvar[,,t,s])
#     if(t==1){
#       epsilon[,,t,s] = u[,,t,s]
#       Omega[t,t,s] = (1-psi[s]^2)*exp(h[t,1,s])
#     }
#     else{
#       epsilon[,,t,s] = u[,,t,s] + psi[s]*u[,,t-1,s]
#       Omega[t,t,s] = psi[s]^2 * exp(h[t-1,1,s])+exp(h[t,1,s])
#       Omega[t-1,t,s] = psi[s] * exp(h[t-1,1,s])
#       Omega[t,t-1,s] = psi[s] * exp(h[t-1,1,s])
#     }
#   }
# }
# 
# 
# 
# for (s in 1:S){
#   V_bar = solve(t(X)%*%solve(Omega[,,s])%*%X + solve(V_prior))
#   A_bar = V_bar%*%(t(X)%*%solve(Omega[,,s])%*%data + solve(V_prior)%*%A_prior)
#   nu_bar = Ty + nu_prior
#   #S_bar  = S_prior + t(data)%*%data + t(A_prior)%*%diag(1/diag(V_prior))%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
#   S_bar = S_prior +  t(data)%*%solve(Omega[,,s])%*%data + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
#   S_bar_inv = solve(S_bar)
#   #Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)
#   Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)
#   Sigma_posterior = apply(Sigma_posterior,3,solve)
#   Sigma_posterior  = array(Sigma_posterior,c(N,N,S))
#   A_posterior = array(rnorm(prod(c(dim(A_bar),S))),c(dim(A_bar),S))
#   L = t(chol(V_bar))
#   A_posterior[,,s]= A_bar + L%*%A_posterior[,,s]%*%chol(Sigma_posterior[,,s])
#   }
# A_posterior = A_posterior[,,(S1+1):(S1+S2)]
# Sigma_posterior = Sigma_posterior[,,(S1+1):(S1+S2)]
# 
# round(apply(A_posterior,1:2,mean),3)
# round(apply(A_posterior,1:2,sd),3)
# round(apply(Sigma_posterior,1:2,mean),3)
# round(apply(Sigma_posterior,1:2,sd),3)
```
### 4.4.2 Empirical Result

```


<!-- Furthermore, a non-Gaussian framework allows the modeling of asymmetries in how economies react to positive versus negative shocks. Traditional BVAR models with Gaussian assumptions is often unable to capture such nuances, while flexible error structures can incorporate the differing impacts of shocks of different magnitudes or directions. 
economic variables are often characterized by distributions that are far from Gaussian, often featuring fat tails and skewness. Secondly, -->