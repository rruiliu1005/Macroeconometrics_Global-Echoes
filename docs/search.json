[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "",
    "text": "Objective: Construct a Bayesian Vector Autoregression model to forecast major macroeconomic indicators for the United States, Australia, Japan, and China to facilitate an investigation into the prospective interdependencies between the economies of these nations.\nQuestion: This research project will examine how trade relationships, investment flows, monetary policy environments, and economic performances within the United States, Australia, Japan and China mutually influence each other, and assess the implications of these interactions for predicting future values of these economic indicators.\nMotivation: Since the onset of the COVID-19 pandemic, the global economic landscape has witnessed a series of unprecedented shifts in key macroeconomic indicators, spurred by governments‚Äô adoption of varied expansionary monetary policies. Initially, to buffer their economies, many nations implemented expansive monetary strategies, later swiftly transitioning to interest rate hikes in a bid to manage surging inflation rates‚Äîa scenario not seen in decades. The pandemic‚Äôs disruption to trade further exacerbated inflationary pressures for some economies, highlighting the intricate interdependencies among major economies with significant trade and financial ties. This period recorded stark contrast in inflation levels, with unprecedented highs in the US and Australia and notably low inflation in China and Japan. Amidst this turmoil, a divergence in economic paths also became apparent, thei9 United States and Australia have witness robust economic rebounds, whereas China and Japan saw more tepid recoveries. This research aims to dissect the nuanced web of economic interdependencies between the United States, Australia, Japan, and China, analyzing how their trade relationships, investment flows, and monetary policy environments have mutually influenced their economic performances. Additionally, it seeks to understand the ramifications of these dynamics for the predictive accuracy of future economic indicators, offering insights into the evolving global economic order."
  },
  {
    "objectID": "index.html#data-plots",
    "href": "index.html#data-plots",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "Data Plots",
    "text": "Data Plots\n\nlayout &lt;-   (XCH_plot) / (GDP_plot)/ ( CPI_plot)\n#layout &lt;-   (BOP_plot | XCH_plot) / (GDP_plot | FDI_plot) / (CPI_plot)\n#FDI_original_plot | BOP_original_plot) / (XCH_original_plot | GDP_original_plot)\nlayout"
  },
  {
    "objectID": "index.html#stationarity-check",
    "href": "index.html#stationarity-check",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "Stationarity Check",
    "text": "Stationarity Check\nStationary Tests\nThis section employs the Augmented Dickey-Fuller test to examine the null hypothesis that a unit root is present in these time series. A lag order of 4 was chosen for the ADF test given our data‚Äôs quarterly frequency. The results indicate that most of the macroeconomic indicators possess a unit root and are not stationary, with a few exceptions. Specifically, we have enough evidence to reject the null hypothesis for the CPI series in China and the United States at a 5% significance level. Interestingly, Japan‚Äôs GDP series is found to be unit-root stationary at this significance level. This implies that Japan‚Äôs GDP exhibits a consistent mean and variance over time, contrasting with the general trend observed in other national GDP series. Japan‚Äôs GDP growth has been known to be stagnant for quite some years and it‚Äôs macroeconomic indicators may respond differently to economic shocks compared to its non-stationary counterparts\n\n#install.packages(\"tseries\")\nperform_adf_test &lt;- function(series) {\n  adf_test &lt;- adf.test(series, alternative = \"stationary\", k = 4)\n  list(\n    statistic = adf_test$statistic,\n    lag.order = as.numeric(adf_test$parameter),\n    p.value = adf_test$p.value\n  )\n}\n\nresults &lt;- sapply(CPI_data[,2:5], perform_adf_test)\nresults_df &lt;- as.data.frame(t(results), stringsAsFactors = FALSE)\nnames(results_df) &lt;- c(\"Dickey-Fuller Statistic\", \"Lag Order\", \"P-value\")\nresults_df$Country &lt;- rownames(results_df)\nrownames(results_df) &lt;- NULL\nkable(results_df, caption = \"ADF Test Results for CPI Data by Country\")\n\n\nADF Test Results for CPI Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-3.258408\n4\n0.08079872\nJP_CPI\n\n\n-4.299916\n4\n0.01\nCN_CPI\n\n\n-3.55799\n4\n0.0394958\nUS_CPI\n\n\n-3.333834\n4\n0.06822768\nAU_CPI\n\n\n\n\n\n\nresults &lt;- sapply(XCH_data[2:nrow(XCH_data),c(2,3,5)], perform_adf_test)\nresults_df &lt;- as.data.frame(t(results), stringsAsFactors = FALSE)\nnames(results_df) &lt;- c(\"Dickey-Fuller Statistic\", \"Lag Order\", \"P-value\")\nresults_df$Country &lt;- rownames(results_df)\nrownames(results_df) &lt;- NULL\nkable(results_df, caption = \"ADF Test Results for Exchange Rate (% change) Data by Country\")\n\n\nADF Test Results for Exchange Rate (% change) Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-1.791111\n4\n0.6635047\nJP_XCH\n\n\n-1.565957\n4\n0.7575635\nCN_XCH\n\n\n-2.56175\n4\n0.3415666\nAU_XCH\n\n\n\n\n\n\nresults &lt;- sapply(adjusted_GDP_data[2:nrow(adjusted_GDP_data),2:5], perform_adf_test)\nresults_df &lt;- as.data.frame(t(results), stringsAsFactors = FALSE)\nnames(results_df) &lt;- c(\"Dickey-Fuller Statistic\", \"Lag Order\", \"P-value\")\nresults_df$Country &lt;- rownames(results_df)\nrownames(results_df) &lt;- NULL\nkable(results_df, caption = \"ADF Test Results for seasonally adjusted Data by Country\")\n\n\nADF Test Results for seasonally adjusted Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-3.499383\n4\n0.04535076\nJP_GDP\n\n\n-2.472818\n4\n0.3802514\nCN_GDP\n\n\n0.7030938\n4\n0.99\nUS_GDP\n\n\n-2.030497\n4\n0.5639204\nAU_GDP\n\n\n\n\n\nThe ACF plots of seasonally adjusted GDP data shows a strong autocorrelation at smaller lags and the autocorrelation decreases sharply as the lags increase. It is worth noting that the autocorrelations decreases much faster for Japan than for the other countries. Similar phenomenon is observed with exchange rates against the dollar for all three countries. It is worth noting that the autocorrelations for the Chinese series a more persistent than the others, possibly due to the fact that China has a managed floating exchange rate system, where the Chinese central banks sets a target exchange rate. The CPI data for all four countries shows generally low autocorrelation across lags, particularly beyond the first few periods. This suggests that CPI measurements are relatively independent from quarter to quarter. The alternating signs in the autocorrelation function (ACF) plot for Japan and US‚Äôs CPI data suggest possible seasonal effects, cyclical patterns or due to data collection methods.\n\npar(mfrow = c(3, 4))\nacf(adjusted_GDP_data[,2],  main = colnames(adjusted_GDP_data)[2])\nacf(adjusted_GDP_data[,3],  main = colnames(adjusted_GDP_data)[3])\nacf(adjusted_GDP_data[,4],  main = colnames(adjusted_GDP_data)[4])\nacf(adjusted_GDP_data[,5],  main = colnames(adjusted_GDP_data)[5])\n\nacf(XCH_data[,2], main = colnames(XCH_data)[2])\nacf(XCH_data[,3], main = colnames(XCH_data)[3])\nplot.new()\ntitle(main = colnames(XCH_data)[4], cex.main = 1)\nacf(XCH_data[,5], main = colnames(XCH_data)[5])\n\nacf(CPI_data[,2], main = colnames(CPI_data)[2])\nacf(CPI_data[,3], main = colnames(CPI_data)[3])\nacf(CPI_data[,4], main = colnames(CPI_data)[4])\nacf(CPI_data[,5], main = colnames(CPI_data)[5])"
  },
  {
    "objectID": "index.html#model-1-standard-bvarp-model",
    "href": "index.html#model-1-standard-bvarp-model",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.1 Model 1: Standard BVAR(p) Model",
    "text": "3.1 Model 1: Standard BVAR(p) Model\n\n3.1.1 Model Specification\n\\[ Y = XA+E\\] \\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, I_T)\\] \\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\nwhere\n\n\\(T\\) is the number of time periods under consideration\n\\(N\\) is the number of variables, in our case, N = 20\n\\(P\\) is the number of lags\n\\(Y\\) is a \\(T \\times N\\) matrix of variables of response variables we aim to model.\n\\(A\\) is a \\(K \\times N\\) matrix of coefficients, \\(K = (1+ùëù\\times N)\\).\n\\(E\\) is a \\(T \\times N\\) matrix of the error terms\n\\(X\\) is a \\(T \\times (1+ùëù\\times N)\\) matrix of covariates\n\\(\\Sigma\\) is a \\(N \\times N\\) matrix representing the row-specific covariance matrix\n\\(I_T\\) is a \\(T \\times T\\) identity matrix representing the column specific covariance matrix\n\\(E|X\\) follows a matrix-variate normal distribution with mean \\(0_{T \\times N}\\), row specific covariance matrix \\(\\Sigma_{N \\times N}\\) and column specific covariance matrix \\(I_T\\)\n\\(x_{t}^T = \\left( \\begin{array}{cccc}1  & y_{t-1} & y_{t-2} & \\cdots & y_{t-p} \\end{array} \\right)\\)\n\nIn our specific application, the Y matrix is formulated as follows:\n\\[\nY = \\begin{pmatrix}\n    \\text{CPI}_{\\text{CN}, p+1} & \\text{XCH}_{\\text{CN}, p+1} & \\log(\\text{GDP})_{\\text{CN}, p+1} & \\text{CPI}_{\\text{US}, p+1} & \\log(\\text{GDP})_{\\text{US}, p+1} & \\text{CPI}_{\\text{JP}, p+1} & \\text{XCH}_{\\text{JP}, p+1} & \\log(\\text{GDP})_{\\text{JP}, p+1} & \\text{CPI}_{\\text{AU}, p+1} & \\text{XCH}_{\\text{AU}, p+1} & \\log(\\text{GDP})_{\\text{AU}, p+1} \\\\\n    \\text{CPI}_{\\text{CN}, p+2} & \\text{XCH}_{\\text{CN}, p+2} & \\log(\\text{GDP})_{\\text{CN}, p+2} & \\text{CPI}_{\\text{US}, p+2} & \\log(\\text{GDP})_{\\text{US}, p+2} & \\text{CPI}_{\\text{JP}, p+2} & \\text{XCH}_{\\text{JP}, p+2} & \\log(\\text{GDP})_{\\text{JP}, p+2} & \\text{CPI}_{\\text{AU}, p+2} & \\text{XCH}_{\\text{AU}, p+2} & \\log(\\text{GDP})_{\\text{AU}, p+2} \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n    \\text{CPI}_{\\text{CN}, T} & \\text{XCH}_{\\text{CN}, T} & \\log(\\text{GDP})_{\\text{CN}, T} & \\text{CPI}_{\\text{US}, T} & \\log(\\text{GDP})_{\\text{US}, T} & \\text{CPI}_{\\text{JP}, T} & \\text{XCH}_{\\text{JP}, T} & \\log(\\text{GDP})_{\\text{JP}, T} & \\text{CPI}_{\\text{AU}, T} & \\text{XCH}_{\\text{AU}, T} & \\log(\\text{GDP})_{\\text{AU}, T}\n\\end{pmatrix}\n\\]\nThe Bayesian Vector Autoregression model as formulated above provides a robust framework for investigating the relationships among selected economic indicators across different nations. By employing this model, this research aims to quantitatively measure the influence of one country‚Äôs economic indicators on another, such as how lagged changes in China‚Äôs i consumer price index may influence the GDP growth rate of the United States and vice versa. The BVAR model, with its estimation of coefficients across various lags, offers a deep understanding of both immediate and more delayed economic interactions, which is crucial to analyzing the cyclical nature of trade relationships, investment flows, monetary policy environments, and economic performances and the transmission of these metrics across borders.\nThe strength of this BVAR model lies in its ability to incorporate prior economic knowledge and beliefs into the estimation process. By setting prior distributions for the matrix of coefficients A and the covariance matrix \\(\\Sigma\\), the model can be tailors to reflect established economic theories regarding international economic linkages and the time it takes for policy changes in one country to affect another. By calibrating the prior variances, particularly for the autoregressive coefficients, we can integrate prior knowledge or hypotheses, such as the presence of unit roots or the diminishing influence of distant lags on current values, into the analysis. When interpreting the estimation output, attention will be given to the posterior means and variances of the coefficients, which represent the model‚Äôs ‚Äúlearnt‚Äù understanding of the underlying economic structure. The analysis will be supplemented by forecast error variance decompositions to better understand the proportion of the movements in economic indicators that can be accounted for by their own shocks versus shocks to other variables.\nThe economic context underscoring this analysis is the increased globalization over the past decade, marking an era where economies are more intertwined than ever through trade, capital flows, and policy decisions. This period has witnessed not only the strengthening of global economic ties but also recent calls from political leaders advocating for a reduction in globalization. These contrasting dynamics highlight the complexity of the current global economic landscape, where the push for deeper integration coexists with growing sentiments for retrenchment. This dual trend sets the stage for our investigation, providing a rich context to explore how economic variables across nations influence each other amidst fluctuating levels of global interconnectedness. In this environment, understanding the cross-country spillover effects is vital for policymakers and businesses alike, as decisions made in one country can have far-reaching implications. By addressing these aspects, this research will contribute to the discourse on economic policy formulation, risk assessment, and strategic planning.\n\n\n3.1.2 Prior Settings\nWe will employ a Normal-Inverse Wishart distribution for the joint distribution of coefficient matrices A and the row-specific variance matrix \\(\\Sigma\\), and a Minnesota prior on the coefficients A. Specifically, we have:\n\\[\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0) \\]\n\\[p(\\Sigma) \\propto |\\Sigma|^{-\\frac{\\nu_0+N+1}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_0))\\]\n\\[A|\\Sigma \\sim \\mathcal{MN}_{K \\times N}(A_0, \\Sigma, V_A)\\]\n\\[p(A|\\Sigma) \\propto |\\Sigma|^{-\\frac{K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^T(A-A_0)))\\]\n\\[(A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N} (A_0, V_A, S_0, \\nu_0)\\]\n\\[p(A,\\Sigma) \\propto |\\Sigma|^{-\\frac{K+\\nu_0+N+1}{2}} \\times exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_0))\\times exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^T(A-A_0)))\\]\nwhere\n\\[V_A = diag(\\kappa_2 \\quad \\kappa_1(\\mathbf{p} \\otimes I_N^T))\\]\n\\[\\mathbf{p} = [1 \\quad 2 \\quad ... \\quad p]\\] \\[I_N = [1 \\quad 1 \\quad ... \\quad 1] \\in \\mathbb{R}^N\\] \\[\\kappa_1 \\text{ is the overall shrinkage level for autoregressive slopes}\\] \\[\\kappa_2 \\text{ is the overall shrinkage lvel for the constant term }\\]\nAdditionally, we adopt commonly used values for the hyperparameters as established in the literature.\n\\[A_0 = 0\\]\n\\[v_0 = N+3\\]\n\\[S_0 = I_N\\]\n\\[\\kappa_1 = 0.2^2 \\quad \\kappa_2 = 10^2\\]\nThe hyperparameters \\(\\kappa_1\\) and \\(\\kappa_2\\) are specified in a way such that the coefficient associated with a lag l variable is shrunk more heavily as lag length increases whereas the intercepts are not shrunk to 0.\n\n\n3.1.3 Posterior Distributions\nThe posterior distribution specified above has the form\n\\[p(Y|A, \\Sigma) = (2\\pi)^{-\\frac{TN}{2}}|\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T(Y-XA))\\]\nand the joint posterior distribution\n\\[p(A, \\Sigma \\mid Y)  = \\frac{p(A,\\Sigma,Y)}{p(Y)}\\propto p(A, \\Sigma, Y) \\propto p(Y|A,\\Sigma)\\times p(A,\\Sigma) =  p(Y|A,\\Sigma) p(A \\mid \\Sigma) p(\\Sigma)\\]\n\\[\\propto |\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T(Y-XA))) \\times\\]\n\\[\\mid \\Sigma \\mid^{-\\frac{\\nu_0+N+K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_o))exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\\]\n\\[\\propto |\\Sigma|^{-\\frac{T+N+K+\\nu_0+1}{2}} \\times \\exp (-\\frac{1}{2}tr(\\Sigma^{-1}S_0)) \\times \\]\n\\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^TY-\\bar{A}^T\\bar{V}^{-1}\\bar{A})\\times\\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-\\bar{A})^T\\bar{V}^{-1}(A-\\bar{A})))\\] Hence,\n\\[p(A, \\Sigma \\mid Y, X) = p(A \\mid Y, X, \\Sigma) p(\\Sigma \\mid Y, X)\\]\n\\[p(A \\mid Y, X, \\Sigma) = \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\]\n\\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\]\nwhere\n\\[\\bar{V} = (X^TX + V_A^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X^TY + V_A^{-1}A_0)\\]\n\\[\\bar{\\nu} = T + \\nu_0\\]\n\\[\\bar{S} = S_0 + Y^TY + A_0^TV_A^{-1}A_0 - \\bar{A}^T\\bar{V}^{-1}\\bar{A}\\]\n\n\n3.1.4 Estimation Procedure\nIn this setting, we have\n\n\\((A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N} (A_0, V_A, S_0, \\nu_0)\\)\n\nthen, prior draws can be sampled from\n\n\\(\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0)\\)\n\\(A|\\Sigma \\sim \\mathcal{MN}_{K \\times N}(A_0, \\Sigma, V_A)\\)\n\nwe use the following Gibb‚Äôs Sampler algorithm to sample from the posterior distribution:\n\nInitialize \\(\\Sigma\\) at \\(\\Sigma^0\\)\nFor \\(s = 1,...,S_1+S_2\\)\n\nDraw a sample \\(A^{(s)}\\) from \\(p(A \\mid Y, X, \\Sigma^{(s-1)}) \\sim \\mathcal{MN}_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\)\nDraw a sample \\(\\Sigma^{(s)}\\) from \\(p(\\Sigma \\mid Y, X, A^{(s)}) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\)\n\n\nWe discard the first \\(S_1\\) sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain \\(S_2\\) sampled draws from the joint posterior distribution.\n\\[\\left\\{A^{(s)}, \\Sigma^{(s)}\\right\\}_{s=S_1+1}^{S_1+S_2}\\]\nThe draws from joint predictive density can then be obtained using the following algorithm:\n\nSample S draws \\(\\left\\{ A^{(s)}, \\Sigma^{(s)} \\right\\}_{s=1}^{S}\\) from \\(p(A,\\Sigma|Y, X)\\)\nSample S draws \\(\\left\\{ Y_{t+h}^{(s)} \\right\\}_{s=1}^S\\) from \\(Y_{t+h}^{(s)} \\sim \\mathcal{N}_{hN}(Y_{t+h|t}(A^{(s)}), \\mathbb{V}ar[Y_{t+h|t}|A^{(s)}, \\Sigma^{(s)}])\\)\n\nwhere\n\\[\\underset{\\text{hN} \\times1}{Y_{t+h}} = \\begin{pmatrix}\n    Y_{t+1} \\\\\n    Y_{t+2} \\\\\n    \\vdots    \\\\\n    Y_{t+h}\n\\end{pmatrix}\\]\nTo derive the posterior predictive density for \\(Y_{t+h}\\), we first note that:\n\\[p(Y_{t+h}|Y_{t}) = \\int \\int p(Y_{t+h}|Y_{t},A, \\Sigma)\\times p(A,\\Sigma|Y,X)dAd\\Sigma\\] where\n\\[p(y_{t+h}|Y, X, A, \\Sigma, I_T) \\sim \\mathcal{N}_{hN} (Y_{t+h|t}(A), Var(Y_{t+h}|A, \\Sigma))\\]\n\\[Y_{t+h|t}(A) =\\begin{pmatrix}\n    Y_{t+1|t} \\\\\n    Y_{t+2|t} \\\\\n    \\vdots    \\\\\n    Y_{t+h|t}\n\\end{pmatrix} = \\begin{pmatrix}  \n\\mu_0 + A_1 Y_t + \\cdots + A_pY_{t-p+1|t} \\\\\n\\mu_0 + A_1 Y_{t+1|t} + \\cdots + A_p Y_{t-p+2|t}\\\\\n\\vdots \\\\\n\\mu_0 + A_1 Y_{t+h-1|t} + \\cdots + A_p Y_{t+h-p|t}\n\\end{pmatrix}\\]\n\\[\\underset{\\text{(hN)} \\times \\text{(hN)}}{\\mathbb{V}ar(Y_{t+h|t}|A, \\Sigma)} = \\begin{pmatrix}\n    \\Sigma & \\Sigma \\phi_1^T & \\cdots & \\Sigma \\phi_{h-1}^T \\\\\n    \\phi_1\\Sigma &\\Sigma + \\phi_1 \\Sigma \\phi_1^T  & \\cdots & \\Sigma \\phi_1^T + \\phi_1 \\Sigma \\phi_2^T+ \\cdots + \\phi_{h-2}\\Sigma \\phi_{h-1}^T + \\phi_{h-1} \\Sigma \\phi_{h}^T \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\phi_{h-1}\\Sigma\\ & \\phi_1\\Sigma  + \\phi_2 \\Sigma \\Phi_1^T + \\cdots + \\phi_{h} \\Sigma \\phi_{h-1}^T & \\cdots & \\Sigma + \\phi_1 \\Sigma \\phi_1^T + \\cdots+\\phi_{h-2}\\Sigma\\phi_{h-2}^T+\\phi_{h-1}\\Sigma\\phi_{h-1}^T\n\\end{pmatrix}\n\\]\nwhere \\(\\phi_i = J A^i J'\\) are the parameters of the VMA(\\(\\infty\\)) representation of VAR(\\(p\\)) and \\(A\\) is the parameter matrix of \\(VAR(1)\\) representation of VAR(\\(p\\)). \\(A^i\\) is the matrix \\(A\\) raised to the power of \\(i\\). \\[\n\\underset{Np \\times Np}{A} =\n\\begin{pmatrix} A_1 & A_2 & \\cdots & A_{p-1} & A_p \\\\\nI_N & 0_{N \\times N} & \\cdots & 0_{N \\times N} & 0_{N \\times N}  \\\\\n0 & I_N  & \\cdots & 0 & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & \\cdots & I_N & 0 \\end{pmatrix}\n\\qquad\n\\underset{\\text{N} \\times \\text{Np}}{J} = [I_N \\quad 0_{N \\times N(p-1)}]\n\\] \\[p(A|Y, X, \\Sigma) \\sim \\mathcal{MN}_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\] \\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\] as before. In this approach, we are incorporating into our prediction the uncertainties in the estimates of the parameters by integrating over \\(A\\) and \\(\\Sigma\\).\n\n\n3.1.5 Test for Granger Causality\nGranger causality testing, introduced in Granger (1969), is a way to measure whether the past and current values of one time series contains useful information for predicting future values of another. The idea behind granger causality testing is that if a time series does contain predictive information about another, incorporating it into the model should reduce the error variance and improve the precision of the forecasts. The classical approach for testing causality is the Wald test, but it fails to quantify the degree of evidence in the data in favour of or against the causality hypothesis. The Bayesian approach to testing Granger causality based on Baye‚Äôs factor, on the other hand, has the ability to quantify the evidence for and against the hypothesis with a single number. The Baye‚Äôs factor is defined as:\n\\[B_H = \\frac{p[H_0|Y]}{p[H_1|Y]}\\] In the context of testing for VAR(p) models, we have the marginal likelihood under the assumption of no Granger causality is:\n\\[p_0(Y) = \\int p_1(Y| A_{ij} = 0, A_{-(ij)})dA_{-(ij)} = p_1(Y|A_{ij}=0)\\] using Baye‚Äôs rule, we have: \\[p_0(Y) = \\frac{p_1(A_{ij} = 0|Y) \\times p_1(Y)}{p_1(A_{ij} = 0)}\\] We divide the above equation by \\(p(y_t|H_1)\\) to get Baye‚Äôs factor:\n\\[B_H = \\frac{p_0(Y)}{p_1(Y)} = \\frac{p_1(A_{ij} = 0|Y)}{p_1(A_{ij} = 0)}=\\frac{\\int p_1(A_{ij=0}|Y, \\Sigma)d \\Sigma}{\\int p(A_{ij}=0, \\Sigma)d\\Sigma}\\]\nand the integration is performed using numerical integration methods. The second part is also known as the Savage-Dickey density ratio estimator, which is the ratio of the posterior over the prior density evaluated at \\(A_{ij} = 0\\). Indicative values for interpreting Bayes factors is provided below:\nTo test for Granger causality, we use the equation: \\[B_H = \\frac{p_1(A_{ij} = 0|Y)}{p(A_{ij}=0)} \\] A value much greater than 1 would indicate that the posterior distribution has a higher probability density at \\(A_{ij} = 0\\) than the prior, hence the data provides evidence for the null hypothesis that \\(A_{ij} = 0\\). Conversely, a value much smaller than 1 would suggest that the data provides evidence against the null hypothesis."
  },
  {
    "objectID": "index.html#model-2-large-bvar-model-with-ma1-gaussian-innovations",
    "href": "index.html#model-2-large-bvar-model-with-ma1-gaussian-innovations",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.2 Model 2: Large BVAR model with MA(1) Gaussian Innovations",
    "text": "3.2 Model 2: Large BVAR model with MA(1) Gaussian Innovations\nIncorporating MA(1) Gaussian innovations in a large BVARs model can lead to a significant enhancement over traditional BVAR models, especially when forecasting macroeconomic variables, for several reasons. By allowing for serial correlation in the innovations term, we will be able to capture the momentum or persistence in economic variables that is often observed in real-world data. Recognizing that shocks may have a lasting impact over several periods can enhance the model‚Äôs ability to predict future values by considering the path-dependent nature of the economy. In addition, our BVAR model with common stochastic volatility is a natural extension to the standard BVAR model and formulated as below:\n\n3.2.1 Model Specification\n\\[ Y = XA+E\\] \\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, \\Omega_{T \\times T})\\] \\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\nwhere\n\n\\(T\\) is the number of time periods under consideration\n\\(N\\) is the number of variables, in our case, N = 20\n\\(P\\) is the number of lags\n\\(Y\\) is a \\(T \\times N\\) matrix of variables of response variables we aim to model.\n\\(A\\) is a \\(K \\times N\\) matrix of coefficients, \\(K = (1+ùëù\\times N)\\).\n\\(E\\) is a \\(T \\times N\\) matrix of the error terms\n\\(X\\) is a \\(T \\times (1+ùëù\\times N)\\) matrix of covariates\n\\(\\Sigma\\) is a \\(N \\times N\\) matrix representing the row-specific covariance matrix\n\\(\\Omega\\) is a \\(T \\times T\\) identity matrix representing the column specific covariance matrix\n\\(E|X\\) follows a matrix-variate normal distribution with mean \\(0_{T \\times N}\\), row specific covariance matrix \\(\\Sigma_{N \\times N}\\) and column specific covariance matrix \\(I_T\\)\n\nwith MA(1) innovations, we have, for i = 1,‚Ä¶.,N and t = 1,‚Ä¶,T, \\[e_{t, i} = \\eta_{t, i} + \\psi \\eta_{t-1, i}\\] where \\(|\\psi|&lt;1\\) and \\(\\eta_{t,i} \\sim \\mathcal{N}(0,1)\\)\nIn matrix notation, we have:\n\\[e_i = H_{\\psi} \\eta_i\\]\nwhere\n\\[\ne_i = \\begin{bmatrix}\ne_{1, i} \\\\\ne_{2, i} \\\\\n\\vdots \\\\\ne_{T, i}\n\\end{bmatrix} \\qquad\n\\eta_i = \\begin{bmatrix}\n\\eta_{1, i} \\\\\n\\eta_{2, i} \\\\\n\\vdots \\\\\n\\eta_{T, i}\n\\end{bmatrix} \\qquad\nH_{\\psi} = \\left(\n\\begin{array}{cccc}\n1 & 0  & \\cdots & 0 \\\\\n\\psi & 1  & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\psi & 1\n\\end{array}\n\\right) \\qquad\nO_{\\psi} = diag(1+\\psi^2, 1, ..., 1)\n\\]\nHence, we have\n\\[E \\sim \\mathcal{N}(0, H_{\\psi}O_{\\psi}H_{\\psi}^T) \\qquad \\qquad \\Omega = H_{\\psi}O_{\\psi}H_{\\psi}^T\\] Note that the covariance matrix \\(\\Omega\\) depends on \\(\\psi\\) only.\n\n\n3.2.2 Prior Specification\nWe consider a prior independent distributions for \\((A, \\Sigma, \\Omega)\\), specifically, we have: \\[P(A, \\Sigma, \\Omega) = P(A, \\Sigma) \\times P(\\Omega)\\]\nWe will employ a Normal-Inverse Wishart distribution for the joint distribution of A and \\(\\Sigma\\) and before, \\[(A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N}(A_0, V_A, S_0, \\nu_0)\\]\nWe apply a truncated normal prior on \\(\\psi\\), \\[\\psi \\sim \\mathcal{N}(\\psi_0, V_{\\psi})\\mathbb{1}_{\\{|\\psi|&lt;1\\}}\\]\nFor estimation purposes, we initialize with the following setting:\n\n\\(e_{i1} \\sim \\mathcal{N}(0, 1+\\psi^2)\\)\n\\(\\psi_0 = 0\\) and \\(V_{\\psi} = 1\\) so that the prior centers around 0 with a relatively large variance and has support within (-1, 1)\n\n\n\n3.2.3 Posterior Distributions\n\nThe posterior distribution of Y specified above has the form:\n\n\\[p(Y|A, \\Sigma) = (2\\pi)^{-\\frac{Tn}{2}}|\\Sigma|^{-\\frac{T}{2}}|\\Omega|^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA))\\]\n\\[= (2\\pi)^{-\\frac{Tn}{2}}|\\Sigma|^{-\\frac{T}{2}}(1+\\psi^2)^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T(H_{\\psi}O_{\\psi}H_{\\psi}^T)^{-1}(Y-XA))\\]\n\nThe joint posterior distribution of A and \\(\\Sigma\\) can be derived as follows:\n\n\\[p(A, \\Sigma \\mid Y, \\Omega)  = \\frac{p(A,\\Sigma,Y, \\Omega)}{p(Y, \\Omega)}\\] \\[\\propto p(A, \\Sigma, Y, \\Omega) \\propto p(Y \\mid A,\\Sigma, \\Omega)\\times p(A,\\Sigma, \\Omega)\\] \\[=  p(Y \\mid A,\\Sigma, \\Omega) p(A, \\Sigma) p(\\Omega) = p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma) p(\\Omega)\\]\n\\[\\propto p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma)\\]\n\\[\\propto |\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA)) \\times\\]\n\\[\\mid \\Sigma \\mid^{-\\frac{\\nu_0+N+K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_o))exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\\]\n\\[\\propto |\\Sigma|^{-\\frac{T+N+K+\\nu_0+1}{2}} \\times \\exp (-\\frac{1}{2}tr(\\Sigma^{-1}S_0)) \\times \\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\\Omega^{-1}Y-\\bar{A}^T\\bar{V}^{-1}\\bar{A})\\times\\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-\\bar{A})^T\\bar{V}^{-1}(A-\\bar{A})))\\]\nHence,\n\\[p(A, \\Sigma \\mid Y, X) = p(A \\mid Y, X, \\Sigma, \\Omega) p(\\Sigma \\mid Y, X, \\Omega)\\]\n\\[p(A \\mid Y, X, \\Sigma, \\Omega) = \\mathcal{MN}_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\]\n\\[p(\\Sigma \\mid Y, X, \\Omega) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\]\nwhere\n\\[\\bar{V} = (X^T\\Omega^{-1}X + V_A^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X^T\\Omega^{-1}Y + V_A^{-1}A_0)\\]\n\\[\\bar{\\nu} = T + \\nu_0\\]\n\\[\\bar{S} = S_0 + Y^T\\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \\bar{A}^T\\bar{V}^{-1}\\bar{A}\\] 3. The posterior distribution for the parameter \\(\\psi\\) can be obtained as follows: \\[P(\\psi|Y, A, \\Sigma) = \\frac{P(\\psi, Y, A, \\Sigma)}{P(Y, A, \\Sigma)} \\propto P(\\psi, Y, A, \\Sigma) = P(Y|A, \\Sigma, \\psi) \\times P(A,\\Sigma, \\psi) \\]\n\\[= P(Y|A, \\Sigma, \\psi) \\times P(A, \\Sigma) \\times P(\\psi) \\propto P(Y|A, \\Sigma, \\psi) \\times P(\\psi)\\]\nwe can sample from the posterior distribution of \\(\\psi\\) using an independence-chain Metropolis-Hastings algorithm.\n\n\n3.2.4 Estimation Procedure\nWe obtain posterior estimates of \\(A, \\Sigma, \\psi\\) using a Gibbs sampler, specifically, we initialize \\(\\psi^{(0)}\\) and for s = 1,‚Ä¶,S1+S2, we sequentially sample:\n\n\\(\\Sigma^{(s)} | Y, X, \\psi^{(s-1)} \\sim \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\)\n\\(A^{(s)} | Y, X, \\Sigma^{(s)}, \\psi^{(s-1)} \\sim \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma^{(s)}, \\bar{V})\\)\n\\(\\psi^{(s)} | Y, X, A^{(s)}, \\Sigma^{(s)} \\propto P(Y|A^{(s)}, \\Sigma^{(s)}, \\psi^{s-1})\\times p(\\psi)\\)\n\nThe Metropolis-Hastings Algorithm for Sampling \\(\\psi\\) is given as follows:\nInitialization:\n\nChoose an initial value \\(\\psi^{(0)}\\) within the bounds \\((-1, 1)\\).\n\nProposal Distribution:\n\nSelect the proposal distribution \\(q(\\psi' | \\psi^{(t)}) \\sim N(\\psi^{(t)}, \\tau^2)\\mathcal{I}(\\psi'\\in(-1,1))\\). \\(\\tau^2\\) is a tuning parameter that controls the step size.\n\nSampling Loop:\nFor t = 1,‚Ä¶,\\(T_1+T_2\\)\n\nGenerate a candidate \\(\\psi'^{(t)}\\) from \\(q(\\psi' | \\psi^{(t-1)})\\).\nCheck if \\(\\psi'\\) is within the bounds \\((-1, 1)\\). If not, reject \\(\\psi'\\) (set \\(\\alpha = 0\\)).\nCompute the acceptance ratio \\(\\alpha\\): \\[ \\alpha(\\psi^{(t-1)}, \\psi') = \\min\\left(1, \\frac{p(Y | A, \\Sigma, \\psi') p(\\psi') q(\\psi^{(t-1)} | \\psi')}{p(Y | A, \\Sigma, \\psi^{(t-1)}) p(\\psi^{(t-1)}) q(\\psi' | \\psi^{(t-1)})}\\right)\\]\n\nDecide to accept or reject:\n\nGenerate a random number \\(u\\) from \\(U[0,1]\\).\nIf \\(u \\leq \\alpha\\), accept \\(\\psi'\\) and set \\(\\psi^{(t)} = \\psi'\\).\nOtherwise, reject \\(\\psi'\\) and set \\(\\psi^{(t)} = \\psi^{(t-1)}\\).\n\nBurn in period\n\nDiscard the first \\(T_1\\) samples to allow the algorithm to converge to the true distribution\n\nObtain one sample of \\(\\psi\\)\n\nRandomly draw a \\(\\psi^*\\) from the \\(T_2\\) draws and set \\(\\psi^{(s)} = \\psi^*\\)\n\nWhen combined with Gibbs sampling for the estimation of \\(A\\) and \\(\\Sigma\\), the Metropolis-Hastings step can be embedded into the Gibbs sampler, a method known as Metropolis-within-Gibbs sampling. In this approach, one sample from the Metropolis-Hastings step is generated per Gibbs iteration. Detailed proofs and explanations on the convergence properties and efficiency of the Metropolis-within-Gibbs sampling method are provided in Chib and Greenberg (1995).\nWe monitor the acceptance rate and adjust \\(\\tau^2\\) as necessary to achieve an optimal rate of about 20-40%.\nWe note that since \\(\\Omega\\) is a band matrix, which means we do not need compute \\(\\Omega^{-1}\\). Instead, we obtain the Cholesky decomposition \\(C_{\\Omega}\\) of \\(\\Omega\\), which has a time complexity of \\(O(T)\\) instead of \\(O(T^3)\\). Terms involving \\(\\Omega^{-1}\\) such as \\(X^T\\Omega^{-1}X\\) can be obtained by: \\[X^T\\Omega^{-1}X = X^T(C_{\\Omega}^{-1})^{T}C_{\\Omega}^{-1}X=(C_{\\Omega}^{-1}X)^T(C_{\\Omega}^{-1}X) = \\tilde{X}^T\\tilde{X}\\]\nTo make forecasts of the \\(Y\\), we make the following observations: One-period ahead forecast \\[y_{t+1} = \\mu_0 + A_1y_t + \\cdots + A_py_{t-p+1} +  e_{t+1}\\] \\[E(y_{t+1}) = E(\\mu_0 + A_1y_t + \\cdots + A_py_{t-p+1} +  e_{t+1}) = \\mu_0 + A_1y_t + \\cdots + A_py_{t-p+1}\\] The one-period ahead forecast error is: \\[e_{t+1|t} = y_{t+1} - y_{t+1|t} = e_{t+1}\\] The one period ahead forecast variance is: \\[\\mathbb{V}ar(e_{t+1}|t) = \\mathbb{E}[\\mathbb{E}_t(e_{t+1}e_{t+1}^T)] = \\Sigma\\] Two-period ahead forecast \\[y_{t+2} = \\mu_0 + A_1y_{t+1} + \\cdots + A_py_{t-p+2} +  e_{t+2}\\] \\[E(y_{t+2}) = E(\\mu_0 + A_1y_{t+1} + \\cdots + A_py_{t-p+2} +  e_{t+2}) = \\mu_0 + A_1y_{t+1|t} + \\cdots + A_py_{t-p+2}\\] The two-period ahead forecast error is: \\[e_{t+2|t} = y_{t+2}-y_{t+2|t} = e_{t+2} + A_1(y_{t+1} - y_{t+1|t}) = e_{t+2} + A_1e_{t+1}\\] \\[Var(e_{t+2|t}) = E[e_{t+2|t}e_{t+2|t}^T] = E((e_{t+2} + A_1e_{t+1})(e_{t+2} + A_1e_{t+1})^T) = E((e_{t+2}e_{t+2}^T + A_1e_{t+1}e_{t+2}^T+e_{t+2}e_{t+1}^TA_1^T+A_1e_{t+1}e_{t+1}^TA_1^T)\\] \\[ = \\Sigma + A_i\\Psi+ \\Psi A_1^T + A_1\\Sigma A_1^T\\] since in the MA(1) innovations setup, we have:\n\\[\\mathbb{V}ar(e_{t+2} e_{t+1}^T) = E \\left[\n\\begin{pmatrix}\ne_{t+2, 1} \\\\\ne_{t+2, 2} \\\\\n\\vdots \\\\\ne_{t+2, N}\n\\end{pmatrix}\n\\begin{pmatrix}\ne_{t+1, 1} & e_{t+1, 2} & \\cdots & e_{t+1, N}\n\\end{pmatrix}\n\\right] \\] \\[=\nE \\left[\n\\begin{pmatrix}\ne_{t+2, 1}e_{t+1, 1} & e_{t+2, 1}e_{t+1, 2} & \\cdots & e_{t+2, 1}e_{t+1, N}\\\\\ne_{t+2, 2}e_{t+1, 1} & e_{t+2, 2}e_{t+1, 2} & \\cdots & e_{t+2, 2}e_{t+1, N}\\\\\n\\vdots               &    \\vdots            & \\vdots &    \\vdots          \\\\\ne_{t+2, N}e_{t+2, 1} & e_{t+2, N}e_{t+2, 2} & \\vdots &    e_{t+2,N}e_{t+1,N}\n\\end{pmatrix}\n\\right]\\] \\[ = E \\left[\n\\begin{pmatrix}\n(\\eta_{t+2, 1} + \\psi \\eta_{t+1, 1})(\\eta_{t+1, 1} + \\psi \\eta_{t, 1}) & (\\eta_{t+2, 1} + \\psi \\eta_{t+1, 1})(\\eta_{t+1, 2}+\\psi \\eta_{t,2}) & \\cdots & (\\eta_{t+2, 1} + \\psi \\eta_{t+1, 1})(\\eta_{t,N}+\\psi\\eta_{t,N})) \\\\\n(\\eta_{t+2, 2} + \\psi \\eta_{t+1, 2})(\\eta_{t+1, 1} + \\psi \\eta_{t, 1}) & (\\eta_{t+2, 2} + \\psi \\eta_{t+1, 2})(\\eta_{t+1, 2}+\\psi \\eta_{t,2}) & \\cdots & (\\eta_{t+2, 2} + \\psi \\eta_{t+1, 2})(\\eta_{t,N}+\\psi\\eta_{t,N})) \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n(\\eta_{t+2, N} + \\psi \\eta_{t+1, N})(\\eta_{t+1, 1} + \\psi \\eta_{t, 1}) & (\\eta_{t+2, N} + \\psi \\eta_{t+1, N})(\\eta_{t+1, 2}+\\psi \\eta_{t,2}) & \\cdots & (\\eta_{t+2, N} + \\psi \\eta_{t+1, N})(\\eta_{t,N}+\\psi\\eta_{t,N}))\\\n\\end{pmatrix}\n\\right]\n\\] \\[= \\begin{pmatrix}\n\\psi\\mathbb{E}[\\eta_{t+1,1}^2] & 0 & \\cdots & 0\\\\\n0 & \\psi\\mathbb{E}[\\eta_{t+1,2}^2] & \\cdots & 0 \\\\\n0 & 0 & \\ddots & 0 \\\\\n0 & 0 & \\cdots & \\psi\\mathbb{E}[\\eta_{t+1,N}^2]\n\\end{pmatrix} = \\begin{pmatrix}\n\\psi & 0 & \\cdots & 0\\\\\n0 & \\psi & \\cdots & 0 \\\\\n0 & 0 & \\ddots & 0 \\\\\n0 & 0 & \\cdots & \\psi\n\\end{pmatrix} = \\Psi\\]\nsince \\(\\eta_{t,i} \\overset{iid}{\\sim}\\mathcal{N}(0,1)\\)"
  },
  {
    "objectID": "index.html#model-3-large-bvar-model-with-common-stochastic-volatility",
    "href": "index.html#model-3-large-bvar-model-with-common-stochastic-volatility",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.3 Model 3: Large BVAR model with common Stochastic Volatility",
    "text": "3.3 Model 3: Large BVAR model with common Stochastic Volatility\nThe variances of economic shocks are rarely constant over time. Volatility tends to cluster during periods of economic crisis and becomes more tranquil during stable times. Thus, treating the innovations as having constant variance is an unrealistic assumption in practice. To remedy the situation, we can incorporate stochastic volatility into the model, thereby allowing the model to adapt to changing volatility in the data. This typically lead to improvement in model performance, especially in the presence of financial market instability or shifts in economic policy. A BVAR model with common stochastic volatility is s specified as follows:\n\\[ Y = XA+E\\] \\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\n\\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, \\Omega_{T \\times T})\\] where\n\\[e_t \\sim \\mathcal{N}(0, e^{h_t}\\Sigma)\\] and \\(h_t\\) follows an AR(1) process. \\[h_t = \\rho h_{t-1} + u_t^h\\] \\[u_t^h \\sim N(0, \\sigma^2_h)\\]"
  },
  {
    "objectID": "index.html#model-4-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility",
    "href": "index.html#model-4-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.4 Model 4: Large BVAR model with MA(1) Gaussian innovations and Common Stochastic Volatility",
    "text": "3.4 Model 4: Large BVAR model with MA(1) Gaussian innovations and Common Stochastic Volatility\n\n3.4.1 Model Specification\n\\[ Y = XA+E\\]\nwhere\n\\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\nthe same as before. But, instead of the column specific matrix as the identity matrix, we specify the column specific matrix as a diagonal matrix \\(\\Omega\\). Specifically, we have:\n\\[\\epsilon_t = u_t + \\psi_1 u_{t-1}\\]\n\\[u_t \\sim \\mathcal{N}(0,e^{h_t} \\Sigma)\\]\n\\[h_t = \\rho h_{t-1} + u_t^h \\quad \\text{ follows an Autoregressive process of lag 1 AR(1), and}\\]\n\\[u_t^h \\sim N(0,\\sigma_h^2)\\]\n\\[\\Omega = \\left(\n\\begin{array}{cccc}\n(1 + \\psi_1^2) e^{h_1} & \\psi_1 e^{h_1} & \\cdots & 0 \\\\\n\\psi_1 e^{h_1} & \\psi_1^2 e^{h_1} + e^{h_2} & \\cdots & \\vdots \\\\\n0 & \\cdots & \\ddots & \\vdots \\\\\n\\vdots & \\cdots & \\psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} & \\psi_1 e^{h_{T-1}} \\\\\n0 & \\cdots & \\psi_1 e^{h_{T-1}} & \\psi_1^2 e^{h_{T-1}} + e^{h_T}\n\\end{array}\n\\right)\\]\nIn this specification, each element of \\(e_t\\) may have distinct variances, and the variances of all innovations can be scaled by a common factor. This approach is economically intuitive, as the volatility of macroeconomic variables often exhibit co-movement. It is important to emphasize that each component of \\(e_t\\) must adhere to the same univariate time series model.\n\n\n3.4.2 Prior Specification\nHere, we consider a priori independent distributions for \\((A, \\Sigma, \\Omega)\\), namely:\n\\[p(A, \\Sigma, \\Omega) = p(A, \\Sigma) \\times p(\\Omega)\\] Given this structure, we can sample from the posterior distribution by sequentially sampling from:\n\n\\(P(A, \\Sigma | Y, X, \\Omega)\\)\n\\(P(\\Omega | Y, X, A, \\Sigma)\\)\n\nThe prior distribution of \\((A,\\Sigma)\\) follow the same normal inverse Wishart prior distribution as outlined in model one. But the variance matrix \\(V_A\\) for A is different:\n\\[V_A = diag(v_{A,ii})\\]\n\\[v_{A,ii} = \\begin{cases}\n\\kappa_1(\\frac{l^2}{\\hat{s}_r}) & \\text{for a coefficient associated to lag l of variable r} \\\\\n\\kappa_2& \\text{for an intercept}\n\\end{cases}\\]\nwhere \\(\\hat{s}_r\\) is the sample variance of an AR(4) model for the variable r.\n\\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, \\Omega_{T \\times T})\\]\nand,\nFor the moving average coefficients, we adopt an uninformative truncated normal prior for the MA coefficient \\(\\psi\\):\n\\[\n\\psi \\sim \\mathcal{N}(\\psi_0, V_\\psi) \\mathbb{1}(|\\psi|&lt;1)\n\\]\nand we set \\(\\psi_0 = 0\\) and \\(V_\\psi = 1\\) so that the prior centers around 0 with a relatively large variance and has support within (-1,1). Further more, we assume independent priors for \\(\\sigma^2_h\\) and \\(\\rho\\):\n\\[\\sigma_h^2 \\sim \\mathcal{IG}(\\nu_{h_0}, s_{h_0})\\]\n\\[\\rho \\sim \\mathcal{N}(\\rho_0, V_\\rho) \\mathbb{1}(|\\rho|&lt;1)\\]\nWe set the hyperparameters \\(\\nu_{h_0} = 5\\), \\(s_{h_0} = 0.04\\), \\(\\rho_0 = 0.9\\) and \\(V_\\rho = 0.04\\) so that the prior mean of \\(\\sigma_h^2\\) is 0.01 and \\(\\rho\\) is centered at 0.9.\n\n\n3.4.3 Posterior Distribution\nThe posterior distribution specified above has the following form:\n\\[p(Y|A, \\Sigma) = (2\\pi)^{-\\frac{Tn}{2}}|\\Sigma|^{-\\frac{T}{2}}|\\Omega|^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA))\\]\nand the joint posterior distribution\n\\[p(A, \\Sigma \\mid Y, \\Omega)  = \\frac{p(A,\\Sigma,Y, \\Omega)}{p(Y, \\Omega)}\\] \\[\\propto p(A, \\Sigma, Y, \\Omega) \\propto p(Y \\mid A,\\Sigma, \\Omega)\\times p(A,\\Sigma, \\Omega)\\] \\[=  p(Y \\mid A,\\Sigma, \\Omega) p(A, \\Sigma) p(\\Omega) = p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma) p(\\Omega)\\]\n\\[\\propto p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma)\\]\n\\[\\propto |\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA)) \\times\\]\n\\[\\mid \\Sigma \\mid^{-\\frac{\\nu_0+N+K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_o))exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\\]\n\\[\\propto |\\Sigma|^{-\\frac{T+N+K+\\nu_0+1}{2}} \\times \\exp (-\\frac{1}{2}tr(\\Sigma^{-1}S_0)) \\times \\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\\Omega^{-1}Y-\\bar{A}^T\\bar{V}^{-1}\\bar{A})\\times\\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-\\bar{A})^T\\bar{V}^{-1}(A-\\bar{A})))\\]\nHence,\n\\[p(A, \\Sigma \\mid Y, X) = p(A \\mid Y, X, \\Sigma) p(\\Sigma \\mid Y, X)\\]\n\\[p(A \\mid Y, X, \\Sigma, \\Omega) = \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\]\n\\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\]\nwhere\n\\[\\bar{V} = (X^T\\Omega^{-1}X + V_A^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X^T\\Omega^{-1}Y + V_A^{-1}A_0)\\]\n\\[\\bar{\\nu} = T + \\nu_0\\]\n\\[\\bar{S} = S_0 + Y^T\\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \\bar{A}^T\\bar{V}^{-1}\\bar{A}\\]\n\n\n3.4.4 Estimation Procedure\nIn this setting, we have\n\n\\((A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N} (A_0, V_A, S_0, \\nu_0)\\)\n\\(p(A, \\Sigma, \\Omega) = p(A, \\Sigma) \\times p(\\Omega)\\)\n\nthen, prior draws can be sampled from\n\n\\(\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0)\\)\n\\(A|\\Sigma \\sim \\mathcal{MN}_{K \\times N}(A_0, \\Sigma, V_A)\\)\n\\(\\Omega = \\left(\n\\begin{array}{cccc}\n(1 + \\psi_1^2) e^{h_1} & \\psi_1 e^{h_1} & \\cdots & 0 \\\\\n\\psi_1 e^{h_1} & \\psi_1^2 e^{h_1} + e^{h_2} & \\cdots & \\vdots \\\\\n0 & \\cdots & \\ddots & \\vdots \\\\\n\\vdots & \\cdots & \\psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} & \\psi_1 e^{h_{T-1}} \\\\\n0 & \\cdots & \\psi_1 e^{h_{T-1}} & \\psi_1^2 e^{h_{T-1}} + e^{h_T}\n\\end{array}\n\\right)\\)\n\\(\\epsilon_t = u_t + \\psi_1 u_{t-1}\\)\n\\(\\psi \\sim \\mathcal{N}(\\psi_0, V_\\psi) \\mathbb{1}(|\\psi|&lt;1)\\)\n\\(u_t \\sim \\mathcal{N}(0,e^{h_t} \\Sigma)\\)\n\\(h_t = \\rho h_{t-1} + u_t^h\\)\n\\(\\rho \\sim \\mathcal{N}(\\rho_0, V_\\rho) \\mathbb{1}(|\\rho|&lt;1)\\)\n\\(u_t^h \\sim N(0,\\sigma_h^2)\\)\n\\(\\sigma_h^2 \\sim \\mathcal{IG}(\\nu_{h_0}, s_{h_0})\\)\n\nTo sample \\(S_1+S_2\\) draws of \\(\\left\\{\\Omega^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\)\n\nSample \\(S_1+S_2\\) draws of \\(\\left\\{\\sigma^{2,(s)}_h\\right\\}_{s=1}^{S_1+S_2}\\) from \\(\\mathcal{IG}(v_{h_0}, s_{h_0})\\)\nSample \\(S_1+S_2\\) draws of \\(\\left\\{\\rho^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\) from \\(\\mathcal{N}(\\rho_0, V_\\rho) \\mathbb{1}(|\\rho|&lt;1)\\)\nFor each \\(\\sigma^{2,(s)}_h\\), sample \\(\\left\\{u_t^{h,(s)}\\right\\}_{t=1}^T\\) from \\(N(0,\\sigma_h^{2,(s)})\\)\nFor t = 1,‚Ä¶,T and s = 1,‚Ä¶, \\(S_1+S_2\\), compute \\(h_t^{(s)} = \\rho h_{t-1}^{(s)} + u_t^{h,(s)}\\)\nSample \\(S_1+S_2\\) draws of \\(\\left\\{u_t^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\) from \\(u_t \\sim \\mathcal{N}(0,e^{h_t^{(s)}} \\Sigma)\\) for t = 1,‚Ä¶,T\nSample \\(S_1+S_2\\) draws of \\(\\left\\{\\psi^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\) from \\(\\mathcal{N}(\\psi_0, V_\\psi) \\mathbb{1}(|\\psi|&lt;1)\\)\nfor each t = 1,‚Ä¶,T and s = 1,‚Ä¶\\(S_1+S_2\\), compute \\(\\epsilon_t^{(s)} = u_t^{(s)} + \\psi^{(s)}u_{t-1}^{(s)}\\)\n\nAfter we have obtained \\(\\left\\{\\Omega^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\), we can use the following Gibb‚Äôs Sampler algorithm to sample from the posterior distribution \\(p(A \\mid Y, X, \\Sigma, \\Omega)\\):\n\nInitialize \\(\\Sigma\\) at \\(\\Sigma^0\\)\nFor \\(s = 1,...,S_1+S_2\\)\n\nDraw a sample \\(A^{(s)}\\) from \\(p(A \\mid Y, X, \\Omega^{(s)}, \\Sigma^{(s-1)}) \\sim \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\)\nDraw a sample \\(\\Sigma^{(s)}\\) from \\(p(\\Sigma \\mid Y, X, A^{(s)}, \\Omega^{(s)}) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\)\n\n\nWe discard the first \\(S_1\\) sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain \\(S_2\\) sampled draws from the joint posterior distribution.\n\\[\\left\\{A^{(s)}, \\Sigma^{(s)}\\right\\}_{s=S_1+1}^{S_1+S_2}\\]\nSampling from the joint predictive density is the same as before."
  },
  {
    "objectID": "index.html#standard-bayesian-var",
    "href": "index.html#standard-bayesian-var",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "4.1 Standard Bayesian VAR",
    "text": "4.1 Standard Bayesian VAR\n\n4.1.1 Model Building Code and Validation\nWe verify that our model can replicate the true parameter of the data generate process by: 1. Generate artificial data containing 1000 observations simulated from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2. That is,\n\\[\\mathbf{y_t} = \\begin{pmatrix} y_t,1 \\\\ y_t,2\\end{pmatrix} = \\mathbf{y_{t-1}} + \\mathbf{\\epsilon_t} = \\begin{pmatrix}y_{t-1,1}\\\\y_{t-1, 2}\\end{pmatrix} + \\begin{pmatrix}\\epsilon_{t,1}\\\\ \\epsilon_{t, 2}\\end{pmatrix}\\] and\n\\[\\mathbf{\\epsilon} \\sim iid \\mathcal{N}(\\mathbf{0}, \\mathbf{I_2} )\\]\n\nset.seed(1005)\n\nn = 1000  \ncov_matrix = diag(2)  \n\nrandom_walk_sample = matrix(nrow = n, ncol = 2)\n#initialize the random walk at 0,0\nrandom_walk_sample[1, ] = c(0, 0)\n\nfor (i in 2:n) {\n  random_walk_sample[i, ] = random_walk_sample[i - 1, ] + mvrnorm(n = 1, mu = c(0, 0), Sigma = cov_matrix)\n  }\nmat_plot = random_walk_sample \ncolnames(mat_plot) = c(\"Series1\", \"Series2\")\nmat_plot = as.data.frame(mat_plot)\nmat_plot$Index = 1:nrow(mat_plot) \nmat_plot = melt(mat_plot, id.vars = \"Index\")\n\nggplot(mat_plot, aes(x = Index, y = value, color = variable)) +\n    geom_line() +\n    labs(title = \"1000 observations simulated from a bi-variate Gaussian random walk process\", x = \"\", y = \"\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThe BVAR_fit function takes in ‚Äòdata‚Äô as an argument, where columns of ‚Äòdata‚Äô represent different variables and rows correspond to observations over time. It also requires an integer p, which specifies the number of lags to include in the BVAR model. The parameters S1 and S2 denote the number of burn-in samples and actual samples, respectively. \\(\\kappa_1\\) is the shrinkage level applied to the autoregressive coefficients, while \\(\\kappa_2\\) controls the shrinkage for the constant term. The \\(A_{prior}\\) and \\(V_{prior}\\) parameters set the prior mean and covariance matrices for the matrix A. Finally, \\(S_{prior}\\) and \\(\\nu_{prior}\\) define the scale and shape parameters, respectively, for the prior distribution of the covariance matrix \\(\\Sigma\\)\n\nBVAR_fit &lt;- function(data, p, S1, S2, kappa1, kappa2, A_prior, V_prior, S_prior, nu_prior, N){\n  Y = (data[(p+1):nrow(data),])\n  N = ncol(Y)\n  Ty = nrow(Y)\n  S = S1 + S2\n  Y = (data[(p+1):nrow(data),])\n  X = matrix(1,nrow(Y),1) \n  for (i in 1:p){\n    X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n  }\n  X = as.matrix(X)\n  Y = as.matrix(Y)\n  lambda &lt;- 1e-5\n  V_bar = solve(t(X)%*%X + solve(V_prior) + diag(lambda, nrow = ncol(X)))\n  #V_bar = round(V_bar, 5)\n  A_bar = V_bar%*%(t(X)%*%Y + solve(V_prior)%*%A_prior)\n  nu_bar = Ty + nu_prior\n  #S_bar  = S_prior + t(Y)%*%Y + t(A_prior)%*%diag(1/diag(V_prior))%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar\n  S_bar = S_prior +  t(Y)%*%Y + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar\n  S_bar_inv = solve(S_bar)\n  #Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)\n  Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)\n  Sigma_posterior = apply(Sigma_posterior,3,solve)\n  Sigma_posterior  = array(Sigma_posterior,c(N,N,S))\n  A_posterior = array(rnorm(prod(c(dim(A_bar),S))),c(dim(A_bar),S))\n  L = t(chol(V_bar))\n  for (s in 1:S){\n    A_posterior[,,s]= A_bar + L%*%A_posterior[,,s]%*%chol(Sigma_posterior[,,s])\n    }\n  A_posterior = A_posterior[,,(S1+1):(S1+S2)]\n  Sigma_posterior = Sigma_posterior[,,(S1+1):(S1+S2)]\n  return(list(A_posterior, Sigma_posterior))\n}\n\nWe use our function to estimate a model with a constant term and lag order 1 using simulated data from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2. We show that sampled posterior mean of the autoregressive and the covariance matrices are close to an identity matrix and that the sampled posterior mean of the constant term is close to a vector of zeros.\n\np = 1\nkappa1 = 0.02^2\nkappa2 = 10^2\nN = ncol(random_walk_sample)\nA_prior             = matrix(0,1+p*N,N)\nA_prior[2:(N + 1),] = diag(N)\nV_prior     = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nS_prior     = diag(1, N)\nnu_prior    = ncol(random_walk_sample)+1\nS1 = 10000\nS2 = 20000\n\nposterior_samples = BVAR_fit(data = random_walk_sample, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)\n\nposterior_samples_A = posterior_samples[[1]]\nposterior_samples_Sigma = posterior_samples[[2]]\nprint('Posterior mean of autoregressive parameter:')\n\n[1] \"Posterior mean of autoregressive parameter:\"\n\nround(apply(posterior_samples_A,1:2,mean),3)\n\n      [,1]  [,2]\n[1,] 0.029 0.019\n[2,] 0.987 0.002\n[3,] 0.006 0.993\n\nprint('Posterior standard deviation of autoregressive parameter:')\n\n[1] \"Posterior standard deviation of autoregressive parameter:\"\n\nround(apply(posterior_samples_A,1:2,sd),3)\n\n      [,1]  [,2]\n[1,] 0.033 0.033\n[2,] 0.005 0.005\n[3,] 0.006 0.006\n\nprint('Posterior mean of covariance matrix:')\n\n[1] \"Posterior mean of covariance matrix:\"\n\nround(apply(posterior_samples_Sigma,1:2,mean),3)\n\n      [,1]  [,2]\n[1,] 0.993 0.037\n[2,] 0.037 1.045\n\nprint('Posterior standard deviation of covariance matrix:')\n\n[1] \"Posterior standard deviation of covariance matrix:\"\n\nround(apply(posterior_samples_Sigma,1:2,sd),3)\n\n      [,1]  [,2]\n[1,] 0.045 0.033\n[2,] 0.033 0.047\n\n\n\n\n4.1.2 Empirical Result\n\n4.1.2.1 Fitted Models\n\np = 5\nkappa1 = 0.2^2\nkappa2 = 10^2\ndata = final_df[, 2:ncol(final_df)]\nN = ncol(data)\nA_prior = matrix(0,1+p*N,N)\nA_prior[2:(N + 1),] = diag(N)\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nS_prior = diag(1, N)\nnu_prior  = ncol(data)+1\ndata = final_df[,2:ncol(final_df)]\n#data = final_df[,2:19]\nS1 = 50000\nS2 = 50000\n\nposterior_samples = BVAR_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)\n\nposterior_samples_A = posterior_samples[[1]]\nposterior_samples_Sigma = posterior_samples[[2]]\n# print('Posterior mean of autoregressive parameter:')\n# round(apply(posterior_samples_A,1:2,mean),3)\n# print('Posterior standard deviation of autoregressive parameter:')\n# round(apply(posterior_samples_A,1:2,sd),3)\n# print('Posterior mean of covariance matrix:')\n# round(apply(posterior_samples_Sigma,1:2,mean),3)\n# print('Posterior standard deviation of covariance matrix:')\n# round(apply(posterior_samples_Sigma,1:2,sd),3)\n\n\n\n4.1.2.2 Prediction Plots\n\nBVAR_predict = function(posterior_samples_A, posterior_samples_Sigma, data, h){\n  S = dim(posterior_samples_A)[3]\n  N =dim(posterior_samples_A)[2]\n  p = (dim(posterior_samples_A)[1] -1) / N\n  Y = (data[(p+1):nrow(data),])\n  #last p observations \n  x = Y[(nrow(Y)-p+1):nrow(Y),]\n  # reverse the order t-p t-(p-1)...\n  x = x[p:1,]\n  Y_h   = array(NA,c(h,N,S))\n  for (s in 1:S){\n    A     = posterior_samples_A[,,s]\n    Sigma = posterior_samples_Sigma[,,s]\n    x  = Y[(nrow(Y)-p+1):nrow(Y),]\n    x = x[p:1,]\n    for (i in 1:h){\n      #x_{{t+1}= (1 y_t y_{t-1},... y_{t-p+1})\n      x_T   = c(1,as.vector(t(x)))\n      #y_t+h = mvnnorm(1, x_t%*%A)\n      Period_Y  = mvtnorm::rmvnorm(1, mean = x_T%*%A, sigma=Sigma)\n      colnames(Period_Y) = colnames(x)\n      x   = rbind(Period_Y,x[1:(p-1),])\n      Y_h[i,,s]   = Period_Y[1:N]\n      }\n  }\n  return(Y_h)\n}\n\n\nstart &lt;- system.time({\n  data &lt;- final_df[, 2:ncol(final_df)]\n  h &lt;- 8\n  BVAR_prediction &lt;- BVAR_predict(posterior_samples_A = posterior_samples_A, posterior_samples_Sigma = posterior_samples_Sigma, data = data, h = h)\n})  # Closing the system.time block after all operations\n\n# Assuming S2 is the number of iterations and is defined somewhere in your script\nprint(sprintf(\"Running prediction for %d iterations took: %f seconds\", S2, start[\"elapsed\"]))\n\n[1] \"Running prediction for 50000 iterations took: 131.263000 seconds\"\n\n\n\nlibrary(HDInterval)\n\nplot_CI = function(prediction, data, final_df, var_index, main_title = \"Forecast Plot\"){\n  h = dim(prediction)[1]\n  forecast_mean = apply(prediction, 1, mean)\n  forecast_interval = apply(prediction, 1, hdi, credMass=0.50)\n  forecast_range = range(data[,var_index], forecast_interval)\n\n  mcxs1 = \"#05386B\"\n  mcxs1.shade1 = rgb(t(col2rgb(mcxs1)), alpha=120, maxColorValue=255)\n  x_value = final_df$Time\n  for (i in 1:h){\n    x_value[length(x_value)+1] = max(x_value) + 1/4\n  }\n  \n  par(mar = c(5.1, 4.1, 4.1, 2.1))\n  plot(x_value, c(data[,var_index], forecast_mean), type=\"l\",\n       ylim=forecast_range, axes=FALSE, xlab=\"\", ylab=\"\", lwd=2, main = main_title)\n  axis(1) \n  axis(2) \n  abline(v=length(data[,var_index]), col=mcxs1)\n\n  polygon(x = c(x_value[(length(x_value)-h+1) : length(x_value)], rev(x_value[(length(x_value)-h+1) : length(x_value)])),\n          y = c(forecast_interval[1,], rev(forecast_interval[2,])),\n          col=mcxs1.shade1, border=NA)\n  # polygon(x = c((length(data[,var_index])+1):(length(data[,var_index])+h), rev((length(data[,var_index])+1):(length(data[,var_index])+h))),\n  #         y = c(forecast_interval[1,], rev(forecast_interval[2,])),\n  #         col=mcxs1.shade1, border=NA)\n}\n\n\npar(mfrow = c(4, 3), mar = c(5, 4, 1, 1), cex.axis = 1)\n\nfor (i in 1:ncol(data)) {\n  plot_title = paste(\"Forecast for\", colnames(data)[i]) \n  plot_CI(prediction = BVAR_prediction[, i, ], data = data, final_df = final_df, var_index = i, main_title = plot_title)\n}\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\nlibrary(plot3D)\nmcxs1  = \"#05386B\"\nmcxs2  = \"#379683\"\nmcxs3  = \"#5CDB95\"\nmcxs4  = \"#8EE4AF\"\nmcxs5  = \"#EDF5E1\"\nthreeD_plolts_MA = c()\n\nplots = list()\nfor (j in 1:ncol(data)){\n  lower        = range(BVAR_prediction[,j,])\n  prediction_mean         = apply(BVAR_prediction[,j,],1,mean)\n  prediction_CI      = apply(BVAR_prediction[,j,],1,hdi,credMass=0.90)\n  theta = 180\n  phi   = 15.5\n  \n  x_range           = seq(from=lower[1], to=lower[2], length.out=10)\n  z_range           = matrix(NA,h,9)\n  for (i in 1:h){\n    z_range[i,]     = hist(BVAR_prediction[i,j,], breaks=x_range, plot=FALSE)$density\n  }\n  x_range           = hist(BVAR_prediction[i,j,], breaks=x_range, plot=FALSE)$mids\n  y_range          = 1:h\n  \n  # plot using plot_ly\n  p &lt;- plot_ly(y = y_range, x = x_range, z = z_range, type = \"surface\") %&gt;%\n    layout(scene = list(xaxis = list(title = sprintf(\"%s\", colnames(data)[j])),\n                        yaxis = list(title = \"h\"),\n                        zaxis = list(title = \"density\")),\n           title = sprintf(\"%s\", colnames(data)[j]))\n  # Print or display the plot\n  print(p)\n  plots[[j]] = p\n}\n\n\n\n4.1.2.3 Test for Granger Causality\nWe first perform Granger Causality test for each country, we test whether the macroeconomic factor of one country has some predictive power on the macroeconomic factors of another. Specifically, we test the following hypothesis:\n\\(H_0:\\) The consumer price index, interest rate, foreign direct investment, balance of payments, gross domestic product, exchange rate against the dollar of country A has no impact on these variables in the country B$\nThe Baye‚Äôs factor in this case is defined as:\n\\[_H = \\frac{p_0(Y)}{p_1(Y)} = \\frac{p_1(A_{ij} = 0|Y)}{p_1(A_{ij} = 0)}=\\frac{\\int p_1(A_{ij=0}|Y, \\Sigma)p(\\Sigma|Y) d \\Sigma}{\\int p(A_{ij}=0, \\Sigma)d\\Sigma}\\] \\[p_1(A_{ij} = 0|Y, X,\\Sigma) \\sim \\mathcal{MN}_{K \\times N}(\\bar{A_{ij}}, \\Sigma_{ij}, \\bar{V}_{ij})\\] \\[p_1(A_{ij} = 0, \\Sigma) \\sim \\mathcal{NIW}_{K \\times N}(A_{0, ij}, V_{A, ij}, S_{0, ij}, \\nu_{0, ij})\\] \\[p_1(A_{ij} = 0, \\Sigma) = p(A_{ij}|\\Sigma)\\times p(\\Sigma)\\] \\[A_{ij}|\\Sigma \\sim \\mathcal{MN}_{K \\times N} (A_{0,ij}, \\Sigma, V_{A, ij}) \\] \\[\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0)\\] and we test the bi-lateral relationship between each of these countries.\n1. Granger Causality Test for all countries from Q1 1994 to Q3 2023\n\n\nShow code\ncalc_bayes_factor = function(country1, country2, S){\n  #S cannot exceed the number of posterior samples generated\n  S = min(S, dim(posterior_samples_Sigma)[3])\n  ratio_list = list(dim = S)\n  var_list = colnames(data)\n  country1 = var_list[grepl(country1, var_list)]\n  var_list = colnames(X)\n  country2 = var_list[grepl(country2, var_list)]\n  \n  #set dim names for V\n  dimnames(V_prior) = list(colnames(X), colnames(X))\n  dimnames(posterior_V) = list(colnames(X), colnames(X))\n  #set this in case we want to test less samples than the number of posterior samples generated\n  total_s = dim(posterior_samples_Sigma)[3]\n  #set dim names for A_prior\n  dimnames(A_prior) = list(colnames(X), colnames(data))\n  for (s in 1:S){\n    #calculate prior probability\n    A_prior_test = A_prior[country2, country1]\n    prior_Sigma = MCMCpack::riwish(v = nu_prior, S = S_prior)\n    dimnames(prior_Sigma) = list(colnames(data), colnames(data))\n    #Sigma is row specific covariance matrix\n    prior_Sigma_test = prior_Sigma[colnames(A_prior_test),colnames(A_prior_test)]\n    # V is the column specific covariance matrix\n    V_prior_test = V_prior[rownames(A_prior_test), rownames(A_prior_test)]\n    Q_prior = kronecker(prior_Sigma_test, V_prior_test)\n    prior_prob =  mvtnorm::dmvnorm(x = c(array(0, dim=c(nrow(A_prior_test), ncol(A_prior_test)))), \n                                   c(A_prior_test), \n                                   sigma = Q_prior, log = TRUE)\n    #select the s sampled posterior A\n    A_0_mat_full = posterior_samples_A[,,s]\n    dimnames(A_0_mat_full) = list(colnames(X), colnames(data))\n    row_names = rownames(A_0_mat_full)\n    A_0_mat_posterior = A_0_mat_full[country2, country1]\n    M = c(posterior_samples_A[,,(total_s - s + 1)])\n    M = array(M, dim = c(nrow(A_0_mat_full), ncol(A_0_mat_full)))\n    dimnames(M) = list(colnames(X), colnames(data))\n    M = M[country2, country1]\n    posteior_sigma_test = posterior_samples_Sigma[,,(total_s - s + 1)]\n    dimnames(posteior_sigma_test) = list(colnames(data), colnames(data))\n    posteior_sigma_test = posteior_sigma_test[colnames(A_0_mat_posterior),colnames(A_0_mat_posterior)]\n    posterior_V_test = posterior_V[rownames(A_0_mat_posterior), rownames(A_0_mat_posterior)]\n    Q = kronecker(posteior_sigma_test, posterior_V_test)\n    posterior_prob = mvtnorm::dmvnorm(x = c(array(0, dim=c(nrow(A_0_mat_posterior), ncol(A_0_mat_posterior)))), mean = c(M), sigma = Q, log = TRUE)\n    ratio_list[s] = posterior_prob - prior_prob\n  }\n  return(ratio_list)\n}\n\n\n\n\nShow code\n#aa = calc_bayes_factor('CN', \"US\")\ncountries_list = c('CN', 'US', \"JP\", \"AU\")\np = 5\nS = 50000\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1)\nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nX = data.frame(X) %&gt;% rename_with(~ make.unique(.), .cols = names(.)[duplicated(names(.))])\nY = as.matrix(Y)\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\nposterior_V = solve(t(X)%*%X + solve(V_prior) + diag(lambda, nrow = ncol(X)))\nA_prior = matrix(0,1+p*N,N)\nA_prior[2:(N + 1),] = diag(N)\n\nbayesfactor = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\nstart = system.time({\n  pb &lt;- progress_bar$new(total = length(countries_list))\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){  \n      bayesfactor[country_a, country_b] = mean(unlist(calc_bayes_factor(country_a, country_b, S)))\n      \n    }\n   gc()\n  }\n})\n\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 50000 iterations took: 401.001000 seconds\"\n\n\nShow code\nprint(bayesfactor)\n\n\n            CN           US          JP          AU\nCN -1570.58201     7.432428    26.90968    11.92190\nUS    17.89544 -1512.714188    34.73056    21.27134\nJP   -13.90457   -10.155720 -1747.54612   -14.59196\nAU    41.80227    25.967628    58.43271 -2363.09432\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nSome Evidence\nSome Evidence\nStrong Evidence\nSome Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\n2. Granger Causality Test for all countries from Q1 1994 to Q1 2020 (Pre-Covid)\n\n\nShow code\np = 5\nkappa1 = 0.2^2\nkappa2 = 10^2\ndata = final_df[final_df$Time &lt;=as.yearqtr('2020 Q1'),2:ncol(final_df)]\nN = ncol(data)\nA_prior = matrix(0,1+p*N,N)\nA_prior[2:(N + 1),] = diag(N)\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nS_prior = diag(1, N)\nnu_prior  = ncol(data)+1\nS1 = 50000\nS2 = 50000\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nY = as.matrix(Y)\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\nposterior_V = solve(t(X)%*%X + solve(V_prior) + diag(lambda, nrow = ncol(X)))\nposterior_samples = BVAR_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)\nposterior_samples_A = posterior_samples[[1]]\nposterior_samples_Sigma = posterior_samples[[2]]\n\ncountries_list = c('CN', 'US', \"JP\", \"AU\")\nS = 50000\nX = data.frame(X) %&gt;% rename_with(~ make.unique(.), .cols = names(.)[duplicated(names(.))])\nX = as.matrix(X)\nbayesfactor = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      bayesfactor[country_a, country_b] = mean(unlist(calc_bayes_factor(country_a, country_b, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 50000 iterations took: 401.722000 seconds\"\n\n\nShow code\nprint(bayesfactor)\n\n\n            CN           US          JP          AU\nCN -1658.35140     7.650737    27.66237    11.82414\nUS    16.42595 -1326.997653    34.15129    21.06936\nJP   -15.77123   -10.111587 -1497.03284   -15.49152\nAU    41.34139    25.307909    56.91642 -2086.87136\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nSome Evidence\nSome Evidence\nStrong Evidence\nSome Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\n3. Granger Causality Test for all countries from Q1 2007 to Q1 2020 (After China joined WTO and before Covid)\n\n\nShow code\np = 5\nkappa1 = 0.2^2\nkappa2 = 10^2\ndata = final_df[(final_df$Time &lt;=as.yearqtr('2020 Q1'))&(final_df$Time &gt;=as.yearqtr('2007 Q1')),2:ncol(final_df)]\nN = ncol(data)\nA_prior = matrix(0,1+p*N,N)\nA_prior[2:(N + 1),] = diag(N)\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nS_prior = diag(1, N)\nnu_prior  = ncol(data)+1\nS1 = 50000\nS2 = 50000\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nY = as.matrix(Y)\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\nposterior_V = solve(t(X)%*%X + solve(V_prior) + diag(lambda, nrow = ncol(X)))\nposterior_samples = BVAR_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)\nposterior_samples = BVAR_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)\nposterior_samples_A = posterior_samples[[1]]\nposterior_samples_Sigma = posterior_samples[[2]]\n\ncountries_list = c('CN', 'US', \"JP\", \"AU\")\nS = 50000\nX = data.frame(X) %&gt;% rename_with(~ make.unique(.), .cols = names(.)[duplicated(names(.))])\nX = as.matrix(X)\n\nbayesfactor = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      \n      bayesfactor[country_a, country_b] = mean(unlist(calc_bayes_factor(country_a, country_b, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 50000 iterations took: 406.405000 seconds\"\n\n\nShow code\nprint(bayesfactor)\n\n\n            CN         US         JP          AU\nCN -1013.69449   13.22565   34.54541    19.75187\nUS    10.42473 -614.39611   22.02464    12.92040\nJP   -20.42617  -12.62750 -820.92075   -19.52982\nAU    33.42174   21.50034   47.78869 -1062.21448\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nSome Evidence\nSome Evidence\nStrong Evidence\nSome Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\nWe see that Granger causality doesn‚Äôt change much according to the basic model.\n4. Granger Causality Test for all countries (excluding China) from Q1 1994 to Q3 2023\n\n\nShow code\np = 5\nkappa1 = 0.2^2\nkappa2 = 10^2\n#data = final_df[final_df$Time &lt;=as.yearqtr('2020 Q1'),2:ncol(final_df)]\ndata = final_df[grep(c('CN|Time'), colnames(final_df), value = TRUE, invert = TRUE)]\nN = ncol(data)\nA_prior = matrix(0,1+p*N,N)\nA_prior[2:(N + 1),] = diag(N)\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nS_prior = diag(1, N)\nnu_prior  = ncol(data)+1\nS1 = 50000\nS2 = 50000\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nY = as.matrix(Y)\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\nposterior_V = solve(t(X)%*%X + solve(V_prior) + diag(lambda, nrow = ncol(X)))\nposterior_samples = BVAR_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)\nposterior_samples_A = posterior_samples[[1]]\nposterior_samples_Sigma = posterior_samples[[2]]\n\ncountries_list = c('US', \"JP\", \"AU\")\nS = 50000\nX = data.frame(X) %&gt;% rename_with(~ make.unique(.), .cols = names(.)[duplicated(names(.))])\nX = as.matrix(X)\nbayesfactor = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      bayesfactor[country_a, country_b] = mean(unlist(calc_bayes_factor(country_a, country_b, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 50000 iterations took: 218.616000 seconds\"\n\n\nShow code\nprint(bayesfactor)\n\n\n            US          JP          AU\nUS -1699.45041    33.00494    21.32670\nJP   -10.30516 -1817.04882   -14.93476\nAU    26.07606    58.04532 -2882.27562\n\n\n\n\n\n\n\n\n\n\n\n\nUnited States\nJapan\nAustralia\n\n\n\n\nUS\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nSome Evidence\nStrong Evidence\nSome Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nStrong Evidence"
  },
  {
    "objectID": "index.html#large-bvar-model-with-ma1-gaussian-innovations",
    "href": "index.html#large-bvar-model-with-ma1-gaussian-innovations",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "4.2 Large BVAR model with MA(1) Gaussian Innovations",
    "text": "4.2 Large BVAR model with MA(1) Gaussian Innovations\n\n4.2.1 Model Building Code and Validation\n\n\nShow code\nBVAR_MA_fit &lt;- function(data, p, S1, S2, kappa1, kappa2, A_prior, V_prior, S_prior, nu_prior, N){\n  Y = (data[(p+1):nrow(data),])\n  N = ncol(Y)\n  Ty = nrow(Y)\n  S = S1 + S2\n  X = matrix(1,nrow(Y),1) \n  K = 1+p*N\n  tau = 0.15\n  for (i in 1:p){\n    X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n  }\n  X = as.matrix(X)\n  Y = as.matrix(Y)\n  \n  nu_bar = Ty + nu_prior\n  psi = 0\n  H_psi = matrix(0, nrow = Ty, ncol = Ty)\n  O_psi = diag(rep(1, Ty))\n  O_psi[1,1] = 1+psi^2\n  for (t in 1:Ty){\n    H_psi[t,t] = 1\n    if (t &lt; Ty){\n      H_psi[t+1, t] = psi\n    }\n  }\n  A_posterior = array(rnorm(prod(c(c(K,N),S))),c(c(K,N),S))\n  Sigma_posterior = array(dim = c(N,N,S))\n  Omega_posterior = array(dim = c(Ty, Ty, S+1))\n  Omega_posterior[,,1] = H_psi %*% O_psi %*% t(H_psi)\n  \n  accept = 0\n  pb &lt;- progress_bar$new(total = S)\n  \n  for (s in 1:S){\n    pb$tick()\n\n    chol_Omega = chol(Omega_posterior[,,s]) #########change \n    chol_Omega_inv = solve(chol_Omega)\n    #V_bar = solve(t(X)%*%X + solve(V_prior))\n    V_bar = solve(t(chol_Omega_inv%*%X)%*%(chol_Omega_inv%*%X) + solve(V_prior))\n    L = t(chol(V_bar))\n    #A_bar = V_bar%*%(t(X)%*%Y + solve(V_prior)%*%A_prior)\n    A_bar = V_bar%*%(t(chol_Omega_inv%*%X)%*%(chol_Omega_inv%*%Y) + solve(V_prior)%*%A_prior)\n    #S_bar = S_prior +  t(Y)%*%Y + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar\n    S_bar = S_prior +  t(chol_Omega_inv%*%Y)%*%(chol_Omega_inv%*%Y) + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar\n    S_bar_inv = solve(S_bar)\n    Sigma_posterior[,,s] = solve(rWishart(1, df=nu_bar, Sigma=S_bar_inv)[,,1])\n    A_posterior[,,s]= A_bar + L%*%A_posterior[,,s]%*%chol(Sigma_posterior[,,1]) ############change \n    ###############MH Step    ##################################\n    # psi_candidate = rnorm(n = 1, mean = psi, sd = tau)\n    # if(abs(psi_candidate) &gt; 1){\n    #   alpha = 0\n    # }\n    psi_candidate = RcppTN::rtn(psi, tau, -1, 1)\n    #else{\n    H_psi = matrix(0, nrow = Ty, ncol = Ty)\n    O_psi = diag(rep(1, Ty))\n    O_psi[1,1] = 1+psi_candidate^2\n    for (i in 1:Ty){\n      H_psi[i,i] = 1\n      if (i &lt; Ty){\n        H_psi[i+1, i] = psi_candidate\n      }\n    }\n    Omega_draw = H_psi %*% O_psi %*% t(H_psi)\n    chol_Omega_draw = chol(Omega_draw)\n    chol_Omega_draw_inv = solve(chol_Omega_draw)\n    Sigma_posterior_inv = solve(Sigma_posterior[,,s])\n    Likelihood_ratio = ((1+psi_candidate^2)/(1+psi^2))^(N/2) * exp(-1/2*(tr(\n      Sigma_posterior_inv%*%t(chol_Omega_draw_inv%*%(Y-X%*%A_posterior[,,s]))%*%(chol_Omega_draw_inv%*%(Y-X%*%A_posterior[,,s]))\n      ) - tr(\n        Sigma_posterior_inv%*%t(chol_Omega_inv%*%(Y-X%*%A_posterior[,,s]))%*%(chol_Omega_inv%*%(Y-X%*%A_posterior[,,s]))))\n      )\n    prior_ratio = dtruncnorm(x = psi_candidate, mean = psi_prior, sd = Vpsi_prior)/dtruncnorm(x = psi, mean = psi_prior, sd = Vpsi_prior)\n    #proposal_ratio = dnorm(x = psi, mean = psi_candidate, sd = tau)/dnorm(x=psi_candidate, mean = psi, sd = tau)\n    proposal_ratio = RcppTN::dtn(.x = psi, .mean = psi_candidate, .sd = tau, .low = -1, .high = 1) / RcppTN::dtn(.x = psi_candidate, .mean = psi, .sd = tau, .low = -1, .high = 1)\n    alpha = min(1, Likelihood_ratio, prior_ratio, proposal_ratio)\n    #}\n    u = runif(1, min = 0, max = 1)\n    if (u &lt;= alpha){\n      accept = accept + 1\n      psi = psi_candidate\n      Omega_posterior[,,s+1] = Omega_draw\n      }\n    else{\n      Omega_posterior[,,s+1] = Omega_posterior[,,s]}\n    ##################################################    #####\n  }\n  A_posterior = A_posterior[,,(S1+1):(S1+S2)]\n  Sigma_posterior = Sigma_posterior[,,(S1+1):(S1+S2)]\n  Omega_posterior = Omega_posterior[,,(S1+2):(S1+S2+1)]\n  print(\"acceptance rate of the Metropolis Hasting Step: \")\n  print(accept/S)\n  return(list(A_posterior, Sigma_posterior, Omega_posterior))\n}\n\n\n\n\nShow code\np = 1\nkappa1 = 0.1^2\nkappa2 = 10^2\n\nN = ncol(random_walk_sample)\nA_prior     = matrix(0, 1+p*N, ncol = N)\nV_prior     = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nS_prior     = diag(1, N)\nnu_prior    = ncol(random_walk_sample)+1\nS1 = 1000\nS2 = 1000\nvh0_prior = 5\nsh0_prior = 0.04\nrho_prior = 0.9\nVrho_prior = 0.04\npsi_prior = 0\nVpsi_prior = 1\n\nstart = system.time({\n  posterior_samples_MA = BVAR_MA_fit(data = random_walk_sample, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)\n})\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.1915\n\n\nShow code\nprint(sprintf(\"Fit MA BVAR on 1000 bi-variate random walk for %d iterations took: %f seconds\", (S1+S2), start[\"elapsed\"])) \n\n\n[1] \"Fit MA BVAR on 1000 bi-variate random walk for 2000 iterations took: 3885.405000 seconds\"\n\n\nShow code\nposterior_samples_A_MA = posterior_samples_MA[[1]]\nposterior_samples_Sigma_MA = posterior_samples_MA[[2]]\nposterior_samples_Omega_MA = posterior_samples_MA[[3]]\n# print('Posterior mean of autoregressive parameter:')\n# round(apply(posterior_samples_A_MA,1:2,mean),3)\n# print('Posterior standard deviation of autoregressive parameter:')\n# round(apply(posterior_samples_A_MA,1:2,sd),3)\n# print('Posterior mean of row specific covariance matrix:')\n# round(apply(posterior_samples_Sigma_MA,1:2,mean),3)\n# print('Posterior standard deviation of row specific covariance matrix:')\n# round(apply(posterior_samples_Sigma_MA,1:2,sd),3)\n# print('Posterior mean of column specific covariance matrix (first 10 rows and columns):')\n# round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,mean),3)\n# print('Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):')\n# round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,sd),3)\n\n\n\n4.2.2.1 Fitted Models\n\n\nShow code\np = 5\nkappa1 = 0.02^2\nkappa2 = 10^2\ndata = final_df[,2:ncol(final_df)]\nN = ncol(data)\nA_prior = matrix(0,1+p*N,N)\nA_prior[2:(N + 1),] = diag(N)\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nS_prior = diag(1, N)\nnu_prior  = ncol(data)+1\ndata = final_df[,2:ncol(final_df)]\nS1 = 5000\nS2 = 5000\n\nvh0_prior = 5\nsh0_prior = 0.04\nrho_prior = 0.9\nVrho_prior = 0.04\npsi_prior = 0\nVpsi_prior = 1\nfstart = system.time({\n  posterior_samples_MA = BVAR_MA_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)\n})\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.2449\n\n\nShow code\nprint(sprintf(\"Fit MA BVAR on data for %d iterations took: %f seconds\", (S1+S2), start[\"elapsed\"])) \n\n\n[1] \"Fit MA BVAR on data for 10000 iterations took: 3885.405000 seconds\"\n\n\nShow code\nposterior_samples_A_MA = posterior_samples_MA[[1]]\nposterior_samples_Sigma_MA = posterior_samples_MA[[2]]\nposterior_samples_Omega_MA = posterior_samples_MA[[3]]\n# print('Posterior mean of autoregressive parameter:')\n# round(apply(posterior_samples_A_MA,1:2,mean),3)\n# print('Posterior standard deviation of autoregressive parameter:')\n# round(apply(posterior_samples_A_MA,1:2,sd),3)\n# print('Posterior mean of row specific covariance matrix:')\n# round(apply(posterior_samples_Sigma_MA,1:2,mean),3)\n# print('Posterior standard deviation of row specific covariance matrix:')\n# round(apply(posterior_samples_Sigma_MA,1:2,sd),3)\n# print('Posterior mean of column specific covariance matrix (first 10 rows and columns):')\n# round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,mean),3)\n# print('Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):')\n# round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,sd),3)\n\n\n\n\n4.2.2.2 Prediction Plots \n\n\nShow code\nBVAR_MA_predict = function(posterior_samples_A_MA, posterior_samples_Sigma_MA ,posterior_samples_Omega_MA,  data, h){\n  S = dim(posterior_samples_A_MA)[3]\n  N = dim(posterior_samples_A_MA)[2]\n  p = (dim(posterior_samples_A_MA)[1] -1) / N\n  Y = (data[(p+1):nrow(data),])\n  #last p observations \n  x = Y[(nrow(Y)-p+1):nrow(Y),]\n  # reverse the order t-p t-(p-1)...\n  x = x[p:1,]\n  Y_h   = array(NA,c(h,N,S))\n  for (s in 1:S){\n    A     = posterior_samples_A_MA[,,s]\n    Sigma = posterior_samples_Sigma_MA[,,s]\n    Omega = posterior_samples_Omega_MA[]\n    N = ncol(A)\n    p = (nrow(A) -1) / N\n    Y = (data[(p+1):nrow(data),])\n    x  = Y[(nrow(Y)-p+1):nrow(Y),]\n    x = x[p:1,]\n    \n    for (i in 1:h){\n      #x_{{t+1}= (1 y_t y_{t-1},... y_{t-p+1})\n      x_T               = c(1,purrr::as_vector(t(x)))\n      #y_t+h = mvnnorm(1, x_t%*%A)\n      Period_Y               = mvtnorm::rmvnorm(1, mean = x_T%*%A, sigma=Sigma)\n      colnames(Period_Y) = colnames(x)\n      x            = rbind(Period_Y,x[1:(p-1),])\n      Y_h[i,,s]   = Period_Y[1:N]\n      }\n  }\n  return(Y_h)\n}\n\n\n\n\nShow code\ndata = final_df[,2:ncol(final_df)]\nh = 8\nstart = system.time({\n  BVAR_MA_prediction = BVAR_MA_predict(posterior_samples_A_MA = posterior_samples_A_MA, \n                                       posterior_samples_Sigma_MA = posterior_samples_Sigma_MA,\n                                       posterior_samples_Omega_MA = posterior_samples_Omega_MA,\n                                       data = data, h = h)\n}) \nprint(sprintf(\"Predict %d periods ahead MA BVAR on data for %d iterations took: %f seconds\", h, S1 + S2, start[[\"elapsed\"]]))\n\n\n[1] \"Predict 8 periods ahead MA BVAR on data for 10000 iterations took: 278.933000 seconds\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.2.3 Test for Granger Causality\n1. Granger Causality Test for all countries from Q1 1994 to Q3 2023\n\n\nShow code\np = 5\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\n\nS = 5000\nX = data.frame(X) %&gt;% rename_with(~ make.unique(.), .cols = names(.)[duplicated(names(.))])\nX = as.matrix(X)\n##########\ncalc_bayes_factor_MA = function(country1, country2, posterior_samples_A_MA, posterior_samples_Sigma_MA, posterior_samples_Omega_MA, v_prior, S_prior, A_prior, nu_prior, S) {\n#S cannot exceed the number of posterior samples generated\n  S = min(S, dim(posterior_samples_Sigma_MA)[3])\n  ratio_list = list(dim = S)\n  var_list = colnames(data)\n  country1 = var_list[grepl(country1, var_list)]\n  var_list = colnames(X)\n  country2 = var_list[grepl(country2, var_list)]\n  \n  #set dim names for V\n  dimnames(V_prior) = list(colnames(X), colnames(X))\n  \n  #set this in case we want to test less samples than the number of posterior samples generated\n  total_S = dim(posterior_samples_Sigma_MA)[3]\n  #set dim names for A_prior\n  dimnames(A_prior) = list(colnames(X), colnames(data))\n  for (s in 1:S){\n    #calculate prior probability\n    A_prior_test = A_prior[country2, country1]\n    prior_Sigma = MCMCpack::riwish(v = nu_prior, S = S_prior)\n    dimnames(prior_Sigma) = list(colnames(data), colnames(data))\n    #Sigma is row specific covariance matrix\n    prior_Sigma_test = prior_Sigma[colnames(A_prior_test),colnames(A_prior_test)]\n    # V is the column specific covariance matrix\n    V_prior_test = V_prior[rownames(A_prior_test), rownames(A_prior_test)]\n    Q_prior = kronecker(prior_Sigma_test, V_prior_test)\n    prior_prob =  mvtnorm::dmvnorm(x = c(array(0, dim=c(nrow(A_prior_test), ncol(A_prior_test)))), \n                                   c(A_prior_test), \n                                   sigma = Q_prior, log = TRUE)\n    #calculate posterior probability\n    #select the s sampled posterior A\n    #select mean matrix\n    A_0_mat_full = posterior_samples_A_MA[,,s]\n    dimnames(A_0_mat_full) = list(colnames(X), colnames(data))\n    row_names = rownames(A_0_mat_full)\n    A_0_mat_posterior = A_0_mat_full[country2, country1]\n    M = c(posterior_samples_A_MA[,,(total_S - s + 1)])\n    M = array(M, dim = c(nrow(A_0_mat_full), ncol(A_0_mat_full)))\n    dimnames(M) = list(colnames(X), colnames(data))\n    M = M[country2, country1]\n    #select row specific covariance matrix\n    posteior_sigma_test = posterior_samples_Sigma_MA[,,(total_S - s + 1)]\n    dimnames(posteior_sigma_test) = list(colnames(data), colnames(data))\n    posteior_sigma_test = posteior_sigma_test[colnames(A_0_mat_posterior),colnames(A_0_mat_posterior)]\n    #select column specific covariance matrix\n    posterior_V = solve(t(X) %*% posterior_samples_Omega_MA[,,(total_S - s +1)] %*% X + solve(V_prior) + diag(lambda, ncol(X)))\n    dimnames(posterior_V) = list(colnames(X), colnames(X))\n    posterior_V_test = posterior_V[rownames(A_0_mat_posterior), rownames(A_0_mat_posterior)]\n    Q = kronecker(posteior_sigma_test, posterior_V_test)\n    posterior_prob = mvtnorm::dmvnorm(x = c(array(0, dim=c(nrow(A_0_mat_posterior), ncol(A_0_mat_posterior)))), mean = c(M), sigma = Q, log = TRUE)\n    ratio_list[s] = posterior_prob - prior_prob\n  }\n  return(ratio_list)\n}\n\n\n\n\nShow code\nbf_ma = list()\ni = 1\nS = 5000\nposterior_samples_A_MA = posterior_samples_MA[[1]]\nposterior_samples_Sigma_MA = posterior_samples_MA[[2]]\nposterior_samples_Omega_MA = posterior_samples_MA[[3]]\ncountries_list = c('CN', 'US', \"JP\", \"AU\")\nX = data.frame(X) %&gt;% rename_with(~ make.unique(.), .cols = names(.)[duplicated(names(.))])\nX = as.matrix(X)\nbayesfactor_MA = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      i = i+1\n      bayesfactor_MA[country_a, country_b] = mean(unlist(calc_bayes_factor_MA(country_a, country_b,posterior_samples_A_MA, posterior_samples_Sigma_MA, posterior_samples_Omega_MA, V_prior, S_prior, A_prior, nu_prior, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 5000 iterations took: 105.346000 seconds\"\n\n\nShow code\nprint(bayesfactor_MA)\n\n\n           CN           US          JP          AU\nCN -989.16224    21.381086    51.67024    30.72569\nUS   38.07576 -1500.305271    48.82996    34.42572\nJP    9.74343     3.883998 -1559.50719     4.85254\nAU   64.49178    40.120722    79.87182 -2095.26055\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\n2. Granger Causality Test for all countries from Q1 1994 to Q1 2020 (Pre-Covid)\n\n\nShow code\np = 5\ndata = final_df[final_df$Time &lt;= '2020 Q1', 2:ncol(final_df)]\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\nS1 = 5000\nS2 = 5000\nfstart = system.time({\n  posterior_samples_MA = BVAR_MA_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)\n})\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.2836\n\n\nShow code\nprint(sprintf(\"Fit MA BVAR on data for %d iterations took: %f seconds\", (S1+S2), start[\"elapsed\"])) \n\n\n[1] \"Fit MA BVAR on data for 10000 iterations took: 105.346000 seconds\"\n\n\nShow code\nposterior_samples_A_MA = posterior_samples_MA[[1]]\nposterior_samples_Sigma_MA = posterior_samples_MA[[2]]\nposterior_samples_Omega_MA = posterior_samples_MA[[3]]\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nS = 5000\nX = data.frame(X) %&gt;% rename_with(~ make.unique(.), .cols = names(.)[duplicated(names(.))])\nX = as.matrix(X)\ncountries_list = c('CN', 'US', \"JP\", \"AU\")\nbf_ma = list()\ni = 1\nbayesfactor_MA = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      i = i+1\n      bayesfactor_MA[country_a, country_b] = mean(unlist(calc_bayes_factor_MA(country_a, country_b,posterior_samples_A_MA, posterior_samples_Sigma_MA, posterior_samples_Omega_MA, V_prior, S_prior, A_prior, nu_prior, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 5000 iterations took: 98.312000 seconds\"\n\n\nShow code\nprint(bayesfactor_MA)\n\n\n            CN           US          JP           AU\nCN -1291.25334     4.453871    24.57865     6.792789\nUS    15.01675 -1330.320131    34.18523    22.083329\nJP   -14.20843    -9.545975 -1435.93724   -14.467556\nAU    40.69432    24.268737    55.56684 -2010.133978\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nSome evidence\nSome evidence\nStrong Evidence\nSome evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\n3. Granger Causality Test for all countries from Q1 2007 to Q1 2020 (After China joined the WTO and after Covid)\n\n\nShow code\np = 5\ndata = final_df[(final_df$Time &gt;= '2007 Q1') & (final_df$Time &lt;= '2020 Q1'), 2:ncol(final_df)]\n#data = final_df[,2:ncol(final_df)]\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\nS1 = 5000\nS2 = 5000\nfstart = system.time({\n  posterior_samples_MA = BVAR_MA_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)\n})\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.4305\n\n\nShow code\nposterior_samples_A_MA = posterior_samples_MA[[1]]\nposterior_samples_Sigma_MA = posterior_samples_MA[[2]]\nposterior_samples_Omega_MA = posterior_samples_MA[[3]]\nS = 5000\nX = data.frame(X) %&gt;% rename_with(~ make.unique(.), .cols = names(.)[duplicated(names(.))])\nX = as.matrix(X)\ncountries_list = c('CN', 'US', \"JP\", \"AU\")\nbf_ma = list()\ni = 1\nbayesfactor_MA = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      i = i+1\n      bayesfactor_MA[country_a, country_b] = mean(unlist(calc_bayes_factor_MA(country_a, country_b,posterior_samples_A_MA, posterior_samples_Sigma_MA, posterior_samples_Omega_MA, v_prior, S_prior, A_prior, nu_prior, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 5000 iterations took: 78.299000 seconds\"\n\n\nShow code\nprint(bayesfactor_MA)\n\n\n            CN          US         JP          AU\nCN -1006.95740    8.628844   27.98486    13.20351\nUS    12.63275 -615.271480   23.92370    14.99283\nJP   -17.18085  -10.491872 -785.93118   -16.30192\nAU    32.09303   19.954793   46.70976 -1063.42898\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nWeak evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nStrong evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nStrong evidence\nStrong Evidence\n\n\n\n4. Granger Causality Test for all countries from Q1 1994 to Q1 2020 (Pre-Covid)\n\n\nShow code\np = 5\ndata = final_df[grep(\"CN|Time\", colnames(final_df), value = TRUE, invert = TRUE)]\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\nS1 = 5000\nS2 = 5000\nA_prior     = matrix(0, 1+p*N, ncol = N)\nV_prior     = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nS_prior     = diag(1, N)\nnu_prior    = N+1\nfstart = system.time({\n  posterior_samples_MA = BVAR_MA_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)\n})\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.2262\n\n\nShow code\nprint(sprintf(\"Fit MA BVAR on data for %d iterations took: %f seconds\", (S1+S2), start[\"elapsed\"])) \n\n\n[1] \"Fit MA BVAR on data for 10000 iterations took: 78.299000 seconds\"\n\n\nShow code\nposterior_samples_A_MA = posterior_samples_MA[[1]]\nposterior_samples_Sigma_MA = posterior_samples_MA[[2]]\nposterior_samples_Omega_MA = posterior_samples_MA[[3]]\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nS = 5000\nX = data.frame(X) %&gt;% rename_with(~ make.unique(.), .cols = names(.)[duplicated(names(.))])\nX = as.matrix(X)\ncountries_list = c('US', \"JP\", \"AU\")\nbf_ma = list()\ni = 1\nbayesfactor_MA = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      i = i+1\n      bayesfactor_MA[country_a, country_b] = mean(unlist(calc_bayes_factor_MA(country_a, country_b,posterior_samples_A_MA, posterior_samples_Sigma_MA, posterior_samples_Omega_MA, V_prior, S_prior, A_prior, nu_prior, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 5000 iterations took: 43.252000 seconds\"\n\n\nShow code\nprint(bayesfactor_MA)\n\n\n            US          JP        AU\nUS  -6.2751557    8.003204 -25.04430\nJP -19.7414944 -847.093834 -25.07330\nAU  -0.6507542  -65.555830 -38.87381\n\n\n\n\n\n\n\n\n\n\n\n\nUnited States\nJapan\nAustralia\n\n\n\n\nUS\nSome Evidence\nNo Evidence\nSome Evidence\n\n\nJapan\nSome Evidence\nStrong Evidence\nSome Evidence\n\n\nAustralia\nSome Evidence\nNo Evidence\nStrong Evidence"
  }
]