<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rui Liu">

<title>Macroeconometrics Research Report - Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Macroeconometrics Research Report</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-question-objective-and-motivation" id="toc-the-question-objective-and-motivation" class="nav-link active" data-scroll-target="#the-question-objective-and-motivation">1. The Question Objective, and Motivation</a></li>
  <li><a href="#data-and-their-properties" id="toc-data-and-their-properties" class="nav-link" data-scroll-target="#data-and-their-properties">2. Data and their properties</a>
  <ul class="collapse">
  <li><a href="#data-plots" id="toc-data-plots" class="nav-link" data-scroll-target="#data-plots">Data Plots</a></li>
  <li><a href="#stationarity-check" id="toc-stationarity-check" class="nav-link" data-scroll-target="#stationarity-check">Stationarity Check</a></li>
  </ul></li>
  <li><a href="#the-model-and-hypothesis" id="toc-the-model-and-hypothesis" class="nav-link" data-scroll-target="#the-model-and-hypothesis">3. The Model and Hypothesis</a>
  <ul class="collapse">
  <li><a href="#model-1-standard-bvarp-model" id="toc-model-1-standard-bvarp-model" class="nav-link" data-scroll-target="#model-1-standard-bvarp-model">3.1 Model 1: Standard BVAR(p) Model</a>
  <ul class="collapse">
  <li><a href="#model-specification" id="toc-model-specification" class="nav-link" data-scroll-target="#model-specification">3.1.1 Model Specification</a></li>
  <li><a href="#prior-settings" id="toc-prior-settings" class="nav-link" data-scroll-target="#prior-settings">3.1.2 Prior Settings</a></li>
  <li><a href="#posterior-distributions" id="toc-posterior-distributions" class="nav-link" data-scroll-target="#posterior-distributions">3.1.3 Posterior Distributions</a></li>
  <li><a href="#estimation-procedure" id="toc-estimation-procedure" class="nav-link" data-scroll-target="#estimation-procedure">3.1.4 Estimation Procedure</a></li>
  <li><a href="#test-for-granger-causality" id="toc-test-for-granger-causality" class="nav-link" data-scroll-target="#test-for-granger-causality">3.1.5 Test for Granger Causality</a></li>
  </ul></li>
  <li><a href="#model-2-large-bvar-model-with-ma1-gaussian-innovations" id="toc-model-2-large-bvar-model-with-ma1-gaussian-innovations" class="nav-link" data-scroll-target="#model-2-large-bvar-model-with-ma1-gaussian-innovations">3.2 Model 2: Large BVAR model with MA(1) Gaussian Innovations</a>
  <ul class="collapse">
  <li><a href="#model-specification-1" id="toc-model-specification-1" class="nav-link" data-scroll-target="#model-specification-1">3.2.1 Model Specification</a></li>
  <li><a href="#prior-specification" id="toc-prior-specification" class="nav-link" data-scroll-target="#prior-specification">3.2.2 Prior Specification</a></li>
  <li><a href="#posterior-distributions-1" id="toc-posterior-distributions-1" class="nav-link" data-scroll-target="#posterior-distributions-1">3.2.3 Posterior Distributions</a></li>
  <li><a href="#estimation-procedure-1" id="toc-estimation-procedure-1" class="nav-link" data-scroll-target="#estimation-procedure-1">3.2.4 Estimation Procedure</a></li>
  </ul></li>
  <li><a href="#model-3-large-bvar-model-with-common-stochastic-volatility" id="toc-model-3-large-bvar-model-with-common-stochastic-volatility" class="nav-link" data-scroll-target="#model-3-large-bvar-model-with-common-stochastic-volatility">3.3 Model 3: Large BVAR model with common Stochastic Volatility</a></li>
  <li><a href="#model-4-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility" id="toc-model-4-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility" class="nav-link" data-scroll-target="#model-4-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility">3.4 Model 4: Large BVAR model with MA(1) Gaussian innovations and Common Stochastic Volatility</a>
  <ul class="collapse">
  <li><a href="#model-specification-2" id="toc-model-specification-2" class="nav-link" data-scroll-target="#model-specification-2">3.4.1 Model Specification</a></li>
  <li><a href="#prior-specification-1" id="toc-prior-specification-1" class="nav-link" data-scroll-target="#prior-specification-1">3.4.2 Prior Specification</a></li>
  <li><a href="#posterior-distribution" id="toc-posterior-distribution" class="nav-link" data-scroll-target="#posterior-distribution">3.4.3 Posterior Distribution</a></li>
  <li><a href="#estimation-procedure-2" id="toc-estimation-procedure-2" class="nav-link" data-scroll-target="#estimation-procedure-2">3.4.4 Estimation Procedure</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#model-estimation" id="toc-model-estimation" class="nav-link" data-scroll-target="#model-estimation">4. Model Estimation</a>
  <ul class="collapse">
  <li><a href="#standard-bayesian-var" id="toc-standard-bayesian-var" class="nav-link" data-scroll-target="#standard-bayesian-var">4.1 Standard Bayesian VAR</a>
  <ul class="collapse">
  <li><a href="#model-building-code-and-validation" id="toc-model-building-code-and-validation" class="nav-link" data-scroll-target="#model-building-code-and-validation">4.1.1 Model Building Code and Validation</a></li>
  <li><a href="#empirical-result" id="toc-empirical-result" class="nav-link" data-scroll-target="#empirical-result">4.1.2 Empirical Result</a></li>
  </ul></li>
  <li><a href="#large-bvar-model-with-ma1-gaussian-innovations" id="toc-large-bvar-model-with-ma1-gaussian-innovations" class="nav-link" data-scroll-target="#large-bvar-model-with-ma1-gaussian-innovations">4.2 Large BVAR model with MA(1) Gaussian Innovations</a>
  <ul class="collapse">
  <li><a href="#model-building-code-and-validation-1" id="toc-model-building-code-and-validation-1" class="nav-link" data-scroll-target="#model-building-code-and-validation-1">4.2.1 Model Building Code and Validation</a></li>
  <li><a href="#empirical-results" id="toc-empirical-results" class="nav-link" data-scroll-target="#empirical-results">4.2.1 Empirical Results</a></li>
  </ul></li>
  <li><a href="#large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility" id="toc-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility" class="nav-link" data-scroll-target="#large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility">4.4 Large BVAR model with MA(1) Gaussian Innovations and Common Stochastic Volatility</a>
  <ul class="collapse">
  <li><a href="#model-building-code-and-validation-2" id="toc-model-building-code-and-validation-2" class="nav-link" data-scroll-target="#model-building-code-and-validation-2">4.4.1 Model Building Code and Validation</a></li>
  <li><a href="#empirical-result-1" id="toc-empirical-result-1" class="nav-link" data-scroll-target="#empirical-result-1">4.4.2 Empirical Result</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Rui Liu </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="the-question-objective-and-motivation" class="level1">
<h1>1. The Question Objective, and Motivation</h1>
<p><strong>Objective:</strong> Construct a Bayesian Vector Autoregression model to forecast major macroeconomic indicators for the United States, Australia, Japan, and China to facilitate an investigation into the prospective interdependencies between the economies of these nations.</p>
<p><strong>Question:</strong> This research project will examine how trade relationships, investment flows, monetary policy environments, and economic performances within the United States, Australia, Japan and China mutually influence each other, and assess the implications of these interactions for predicting future values of these economic indicators.</p>
<p><strong>Motivation:</strong> Since the onset of the COVID-19 pandemic, the global economic landscape has witnessed a series of unprecedented shifts in key macroeconomic indicators, spurred by governments‚Äô adoption of varied expansionary monetary policies. Initially, to buffer their economies, many nations implemented expansive monetary strategies, later swiftly transitioning to interest rate hikes in a bid to manage surging inflation rates‚Äîa scenario not seen in decades. The pandemic‚Äôs disruption to trade further exacerbated inflationary pressures for some economies, highlighting the intricate interdependencies among major economies with significant trade and financial ties. This period recorded stark contrast in inflation levels, with unprecedented highs in the US and Australia and notably low inflation in China and Japan. Amidst this turmoil, a divergence in economic paths also became apparent, thei9 United States and Australia have witness robust economic rebounds, whereas China and Japan saw more tepid recoveries. This research aims to dissect the nuanced web of economic interdependencies between the United States, Australia, Japan, and China, analyzing how their trade relationships, investment flows, and monetary policy environments have mutually influenced their economic performances. Additionally, it seeks to understand the ramifications of these dynamics for the predictive accuracy of future economic indicators, offering insights into the evolving global economic order.</p>
</section>
<section id="data-and-their-properties" class="level1">
<h1>2. Data and their properties</h1>
<p><strong>Proposed Dataset:</strong> This study is uses data from the International Monetary Fund‚Äôs (IMF) database. The IMF data offers a comprehensive collection of global economic information; including several key databases such as the World Economic Outlook Databases, International Financial Statistics (IFS), Government Finance Statistics. This analysis will predominantly focus on the IFS database, which is composed of a sizeable collection of financial and economic data from across the global, featuring 1,681 distinct indicators such as consumer price index, interest rates, exchange rates, national accounts, government finance statistics. The data is available in various frequencies ‚Äì annual, semi-annual, quarterly, monthly, daily, and weekly. As this research is primarily focused on analyzing macroeconomic data that are published on a monthly or quarterly basis, quarterly data from Q1 1994 to Q3 2023 will be used. The analysis will examine key macroeconomic variables including consumer price indexes, foreign direct investments, exchange rates, balance of payments, interest rates and the national gross domestic product of the United States, Australia, Japan, and China. sf <strong>Variables and Motivation:</strong></p>
<table class="table">
<colgroup>
<col style="width: 45%">
<col style="width: 19%">
<col style="width: 18%">
<col style="width: 6%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th>Variables</th>
<th>Original Unit</th>
<th>Final Unit</th>
<th>Mnemonic</th>
<th>Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Prices, Consumer Price Index, All items, Previous period</td>
<td>% Change</td>
<td>% Change</td>
<td>% Change</td>
<td>CPI</td>
</tr>
<tr class="even">
<td>Exchange Rates, Domestic Currency per U.S. Dollar, Period Average</td>
<td>per US dollar</td>
<td>per US dollar</td>
<td>XCH</td>
<td>ENDA_XDC_USD_R</td>
</tr>
<tr class="odd">
<td>Gross Domestic Product, Nominal, Seasonally Adjusted</td>
<td>Domestic Currency (millions)</td>
<td>log(US dollar)</td>
<td>GDP</td>
<td>NGDP_NSA_XDC</td>
</tr>
</tbody>
</table>
<p>The variables included in this study were chosen with the objective to include key economic indicators that are susceptible to changes in other nations while also ensuring relatively consistent measures exist for all four nations. These variables were chosen not only for their ability to provide insights into the trade relations, investment dynamics and monetary policy frameworks, but also for their roles as barometers of overall economic health and performance. Exchange rates directly impact trade balances and investment flows, influencing economic performances. By examining the volatility and trends in exchange rates, insights can be gleaned into how monetary policies and economic conditions in one country can affect its trade partners. GDP is the ultimate measure of economic performance, encapsulating the outcome of various economic activities and policies. Analyzing GDP allows for assessing economic momentum and comparing growth rates across countries and over time, offering a clear picture of economic health and trends. CPI was included because it is a crucial indicator of inflation, reflecting changes in the cost of living and purchasing power, and thus, is helpful in quantifying economic stability and monetary policy effectiveness in each nation. The presence of cyclical trends in the variables, alongside the observed impact of lagged values on future outcomes, highlights the suitability of the Bayesian Vector Autoregression model for our analysis. This model can well capture the temporal dynamics and interdependencies inherent in these economic indicators, offering a robust framework for understanding the nuanced interactions and feedback loops that characterize their behavior over time.</p>
<section id="data-plots" class="level2">
<h2 class="anchored" data-anchor-id="data-plots">Data Plots</h2>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="1152"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="stationarity-check" class="level2">
<h2 class="anchored" data-anchor-id="stationarity-check">Stationarity Check</h2>
<p><strong>Stationary Tests</strong></p>
<p>The Augmented Dickey-Fuller Test is used in this section to test the null hypothesis that a unit root is present in the time series and the time series is non-stationary.</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<caption>ADF Test Results for CPI Data by Country</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Dickey-Fuller Statistic</th>
<th style="text-align: left;">Lag Order</th>
<th style="text-align: left;">P-value</th>
<th style="text-align: left;">Country</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">-3.258408</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.08079872</td>
<td style="text-align: left;">JP_CPI</td>
</tr>
<tr class="even">
<td style="text-align: left;">-4.299916</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.01</td>
<td style="text-align: left;">CN_CPI</td>
</tr>
<tr class="odd">
<td style="text-align: left;">-3.55799</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.0394958</td>
<td style="text-align: left;">US_CPI</td>
</tr>
<tr class="even">
<td style="text-align: left;">-3.333834</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.06822768</td>
<td style="text-align: left;">AU_CPI</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The lag order chosen in the ADF test is 4, which is appropriate given our data is of quarterly frequency. The result of the ADF test on the CPI data shows that we do not have enough evidence to reject the null hypothesis that the CPI series is unit root non-stationary at 1% significance level, except for China.</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<caption>ADF Test Results for Foreign Direct Investment (% change) Data by Country</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Dickey-Fuller Statistic</th>
<th style="text-align: left;">Lag Order</th>
<th style="text-align: left;">P-value</th>
<th style="text-align: left;">Country</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">-2.26286</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.4686993</td>
<td style="text-align: left;">JP_FDI</td>
</tr>
<tr class="even">
<td style="text-align: left;">-2.7189</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.285367</td>
<td style="text-align: left;">CN_FDI</td>
</tr>
<tr class="odd">
<td style="text-align: left;">-3.130793</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.1197816</td>
<td style="text-align: left;">US_FDI</td>
</tr>
<tr class="even">
<td style="text-align: left;">-1.607673</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.7320914</td>
<td style="text-align: left;">AU_FDI</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The ADF results shows that we do not have enough evidence to reject the null hypothesis that the foreign direct investment data is not unit-root stationary for Australia, the United States and Japan at 1% significance level and we do have enough evidence to reject the null hypothesis for China at 1% significance level.</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<caption>ADF Test Results for Exchange Rate (% change) Data by Country</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Dickey-Fuller Statistic</th>
<th style="text-align: left;">Lag Order</th>
<th style="text-align: left;">P-value</th>
<th style="text-align: left;">Country</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">-1.791111</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.6635047</td>
<td style="text-align: left;">JP_XCH</td>
</tr>
<tr class="even">
<td style="text-align: left;">-1.565957</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.7575635</td>
<td style="text-align: left;">CN_XCH</td>
</tr>
<tr class="odd">
<td style="text-align: left;">-2.56175</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.3415666</td>
<td style="text-align: left;">AU_XCH</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The result of the ADF test on the exchange rate data shows that we do not have enough evidence to reject the null hypothesis that the exchange rate against the dollar series is unit root non-stationary at 1% significance level.</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<caption>ADF Test Results for Balance of Payments (% change) Data by Country</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Dickey-Fuller Statistic</th>
<th style="text-align: left;">Lag Order</th>
<th style="text-align: left;">P-value</th>
<th style="text-align: left;">Country</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">-2.914308</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.2025054</td>
<td style="text-align: left;">JP_BOP</td>
</tr>
<tr class="even">
<td style="text-align: left;">-2.50488</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.3695164</td>
<td style="text-align: left;">CN_BOP</td>
</tr>
<tr class="odd">
<td style="text-align: left;">-2.533578</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.3578101</td>
<td style="text-align: left;">US_BOP</td>
</tr>
<tr class="even">
<td style="text-align: left;">-2.058287</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.5516879</td>
<td style="text-align: left;">AU_BOP</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The result of the ADF test on the balance of payments data shows that we do not have enough evidence to reject the null hypothesis that the balance of payments series is unit root non-stationary at 1% significance level for all countries.</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<caption>ADF Test Results for GDP (% change) Data by Country</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Dickey-Fuller Statistic</th>
<th style="text-align: left;">Lag Order</th>
<th style="text-align: left;">P-value</th>
<th style="text-align: left;">Country</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">-3.499383</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.04535076</td>
<td style="text-align: left;">JP_GDP</td>
</tr>
<tr class="even">
<td style="text-align: left;">-2.472818</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.3802514</td>
<td style="text-align: left;">CN_GDP</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.7030938</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.99</td>
<td style="text-align: left;">US_GDP</td>
</tr>
<tr class="even">
<td style="text-align: left;">-2.030497</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">0.5639204</td>
<td style="text-align: left;">AU_GDP</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The result of the ADF test on the GDP data shows that we do not have enough evidence to reject the null hypothesis that the GDP series is unit root non-stationary at 5% significance level.</p>
</section>
</section>
<section id="the-model-and-hypothesis" class="level1">
<h1>3. The Model and Hypothesis</h1>
<p>We will employ four models to address our proposed problem. Firstly, we will use a standard Bayesian Vector Autoregressive (BVAR) model with independently and identically distributed innovations, as outlined in details in <span class="citation" data-cites="wozniakBsvarsBayesianEstimation2022">Wo≈∫niak (<a href="#ref-wozniakBsvarsBayesianEstimation2022" role="doc-biblioref">2022</a>)</span>. Additionally, we will investigate a large BVAR model with flexible error covariance structures, following the methodology proposed by <span class="citation" data-cites="Chan_2015">Chan (<a href="#ref-Chan_2015" role="doc-biblioref">2015</a>)</span>. Specifically, the second model will incorporate MA(1) Gaussian innovations to better account for potential model misspecifications such as omitted variable bias and to facilitate shrinkage in VAR coefficients. Our third model will be a BVAR incorporating common stochastic volatilities to allow for time varying distribution of volatility terms, which will be useful to handle the impact of specific events like the Covid-19 Global Pandemic. The final model will combine a common stochastic volatility frame work with MA(1) Gaussian innovations, offering a robust approach to volatility modelling.</p>
<section id="model-1-standard-bvarp-model" class="level2">
<h2 class="anchored" data-anchor-id="model-1-standard-bvarp-model">3.1 Model 1: Standard BVAR(p) Model</h2>
<section id="model-specification" class="level3">
<h3 class="anchored" data-anchor-id="model-specification">3.1.1 Model Specification</h3>
<p><span class="math display">\[ Y = XA+E\]</span> <span class="math display">\[E|X \sim \mathcal{MN}_{T \times N}(0_{T \times N},\Sigma_{N \times N}, I_T)\]</span> <span class="math display">\[
A = \begin{bmatrix}
\mu_0^T \\
A_1^T \\
\vdots \\
A_p^T
\end{bmatrix}
, \quad
Y = \begin{bmatrix}
y_1^T \\
y_2^T \\
\vdots \\
y_T^T
\end{bmatrix}
, \quad
x_t = \begin{bmatrix}
1 \\
y_{t-1}^T \\
\vdots \\
y_{t-p}^T
\end{bmatrix}
, \quad
X = \begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_T^T
\end{bmatrix}
, \quad
E = \begin{bmatrix}
e_1^T \\
e_2^T \\
\vdots \\
e_T^T
\end{bmatrix}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(T\)</span> is the number of time periods under consideration</li>
<li><span class="math inline">\(N\)</span> is the number of variables, in our case, N = 20</li>
<li><span class="math inline">\(P\)</span> is the number of lags</li>
<li><span class="math inline">\(Y\)</span> is a <span class="math inline">\(T \times N\)</span> matrix of variables of response variables we aim to model.</li>
<li><span class="math inline">\(A\)</span> is a <span class="math inline">\(K \times N\)</span> matrix of coefficients, <span class="math inline">\(K = (1+ùëù\times N)\)</span>.</li>
<li><span class="math inline">\(E\)</span> is a <span class="math inline">\(T \times N\)</span> matrix of the error terms</li>
<li><span class="math inline">\(X\)</span> is a <span class="math inline">\(T \times (1+ùëù\times N)\)</span> matrix of covariates</li>
<li><span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(N \times N\)</span> matrix representing the row-specific covariance matrix</li>
<li><span class="math inline">\(I_T\)</span> is a <span class="math inline">\(T \times T\)</span> identity matrix representing the column specific covariance matrix</li>
<li><span class="math inline">\(E|X\)</span> follows a matrix-variate normal distribution with mean <span class="math inline">\(0_{T \times N}\)</span>, row specific covariance matrix <span class="math inline">\(\Sigma_{N \times N}\)</span> and column specific covariance matrix <span class="math inline">\(I_T\)</span></li>
<li><span class="math inline">\(x_{t}^T = \left( \begin{array}{cccc}1  &amp; y_{t-1} &amp; y_{t-2} &amp; \cdots &amp; y_{t-p} \end{array} \right)\)</span></li>
</ul>
<p>In our specific application, the Y matrix is formulated as follows:</p>
<p><span class="math display">\[
Y = \begin{pmatrix}
    \text{CPI}_{\text{CN}, p+1} &amp; \text{XCH}_{\text{CN}, p+1} &amp; \log(\text{GDP})_{\text{CN}, p+1} &amp; \text{CPI}_{\text{US}, p+1} &amp; \log(\text{GDP})_{\text{US}, p+1} &amp; \text{CPI}_{\text{JP}, p+1} &amp; \text{XCH}_{\text{JP}, p+1} &amp; \log(\text{GDP})_{\text{JP}, p+1} &amp; \text{CPI}_{\text{AU}, p+1} &amp; \text{XCH}_{\text{AU}, p+1} &amp; \log(\text{GDP})_{\text{AU}, p+1} \\
    \text{CPI}_{\text{CN}, p+2} &amp; \text{XCH}_{\text{CN}, p+2} &amp; \log(\text{GDP})_{\text{CN}, p+2} &amp; \text{CPI}_{\text{US}, p+2} &amp; \log(\text{GDP})_{\text{US}, p+2} &amp; \text{CPI}_{\text{JP}, p+2} &amp; \text{XCH}_{\text{JP}, p+2} &amp; \log(\text{GDP})_{\text{JP}, p+2} &amp; \text{CPI}_{\text{AU}, p+2} &amp; \text{XCH}_{\text{AU}, p+2} &amp; \log(\text{GDP})_{\text{AU}, p+2} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
    \text{CPI}_{\text{CN}, T} &amp; \text{XCH}_{\text{CN}, T} &amp; \log(\text{GDP})_{\text{CN}, T} &amp; \text{CPI}_{\text{US}, T} &amp; \log(\text{GDP})_{\text{US}, T} &amp; \text{CPI}_{\text{JP}, T} &amp; \text{XCH}_{\text{JP}, T} &amp; \log(\text{GDP})_{\text{JP}, T} &amp; \text{CPI}_{\text{AU}, T} &amp; \text{XCH}_{\text{AU}, T} &amp; \log(\text{GDP})_{\text{AU}, T}
\end{pmatrix}
\]</span></p>
<p>The Bayesian Vector Autoregression model as formulated above provides a robust framework for investigating the relationships among selected economic indicators across different nations. By employing this model, this research aims to quantitatively measure the influence of one country‚Äôs economic indicators on another, such as how lagged changes in China‚Äôs i consumer price index may influence the GDP growth rate of the United States and vice versa. The BVAR model, with its estimation of coefficients across various lags, offers a deep understanding of both immediate and more delayed economic interactions, which is crucial to analyzing the cyclical nature of trade relationships, investment flows, monetary policy environments, and economic performances and the transmission of these metrics across borders.</p>
<p>The strength of this BVAR model lies in its ability to incorporate prior economic knowledge and beliefs into the estimation process. By setting prior distributions for the matrix of coefficients A and the covariance matrix <span class="math inline">\(\Sigma\)</span>, the model can be tailors to reflect established economic theories regarding international economic linkages and the time it takes for policy changes in one country to affect another. By calibrating the prior variances, particularly for the autoregressive coefficients, we can integrate prior knowledge or hypotheses, such as the presence of unit roots or the diminishing influence of distant lags on current values, into the analysis. When interpreting the estimation output, attention will be given to the posterior means and variances of the coefficients, which represent the model‚Äôs ‚Äúlearnt‚Äù understanding of the underlying economic structure. The analysis will be supplemented by forecast error variance decompositions to better understand the proportion of the movements in economic indicators that can be accounted for by their own shocks versus shocks to other variables.</p>
<p>The economic context underscoring this analysis is the increased globalization over the past decade, marking an era where economies are more intertwined than ever through trade, capital flows, and policy decisions. This period has witnessed not only the strengthening of global economic ties but also recent calls from political leaders advocating for a reduction in globalization. These contrasting dynamics highlight the complexity of the current global economic landscape, where the push for deeper integration coexists with growing sentiments for retrenchment. This dual trend sets the stage for our investigation, providing a rich context to explore how economic variables across nations influence each other amidst fluctuating levels of global interconnectedness. In this environment, understanding the cross-country spillover effects is vital for policymakers and businesses alike, as decisions made in one country can have far-reaching implications. By addressing these aspects, this research will contribute to the discourse on economic policy formulation, risk assessment, and strategic planning.</p>
</section>
<section id="prior-settings" class="level3">
<h3 class="anchored" data-anchor-id="prior-settings">3.1.2 Prior Settings</h3>
<p>We will employ a Normal-Inverse Wishart distribution for the joint distribution of coefficient matrices A and the row-specific variance matrix <span class="math inline">\(\Sigma\)</span>, and a Minnesota prior on the coefficients A. Specifically, we have:</p>
<p><span class="math display">\[\Sigma \sim \mathcal{IW}(S_0, \nu_0) \]</span></p>
<p><span class="math display">\[p(\Sigma) \propto |\Sigma|^{-\frac{\nu_0+N+1}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}S_0))\]</span></p>
<p><span class="math display">\[A|\Sigma \sim \mathcal{MN}_{K \times N}(A_0, \Sigma, V_A)\]</span></p>
<p><span class="math display">\[p(A|\Sigma) \propto |\Sigma|^{-\frac{K}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^T(A-A_0)))\]</span></p>
<p><span class="math display">\[(A,\Sigma) \sim \mathcal{NIW}_{K \times N} (A_0, V_A, S_0, \nu_0)\]</span></p>
<p><span class="math display">\[p(A,\Sigma) \propto |\Sigma|^{-\frac{K+\nu_0+N+1}{2}} \times exp(-\frac{1}{2}tr(\Sigma^{-1}S_0))\times exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^T(A-A_0)))\]</span></p>
<p>where</p>
<p><span class="math display">\[V_A = diag(\kappa_2 \quad \kappa_1(\mathbf{p} \otimes I_N^T))\]</span></p>
<p><span class="math display">\[\mathbf{p} = [1 \quad 2 \quad ... \quad p]\]</span> <span class="math display">\[I_N = [1 \quad 1 \quad ... \quad 1] \in \mathbb{R}^N\]</span> <span class="math display">\[\kappa_1 \text{ is the overall shrinkage level for autoregressive slopes}\]</span> <span class="math display">\[\kappa_2 \text{ is the overall shrinkage lvel for the constant term }\]</span></p>
<p>Additionally, we adopt commonly used values for the hyperparameters as established in the literature.</p>
<p><span class="math display">\[A_0 = 0\]</span></p>
<p><span class="math display">\[v_0 = N+3\]</span></p>
<p><span class="math display">\[S_0 = I_N\]</span></p>
<p><span class="math display">\[\kappa_1 = 0.2^2 \quad \kappa_2 = 10^2\]</span></p>
<p>The hyperparameters <span class="math inline">\(\kappa_1\)</span> and <span class="math inline">\(\kappa_2\)</span> are specified in a way such that the coefficient associated with a lag l variable is shrunk more heavily as lag length increases whereas the intercepts are not shrunk to 0.</p>
</section>
<section id="posterior-distributions" class="level3">
<h3 class="anchored" data-anchor-id="posterior-distributions">3.1.3 Posterior Distributions</h3>
<p>The posterior distribution specified above has the form</p>
<p><span class="math display">\[p(Y|A, \Sigma) = (2\pi)^{-\frac{TN}{2}}|\Sigma|^{-\frac{T}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T(Y-XA))\]</span></p>
<p>and the joint posterior distribution</p>
<p><span class="math display">\[p(A, \Sigma \mid Y)  = \frac{p(A,\Sigma,Y)}{p(Y)}\propto p(A, \Sigma, Y) \propto p(Y|A,\Sigma)\times p(A,\Sigma) =  p(Y|A,\Sigma) p(A \mid \Sigma) p(\Sigma)\]</span></p>
<p><span class="math display">\[\propto |\Sigma|^{-\frac{T}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T(Y-XA))) \times\]</span></p>
<p><span class="math display">\[\mid \Sigma \mid^{-\frac{\nu_0+N+K}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}S_o))exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\]</span></p>
<p><span class="math display">\[\propto |\Sigma|^{-\frac{T+N+K+\nu_0+1}{2}} \times \exp (-\frac{1}{2}tr(\Sigma^{-1}S_0)) \times \]</span></p>
<p><span class="math display">\[exp(-\frac{1}{2}tr(\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^TY-\bar{A}^T\bar{V}^{-1}\bar{A})\times\]</span> <span class="math display">\[exp(-\frac{1}{2}tr(\Sigma^{-1}(A-\bar{A})^T\bar{V}^{-1}(A-\bar{A})))\]</span> Hence,</p>
<p><span class="math display">\[p(A, \Sigma \mid Y, X) = p(A \mid Y, X, \Sigma) p(\Sigma \mid Y, X)\]</span></p>
<p><span class="math display">\[p(A \mid Y, X, \Sigma) = \mathcal{MN}_{k \times N}(\bar{A}, \Sigma, \bar{V})\]</span></p>
<p><span class="math display">\[p(\Sigma \mid Y, X) = \mathcal{IW}_N(\bar{S}, \bar{\nu})\]</span></p>
<p>where</p>
<p><span class="math display">\[\bar{V} = (X^TX + V_A^{-1})^{-1}\]</span></p>
<p><span class="math display">\[\bar{A} = \bar{V}(X^TY + V_A^{-1}A_0)\]</span></p>
<p><span class="math display">\[\bar{\nu} = T + \nu_0\]</span></p>
<p><span class="math display">\[\bar{S} = S_0 + Y^TY + A_0^TV_A^{-1}A_0 - \bar{A}^T\bar{V}^{-1}\bar{A}\]</span></p>
</section>
<section id="estimation-procedure" class="level3">
<h3 class="anchored" data-anchor-id="estimation-procedure">3.1.4 Estimation Procedure</h3>
<p>In this setting, we have</p>
<ul>
<li><span class="math inline">\((A,\Sigma) \sim \mathcal{NIW}_{K \times N} (A_0, V_A, S_0, \nu_0)\)</span></li>
</ul>
<p>then, prior draws can be sampled from</p>
<ul>
<li><span class="math inline">\(\Sigma \sim \mathcal{IW}(S_0, \nu_0)\)</span></li>
<li><span class="math inline">\(A|\Sigma \sim \mathcal{MN}_{K \times N}(A_0, \Sigma, V_A)\)</span></li>
</ul>
<p>we use the following Gibb‚Äôs Sampler algorithm to sample from the posterior distribution:</p>
<ul>
<li>Initialize <span class="math inline">\(\Sigma\)</span> at <span class="math inline">\(\Sigma^0\)</span></li>
<li>For <span class="math inline">\(s = 1,...,S_1+S_2\)</span>
<ol type="1">
<li>Draw a sample <span class="math inline">\(A^{(s)}\)</span> from <span class="math inline">\(p(A \mid Y, X, \Sigma^{(s-1)}) \sim \mathcal{MN}_{K \times N}(\bar{A}, \Sigma, \bar{V})\)</span></li>
<li>Draw a sample <span class="math inline">\(\Sigma^{(s)}\)</span> from <span class="math inline">\(p(\Sigma \mid Y, X, A^{(s)}) = \mathcal{IW}_N(\bar{S}, \bar{\nu})\)</span></li>
</ol></li>
</ul>
<p>We discard the first <span class="math inline">\(S_1\)</span> sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain <span class="math inline">\(S_2\)</span> sampled draws from the joint posterior distribution.</p>
<p><span class="math display">\[\left\{A^{(s)}, \Sigma^{(s)}\right\}_{s=S_1+1}^{S_1+S_2}\]</span></p>
<p>The draws from joint predictive density can then be obtained using the following algorithm:</p>
<ol type="1">
<li>Sample S draws <span class="math inline">\(\left\{ A^{(s)}, \Sigma^{(s)} \right\}_{s=1}^{S}\)</span> from <span class="math inline">\(p(A,\Sigma|Y, X)\)</span></li>
<li>Sample S draws <span class="math inline">\(\left\{ Y_{t+h}^{(s)} \right\}_{s=1}^S\)</span> from <span class="math inline">\(Y_{t+h}^{(s)} \sim \mathcal{N}_{hN}(Y_{t+h|t}(A^{(s)}), \mathbb{V}ar[Y_{t+h|t}|A^{(s)}, \Sigma^{(s)}])\)</span></li>
</ol>
<p>where</p>
<p><span class="math display">\[\underset{\text{hN} \times1}{Y_{t+h}} = \begin{pmatrix}
    Y_{t+1} \\
    Y_{t+2} \\
    \vdots    \\
    Y_{t+h}
\end{pmatrix}\]</span></p>
<p>To derive the posterior predictive density for <span class="math inline">\(Y_{t+h}\)</span>, we first note that:</p>
<p><span class="math display">\[p(Y_{t+h}|Y_{t}) = \int \int p(Y_{t+h}|Y_{t},A, \Sigma)\times p(A,\Sigma|Y,X)dAd\Sigma\]</span> where</p>
<p><span class="math display">\[p(y_{t+h}|Y, X, A, \Sigma, I_T) \sim \mathcal{N}_{hN} (Y_{t+h|t}(A), Var(Y_{t+h}|A, \Sigma))\]</span></p>
<p><span class="math display">\[Y_{t+h|t}(A) =\begin{pmatrix}
    Y_{t+1|t} \\
    Y_{t+2|t} \\
    \vdots    \\
    Y_{t+h|t}
\end{pmatrix} = \begin{pmatrix}  
\mu_0 + A_1 Y_t + \cdots + A_pY_{t-p+1|t} \\
\mu_0 + A_1 Y_{t+1|t} + \cdots + A_p Y_{t-p+2|t}\\
\vdots \\
\mu_0 + A_1 Y_{t+h-1|t} + \cdots + A_p Y_{t+h-p|t}
\end{pmatrix}\]</span></p>
<p><span class="math display">\[\underset{\text{(hN-1)} \times \text{(hN-1)}}{\mathbb{V}ar(Y_{t+h|t}|A, \Sigma)} = \begin{pmatrix}
    \Sigma &amp; \Sigma \phi_1^T &amp; \cdots &amp; \Sigma \phi_{h-1}^T \\
    \phi_1\Sigma &amp;\Sigma + \phi_1 \Sigma \phi_1^T  &amp; \cdots &amp; \Sigma \phi_1^T + \phi_1 \Sigma \phi_2^T+ \cdots + \phi_{h-2}\Sigma \phi_{h-1}^T + \phi_{h-1} \Sigma \phi_{h}^T \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    \phi_{h-1}\Sigma\ &amp; \phi_1\Sigma  + \phi_2 \Sigma \Phi_1^T + \cdots + \phi_{h} \Sigma \phi_{h-1}^T &amp; \cdots &amp; \Sigma + \phi_1 \Sigma \phi_1^T + \cdots+\phi_{h-2}\Sigma\phi_{h-2}^T+\phi_{h-1}\Sigma\phi_{h-1}^T
\end{pmatrix}
\]</span></p>
<p>where <span class="math inline">\(\phi_i = J A^i J'\)</span> are the parameters of the VMA(<span class="math inline">\(\infty\)</span>) representation of VAR(<span class="math inline">\(p\)</span>) and <span class="math inline">\(A\)</span> is the parameter matrix of <span class="math inline">\(VAR(1)\)</span> representation of VAR(<span class="math inline">\(p\)</span>). <span class="math inline">\(A^i\)</span> is the matrix <span class="math inline">\(A\)</span> raised to the power of <span class="math inline">\(i\)</span>. <span class="math display">\[
\underset{Np \times Np}{A} =
\begin{pmatrix} A_1 &amp; A_2 &amp; \cdots &amp; A_{p-1} &amp; A_p \\
I_N &amp; 0_{N \times N} &amp; \cdots &amp; 0_{N \times N} &amp; 0_{N \times N}  \\
0 &amp; I_N  &amp; \cdots &amp; 0 &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; I_N &amp; 0 \end{pmatrix}
\qquad
\underset{\text{N} \times \text{Np}}{J} = [I_N \quad 0_{N \times N(p-1)}]
\]</span> <span class="math display">\[p(A|Y, X, \Sigma) \sim \mathcal{MN}_{K \times N}(\bar{A}, \Sigma, \bar{V})\]</span> <span class="math display">\[p(\Sigma \mid Y, X) = \mathcal{IW}_N(\bar{S}, \bar{\nu})\]</span> as before. In this approach, we are incorporating into our prediction the uncertainties in the estimates of the parameters by integrating over <span class="math inline">\(A\)</span> and <span class="math inline">\(\Sigma\)</span>.</p>
</section>
<section id="test-for-granger-causality" class="level3">
<h3 class="anchored" data-anchor-id="test-for-granger-causality">3.1.5 Test for Granger Causality</h3>
<p>Granger causality testing, introduced in <span class="citation" data-cites="Granger_1969">Granger (<a href="#ref-Granger_1969" role="doc-biblioref">1969</a>)</span>, is a way to measure whether the past and current values of one time series contains useful information for predicting future values of another. The idea behind granger causality testing is that if a time series does contain predictive information about another, incorporating it into the model should reduce the error variance and improve the precision of the forecasts. The classical approach for testing causality is the Wald test, but it fails to quantify the degree of evidence in the data in favour of or against the causality hypothesis. The Bayesian approach to testing Granger causality based on Baye‚Äôs factor, on the other hand, has the ability to quantify the evidence for and against the hypothesis with a single number. The Baye‚Äôs factor is defined as:</p>
<p><span class="math display">\[B_H = \frac{p[H_0|Y]}{p[H_1|Y]}\]</span> In the context of testing for VAR(p) models, we have the marginal likelihood under the assumption of no Granger causality is:</p>
<p><span class="math display">\[p_0(Y) = \int p_1(Y| A_{ij} = 0, A_{-(ij)})dA_{-(ij)} = p_1(Y|A_{ij}=0)\]</span> using Baye‚Äôs rule, we have: <span class="math display">\[p_0(Y) = \frac{p_1(A_{ij} = 0|Y) \times p_1(Y)}{p_1(A_{ij} = 0)}\]</span> We divide the above equation by <span class="math inline">\(p(y_t|H_1)\)</span> to get Baye‚Äôs factor:</p>
<p><span class="math display">\[B_H = \frac{p_0(Y)}{p_1(Y)} = \frac{p_1(A_{ij} = 0|Y)}{p_1(A_{ij} = 0)}=\frac{\int p_1(A_{ij=0}|Y, \Sigma)d \Sigma}{\int p(A_{ij}=0, \Sigma)d\Sigma}\]</span></p>
<p>and the integration is performed using numerical integration methods. The second part is also known as the Savage-Dickey density ratio estimator, which is the ratio of the posterior over the prior density evaluated at <span class="math inline">\(A_{ij} = 0\)</span>. Indicative values for interpreting Bayes factors is provided below:</p>
<p>To test for Granger causality, we use the equation: <span class="math display">\[B_H = \frac{p_1(A_{ij} = 0|Y)}{p(A_{ij}=0)} \]</span> A value much greater than 1 would indicate that the posterior distribution has a higher probability density at <span class="math inline">\(A_{ij} = 0\)</span> than the prior, hence the data provides evidence for the null hypothesis that <span class="math inline">\(A_{ij} = 0\)</span>. Conversely, a value much smaller than 1 would suggest that the data provides evidence against the null hypothesis.</p>
</section>
</section>
<section id="model-2-large-bvar-model-with-ma1-gaussian-innovations" class="level2">
<h2 class="anchored" data-anchor-id="model-2-large-bvar-model-with-ma1-gaussian-innovations">3.2 Model 2: Large BVAR model with MA(1) Gaussian Innovations</h2>
<p>Incorporating MA(1) Gaussian innovations in a large BVARs model can lead to a significant enhancement over traditional BVAR models, especially when forecasting macroeconomic variables, for several reasons. By allowing for serial correlation in the innovations term, we will be able to capture the momentum or persistence in economic variables that is often observed in real-world data. Recognizing that shocks may have a lasting impact over several periods can enhance the model‚Äôs ability to predict future values by considering the path-dependent nature of the economy. In addition, our BVAR model with common stochastic volatility is a natural extension to the standard BVAR model and formulated as below:</p>
<section id="model-specification-1" class="level3">
<h3 class="anchored" data-anchor-id="model-specification-1">3.2.1 Model Specification</h3>
<p><span class="math display">\[ Y = XA+E\]</span> <span class="math display">\[E|X \sim \mathcal{MN}_{T \times N}(0_{T \times N},\Sigma_{N \times N}, \Omega_{T \times T})\]</span> <span class="math display">\[
A = \begin{bmatrix}
\mu_0^T \\
A_1^T \\
\vdots \\
A_p^T
\end{bmatrix}
, \quad
Y = \begin{bmatrix}
y_1^T \\
y_2^T \\
\vdots \\
y_T^T
\end{bmatrix}
, \quad
x_t = \begin{bmatrix}
1 \\
y_{t-1}^T \\
\vdots \\
y_{t-p}^T
\end{bmatrix}
, \quad
X = \begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_T^T
\end{bmatrix}
, \quad
E = \begin{bmatrix}
e_1^T \\
e_2^T \\
\vdots \\
e_T^T
\end{bmatrix}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(T\)</span> is the number of time periods under consideration</li>
<li><span class="math inline">\(N\)</span> is the number of variables, in our case, N = 20</li>
<li><span class="math inline">\(P\)</span> is the number of lags</li>
<li><span class="math inline">\(Y\)</span> is a <span class="math inline">\(T \times N\)</span> matrix of variables of response variables we aim to model.</li>
<li><span class="math inline">\(A\)</span> is a <span class="math inline">\(K \times N\)</span> matrix of coefficients, <span class="math inline">\(K = (1+ùëù\times N)\)</span>.</li>
<li><span class="math inline">\(E\)</span> is a <span class="math inline">\(T \times N\)</span> matrix of the error terms</li>
<li><span class="math inline">\(X\)</span> is a <span class="math inline">\(T \times (1+ùëù\times N)\)</span> matrix of covariates</li>
<li><span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(N \times N\)</span> matrix representing the row-specific covariance matrix</li>
<li><span class="math inline">\(\Omega\)</span> is a <span class="math inline">\(T \times T\)</span> identity matrix representing the column specific covariance matrix</li>
<li><span class="math inline">\(E|X\)</span> follows a matrix-variate normal distribution with mean <span class="math inline">\(0_{T \times N}\)</span>, row specific covariance matrix <span class="math inline">\(\Sigma_{N \times N}\)</span> and column specific covariance matrix <span class="math inline">\(I_T\)</span></li>
</ul>
<p>with MA(1) innovations, we have, for i = 1,‚Ä¶.,N and t = 1,‚Ä¶,T, <span class="math display">\[e_{t, i} = \eta_{t, i} + \psi \eta_{t-1, i}\]</span> where <span class="math inline">\(|\psi|&lt;1\)</span> and <span class="math inline">\(\eta_{t,i} \sim \mathcal{N}(0,1)\)</span></p>
<p>In matrix notation, we have:</p>
<p><span class="math display">\[e_i = H_{\psi} \eta_i\]</span></p>
<p>where</p>
<p><span class="math display">\[
e_i = \begin{bmatrix}
e_{1, i} \\
e_{2, i} \\
\vdots \\
e_{T, i}
\end{bmatrix} \qquad
\eta_i = \begin{bmatrix}
\eta_{1, i} \\
\eta_{2, i} \\
\vdots \\
\eta_{T, i}
\end{bmatrix} \qquad
H_{\psi} = \left(
\begin{array}{cccc}
1 &amp; 0  &amp; \cdots &amp; 0 \\
\psi &amp; 1  &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \psi &amp; 1
\end{array}
\right) \qquad
O_{\psi} = diag(1+\psi^2, 1, ..., 1)
\]</span></p>
<p>Hence, we have</p>
<p><span class="math display">\[E \sim \mathcal{N}(0, H_{\psi}O_{\psi}H_{\psi}^T) \qquad \qquad \Omega = H_{\psi}O_{\psi}H_{\psi}^T\]</span> Note that the covariance matrix <span class="math inline">\(\Omega\)</span> depends on <span class="math inline">\(\psi\)</span> only.</p>
</section>
<section id="prior-specification" class="level3">
<h3 class="anchored" data-anchor-id="prior-specification">3.2.2 Prior Specification</h3>
<p>We consider a prior independent distributions for <span class="math inline">\((A, \Sigma, \Omega)\)</span>, specifically, we have: <span class="math display">\[P(A, \Sigma, \Omega) = P(A, \Sigma) \times P(\Omega)\]</span></p>
<p>We will employ a Normal-Inverse Wishart distribution for the joint distribution of A and <span class="math inline">\(\Sigma\)</span> and before, <span class="math display">\[(A,\Sigma) \sim \mathcal{NIW}_{K \times N}(A_0, V_A, S_0, \nu_0)\]</span></p>
<p>We apply a truncated normal prior on <span class="math inline">\(\psi\)</span>, <span class="math display">\[\psi \sim \mathcal{N}(\psi_0, V_{\psi})\mathbb{1}_{\{|\psi|&lt;1\}}\]</span></p>
<p>For estimation purposes, we initialize with the following setting:</p>
<ul>
<li><span class="math inline">\(e_{i1} \sim \mathcal{N}(0, 1+\psi^2)\)</span></li>
<li><span class="math inline">\(\psi_0 = 0\)</span> and <span class="math inline">\(V_{\psi} = 1\)</span> so that the prior centers around 0 with a relatively large variance and has support within (-1, 1)</li>
</ul>
</section>
<section id="posterior-distributions-1" class="level3">
<h3 class="anchored" data-anchor-id="posterior-distributions-1">3.2.3 Posterior Distributions</h3>
<ol type="1">
<li>The posterior distribution of Y specified above has the form:</li>
</ol>
<p><span class="math display">\[p(Y|A, \Sigma) = (2\pi)^{-\frac{Tn}{2}}|\Sigma|^{-\frac{T}{2}}|\Omega|^{-\frac{N}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T\Omega^{-1}(Y-XA))\]</span></p>
<p><span class="math display">\[= (2\pi)^{-\frac{Tn}{2}}|\Sigma|^{-\frac{T}{2}}(1+\psi^2)^{-\frac{N}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T(H_{\psi}O_{\psi}H_{\psi}^T)^{-1}(Y-XA))\]</span></p>
<ol start="2" type="1">
<li>The joint posterior distribution of A and <span class="math inline">\(\Sigma\)</span> can be derived as follows:</li>
</ol>
<p><span class="math display">\[p(A, \Sigma \mid Y, \Omega)  = \frac{p(A,\Sigma,Y, \Omega)}{p(Y, \Omega)}\]</span> <span class="math display">\[\propto p(A, \Sigma, Y, \Omega) \propto p(Y \mid A,\Sigma, \Omega)\times p(A,\Sigma, \Omega)\]</span> <span class="math display">\[=  p(Y \mid A,\Sigma, \Omega) p(A, \Sigma) p(\Omega) = p(Y \mid A,\Sigma, \Omega) p(A \mid \Sigma) p(\Sigma) p(\Omega)\]</span></p>
<p><span class="math display">\[\propto p(Y \mid A,\Sigma, \Omega) p(A \mid \Sigma) p(\Sigma)\]</span></p>
<p><span class="math display">\[\propto |\Sigma|^{-\frac{T}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T\Omega^{-1}(Y-XA)) \times\]</span></p>
<p><span class="math display">\[\mid \Sigma \mid^{-\frac{\nu_0+N+K}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}S_o))exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\]</span></p>
<p><span class="math display">\[\propto |\Sigma|^{-\frac{T+N+K+\nu_0+1}{2}} \times \exp (-\frac{1}{2}tr(\Sigma^{-1}S_0)) \times \]</span> <span class="math display">\[exp(-\frac{1}{2}tr(\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\Omega^{-1}Y-\bar{A}^T\bar{V}^{-1}\bar{A})\times\]</span> <span class="math display">\[exp(-\frac{1}{2}tr(\Sigma^{-1}(A-\bar{A})^T\bar{V}^{-1}(A-\bar{A})))\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[p(A, \Sigma \mid Y, X) = p(A \mid Y, X, \Sigma) p(\Sigma \mid Y, X)\]</span></p>
<p><span class="math display">\[p(A \mid Y, X, \Sigma, \Omega) = \mathcal{MN}_{K \times N}(\bar{A}, \Sigma, \bar{V})\]</span></p>
<p><span class="math display">\[p(\Sigma \mid Y, X) = \mathcal{IW}_N(\bar{S}, \bar{\nu})\]</span></p>
<p>where</p>
<p><span class="math display">\[\bar{V} = (X^T\Omega^{-1}X + V_A^{-1})^{-1}\]</span></p>
<p><span class="math display">\[\bar{A} = \bar{V}(X^T\Omega^{-1}Y + V_A^{-1}A_0)\]</span></p>
<p><span class="math display">\[\bar{\nu} = T + \nu_0\]</span></p>
<p><span class="math display">\[\bar{S} = S_0 + Y^T\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \bar{A}^T\bar{V}^{-1}\bar{A}\]</span> 3. The posterior distribution for the parameter <span class="math inline">\(\psi\)</span> can be obtained as follows: <span class="math display">\[P(\psi|Y, A, \Sigma) = \frac{P(\psi, Y, A, \Sigma)}{P(Y, A, \Sigma)} \propto P(\psi, Y, A, \Sigma) = P(Y|A, \Sigma, \psi) \times P(A,\Sigma, \psi) \]</span></p>
<p><span class="math display">\[= P(Y|A, \Sigma, \psi) \times P(A, \Sigma) \times P(\psi) \propto P(Y|A, \Sigma, \psi) \times P(\psi)\]</span></p>
<p>we can sample from the posterior distribution of <span class="math inline">\(\psi\)</span> using an independence-chain Metropolis-Hastings algorithm.</p>
</section>
<section id="estimation-procedure-1" class="level3">
<h3 class="anchored" data-anchor-id="estimation-procedure-1">3.2.4 Estimation Procedure</h3>
<p>We obtain posterior estimates of <span class="math inline">\(A, \Sigma, \psi\)</span> using a Gibbs sampler, specifically, we initialize <span class="math inline">\(\psi^{(0)}\)</span> and for s = 1,‚Ä¶,S1+S2, we sequentially sample:</p>
<ul>
<li><span class="math inline">\(\Sigma^{(s)} | Y, X, \psi^{(s-1)} \sim \mathcal{IW}_N(\bar{S}, \bar{\nu})\)</span></li>
<li><span class="math inline">\(A^{(s)} | Y, X, \Sigma^{(s)}, \psi^{(s-1)} \sim \mathcal{MN}_{k \times N}(\bar{A}, \Sigma^{(s)}, \bar{V})\)</span></li>
<li><span class="math inline">\(\psi^{(s)} | Y, X, A^{(s)}, \Sigma^{(s)} \propto P(Y|A^{(s)}, \Sigma^{(s)}, \psi^{s-1})\times p(\psi)\)</span></li>
</ul>
<p>The Metropolis-Hastings Algorithm for Sampling <span class="math inline">\(\psi\)</span> is given as follows:</p>
<p><strong>Initialization:</strong></p>
<ul>
<li>Choose an initial value <span class="math inline">\(\psi^{(0)}\)</span> within the bounds <span class="math inline">\((-1, 1)\)</span>.</li>
</ul>
<p><strong>Proposal Distribution:</strong></p>
<ul>
<li>Select the proposal distribution <span class="math inline">\(q(\psi' | \psi^{(t)}) \sim N(\psi^{(t)}, \tau^2)\)</span>. <span class="math inline">\(\tau^2\)</span> is a tuning parameter that controls the step size.</li>
</ul>
<p><strong>Sampling Loop:</strong></p>
<p>For t = 1,‚Ä¶,<span class="math inline">\(T_1+T_2\)</span></p>
<ul>
<li><p>Generate a candidate <span class="math inline">\(\psi'^{(t)}\)</span> from <span class="math inline">\(q(\psi' | \psi^{(t-1)})\)</span>.</p></li>
<li><p>Check if <span class="math inline">\(\psi'\)</span> is within the bounds <span class="math inline">\((-1, 1)\)</span>. If not, reject <span class="math inline">\(\psi'\)</span> (set <span class="math inline">\(\alpha = 0\)</span>).</p></li>
<li><p>Compute the acceptance ratio <span class="math inline">\(\alpha\)</span>: <span class="math display">\[ \alpha(\psi^{(t-1)}, \psi') = \min\left(1, \frac{p(Y | A, \Sigma, \psi') p(\psi') q(\psi^{(t-1)} | \psi')}{p(Y | A, \Sigma, \psi^{(t-1)}) p(\psi^{(t-1)}) q(\psi' | \psi^{(t-1)})}\right)\]</span></p></li>
</ul>
<p><strong>Decide to accept or reject:</strong></p>
<ul>
<li><p>Generate a random number <span class="math inline">\(u\)</span> from <span class="math inline">\(U[0,1]\)</span>.</p></li>
<li><p>If <span class="math inline">\(u \leq \alpha\)</span>, accept <span class="math inline">\(\psi'\)</span> and set <span class="math inline">\(\psi^{(t)} = \psi'\)</span>.</p></li>
<li><p>Otherwise, reject <span class="math inline">\(\psi'\)</span> and set <span class="math inline">\(\psi^{(t)} = \psi^{(t-1)}\)</span>.</p></li>
</ul>
<p><strong>Burn in period</strong></p>
<ul>
<li>Discard the first <span class="math inline">\(T_1\)</span> samples to allow the algorithm to converge to the true distribution</li>
</ul>
<p><strong>Obtain one sample of <span class="math inline">\(\psi\)</span></strong></p>
<ul>
<li>Randomly draw a <span class="math inline">\(\psi^*\)</span> from the <span class="math inline">\(T_2\)</span> draws and set <span class="math inline">\(\psi^{(s)} = \psi^*\)</span></li>
</ul>
<p>When combined with Gibbs sampling for the estimation of <span class="math inline">\(A\)</span> and <span class="math inline">\(\Sigma\)</span>, the Metropolis-Hastings step can be embedded into the Gibbs sampler, a method known as Metropolis-within-Gibbs sampling. In this approach, one sample from the Metropolis-Hastings step is generated per Gibbs iteration. Detailed proofs and explanations on the convergence properties and efficiency of the Metropolis-within-Gibbs sampling method are provided in <span class="citation" data-cites="Chib1995">Chib and Greenberg (<a href="#ref-Chib1995" role="doc-biblioref">1995</a>)</span>.</p>
<p>We monitor the acceptance rate and adjust <span class="math inline">\(\tau^2\)</span> as necessary to achieve an optimal rate of about 20-40%.</p>
<p>We note that since <span class="math inline">\(\Omega\)</span> is a band matrix, which means we do not need compute <span class="math inline">\(\Omega^{-1}\)</span>. Instead, we obtain the Cholesky decomposition <span class="math inline">\(C_{\Omega}\)</span> of <span class="math inline">\(\Omega\)</span>, which has a time complexity of <span class="math inline">\(O(T)\)</span> instead of <span class="math inline">\(O(T^3)\)</span>. Terms involving <span class="math inline">\(\Omega^{-1}\)</span> such as <span class="math inline">\(X^T\Omega^{-1}X\)</span> can be obtained by: <span class="math display">\[X^T\Omega^{-1}X = X^T(C_{\Omega}^{-1})^{T}C_{\Omega}^{-1}X=(C_{\Omega}^{-1}X)^T(C_{\Omega}^{-1}X) = \tilde{X}^T\tilde{X}\]</span></p>
<p>To make forecasts of the <span class="math inline">\(Y\)</span>, we make the following observations: <em>One-period ahead forecast</em> <span class="math display">\[y_{t+1} = \mu_0 + A_1y_t + \cdots + A_py_{t-p+1} +  e_{t+1}\]</span> <span class="math display">\[E(y_{t+1}) = E(\mu_0 + A_1y_t + \cdots + A_py_{t-p+1} +  e_{t+1}) = \mu_0 + A_1y_t + \cdots + A_py_{t-p+1}\]</span> The one-period ahead forecast error is: <span class="math display">\[e_{t+1|t} = y_{t+1} - y_{t+1|t} = e_{t+1}\]</span> The one period ahead forecast variance is: <span class="math display">\[\mathbb{V}ar(e_{t+1}|t) = \mathbb{E}[\mathbb{E}_t(e_{t+1}e_{t+1}^T)] = \Sigma\]</span> <em>Two-period ahead forecast</em> <span class="math display">\[y_{t+2} = \mu_0 + A_1y_{t+1} + \cdots + A_py_{t-p+2} +  e_{t+2}\]</span> <span class="math display">\[E(y_{t+2}) = E(\mu_0 + A_1y_{t+1} + \cdots + A_py_{t-p+2} +  e_{t+2}) = \mu_0 + A_1y_{t+1|t} + \cdots + A_py_{t-p+2}\]</span> The two-period ahead forecast error is: <span class="math display">\[e_{t+2|t} = y_{t+2}-y_{t+2|t} = e_{t+2} + A_1(y_{t+1} - y_{t+1|t}) = e_{t+2} + A_1e_{t+1}\]</span> <span class="math display">\[Var(e_{t+2|t}) = E[e_{t+2|t}e_{t+2|t}^T] = E((e_{t+2} + A_1e_{t+1})(e_{t+2} + A_1e_{t+1})^T) = E((e_{t+2}e_{t+2}^T + A_1e_{t+1}e_{t+2}^T+e_{t+2}e_{t+1}^TA_1^T+A_1e_{t+1}e_{t+1}^TA_1^T)\]</span> <span class="math display">\[ = \Sigma + A_i\Psi+ \Psi A_1^T + A_1\Sigma A_1^T\]</span> since in the MA(1) innovations setup, we have:</p>
<p><span class="math display">\[\mathbb{V}ar(e_{t+2} e_{t+1}^T) = E \left[
\begin{pmatrix}
e_{t+2, 1} \\
e_{t+2, 2} \\
\vdots \\
e_{t+2, N}
\end{pmatrix}
\begin{pmatrix}
e_{t+1, 1} &amp; e_{t+1, 2} &amp; \cdots &amp; e_{t+1, N}
\end{pmatrix}
\right] \]</span> <span class="math display">\[=
E \left[
\begin{pmatrix}
e_{t+2, 1}e_{t+1, 1} &amp; e_{t+2, 1}e_{t+1, 2} &amp; \cdots &amp; e_{t+2, 1}e_{t+1, N}\\
e_{t+2, 2}e_{t+1, 1} &amp; e_{t+2, 2}e_{t+1, 2} &amp; \cdots &amp; e_{t+2, 2}e_{t+1, N}\\
\vdots               &amp;    \vdots            &amp; \vdots &amp;    \vdots          \\
e_{t+2, N}e_{t+2, 1} &amp; e_{t+2, N}e_{t+2, 2} &amp; \vdots &amp;    e_{t+2,N}e_{t+1,N}
\end{pmatrix}
\right]\]</span> <span class="math display">\[ = E \left[
\begin{pmatrix}
(\eta_{t+2, 1} + \psi \eta_{t+1, 1})(\eta_{t+1, 1} + \psi \eta_{t, 1}) &amp; (\eta_{t+2, 1} + \psi \eta_{t+1, 1})(\eta_{t+1, 2}+\psi \eta_{t,2}) &amp; \cdots &amp; (\eta_{t+2, 1} + \psi \eta_{t+1, 1})(\eta_{t,N}+\psi\eta_{t,N})) \\
(\eta_{t+2, 2} + \psi \eta_{t+1, 2})(\eta_{t+1, 1} + \psi \eta_{t, 1}) &amp; (\eta_{t+2, 2} + \psi \eta_{t+1, 2})(\eta_{t+1, 2}+\psi \eta_{t,2}) &amp; \cdots &amp; (\eta_{t+2, 2} + \psi \eta_{t+1, 2})(\eta_{t,N}+\psi\eta_{t,N})) \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
(\eta_{t+2, N} + \psi \eta_{t+1, N})(\eta_{t+1, 1} + \psi \eta_{t, 1}) &amp; (\eta_{t+2, N} + \psi \eta_{t+1, N})(\eta_{t+1, 2}+\psi \eta_{t,2}) &amp; \cdots &amp; (\eta_{t+2, N} + \psi \eta_{t+1, N})(\eta_{t,N}+\psi\eta_{t,N}))\
\end{pmatrix}
\right]
\]</span> <span class="math display">\[= \begin{pmatrix}
\psi\mathbb{E}[\eta_{t+1,1}^2] &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; \psi\mathbb{E}[\eta_{t+1,2}^2] &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \cdots &amp; \psi\mathbb{E}[\eta_{t+1,N}^2]
\end{pmatrix} = \begin{pmatrix}
\psi &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; \psi &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \cdots &amp; \psi
\end{pmatrix} = \Psi\]</span></p>
<p>since <span class="math inline">\(\eta_{t,i} \overset{iid}{\sim}\mathcal{N}(0,1)\)</span></p>
</section>
</section>
<section id="model-3-large-bvar-model-with-common-stochastic-volatility" class="level2">
<h2 class="anchored" data-anchor-id="model-3-large-bvar-model-with-common-stochastic-volatility">3.3 Model 3: Large BVAR model with common Stochastic Volatility</h2>
<p>The variances of economic shocks are rarely constant over time. Volatility tends to cluster during periods of economic crisis and becomes more tranquil during stable times. Thus, treating the innovations as having constant variance is an unrealistic assumption in practice. To remedy the situation, we can incorporate stochastic volatility into the model, thereby allowing the model to adapt to changing volatility in the data. This typically lead to improvement in model performance, especially in the presence of financial market instability or shifts in economic policy. A BVAR model with common stochastic volatility is s specified as follows:</p>
<p><span class="math display">\[ Y = XA+E\]</span> <span class="math display">\[
A = \begin{bmatrix}
\mu_0^T \\
A_1^T \\
\vdots \\
A_p^T
\end{bmatrix}
, \quad
Y = \begin{bmatrix}
y_1^T \\
y_2^T \\
\vdots \\
y_T^T
\end{bmatrix}
, \quad
x_t = \begin{bmatrix}
1 \\
y_{t-1}^T \\
\vdots \\
y_{t-p}^T
\end{bmatrix}
, \quad
X = \begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_T^T
\end{bmatrix}
, \quad
E = \begin{bmatrix}
e_1^T \\
e_2^T \\
\vdots \\
e_T^T
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[E|X \sim \mathcal{MN}_{T \times N}(0_{T \times N},\Sigma_{N \times N}, \Omega_{T \times T})\]</span> where</p>
<p><span class="math display">\[e_t \sim \mathcal{N}(0, e^{h_t}\Sigma)\]</span> and <span class="math inline">\(h_t\)</span> follows an AR(1) process. <span class="math display">\[h_t = \rho h_{t-1} + u_t^h\]</span> <span class="math display">\[u_t^h \sim N(0, \sigma^2_h)\]</span></p>
</section>
<section id="model-4-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility" class="level2">
<h2 class="anchored" data-anchor-id="model-4-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility">3.4 Model 4: Large BVAR model with MA(1) Gaussian innovations and Common Stochastic Volatility</h2>
<section id="model-specification-2" class="level3">
<h3 class="anchored" data-anchor-id="model-specification-2">3.4.1 Model Specification</h3>
<p><span class="math display">\[ Y = XA+E\]</span></p>
<p>where</p>
<p><span class="math display">\[
A = \begin{bmatrix}
\mu_0^T \\
A_1^T \\
\vdots \\
A_p^T
\end{bmatrix}
, \quad
Y = \begin{bmatrix}
y_1^T \\
y_2^T \\
\vdots \\
y_T^T
\end{bmatrix}
, \quad
x_t = \begin{bmatrix}
1 \\
y_{t-1}^T \\
\vdots \\
y_{t-p}^T
\end{bmatrix}
, \quad
X = \begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_T^T
\end{bmatrix}
, \quad
E = \begin{bmatrix}
e_1^T \\
e_2^T \\
\vdots \\
e_T^T
\end{bmatrix}
\]</span></p>
<p>the same as before. But, instead of the column specific matrix as the identity matrix, we specify the column specific matrix as a diagonal matrix <span class="math inline">\(\Omega\)</span>. Specifically, we have:</p>
<p><span class="math display">\[\epsilon_t = u_t + \psi_1 u_{t-1}\]</span></p>
<p><span class="math display">\[u_t \sim \mathcal{N}(0,e^{h_t} \Sigma)\]</span></p>
<p><span class="math display">\[h_t = \rho h_{t-1} + u_t^h \quad \text{ follows an Autoregressive process of lag 1 AR(1), and}\]</span></p>
<p><span class="math display">\[u_t^h \sim N(0,\sigma_h^2)\]</span></p>
<p><span class="math display">\[\Omega = \left(
\begin{array}{cccc}
(1 + \psi_1^2) e^{h_1} &amp; \psi_1 e^{h_1} &amp; \cdots &amp; 0 \\
\psi_1 e^{h_1} &amp; \psi_1^2 e^{h_1} + e^{h_2} &amp; \cdots &amp; \vdots \\
0 &amp; \cdots &amp; \ddots &amp; \vdots \\
\vdots &amp; \cdots &amp; \psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} &amp; \psi_1 e^{h_{T-1}} \\
0 &amp; \cdots &amp; \psi_1 e^{h_{T-1}} &amp; \psi_1^2 e^{h_{T-1}} + e^{h_T}
\end{array}
\right)\]</span></p>
<p>In this specification, each element of <span class="math inline">\(e_t\)</span> may have distinct variances, and the variances of all innovations can be scaled by a common factor. This approach is economically intuitive, as the volatility of macroeconomic variables often exhibit co-movement. It is important to emphasize that each component of <span class="math inline">\(e_t\)</span> must adhere to the same univariate time series model.</p>
</section>
<section id="prior-specification-1" class="level3">
<h3 class="anchored" data-anchor-id="prior-specification-1">3.4.2 Prior Specification</h3>
<p>Here, we consider a priori independent distributions for <span class="math inline">\((A, \Sigma, \Omega)\)</span>, namely:</p>
<p><span class="math display">\[p(A, \Sigma, \Omega) = p(A, \Sigma) \times p(\Omega)\]</span> Given this structure, we can sample from the posterior distribution by sequentially sampling from:</p>
<ul>
<li><span class="math inline">\(P(A, \Sigma | Y, X, \Omega)\)</span></li>
<li><span class="math inline">\(P(\Omega | Y, X, A, \Sigma)\)</span></li>
</ul>
<p>The prior distribution of <span class="math inline">\((A,\Sigma)\)</span> follow the same normal inverse Wishart prior distribution as outlined in model one. But the variance matrix <span class="math inline">\(V_A\)</span> for A is different:</p>
<p><span class="math display">\[V_A = diag(v_{A,ii})\]</span></p>
<p><span class="math display">\[v_{A,ii} = \begin{cases}
\kappa_1(\frac{l^2}{\hat{s}_r}) &amp; \text{for a coefficient associated to lag l of variable r} \\
\kappa_2&amp; \text{for an intercept}
\end{cases}\]</span></p>
<p>where <span class="math inline">\(\hat{s}_r\)</span> is the sample variance of an AR(4) model for the variable r.</p>
<p><span class="math display">\[E|X \sim \mathcal{MN}_{T \times N}(0_{T \times N},\Sigma_{N \times N}, \Omega_{T \times T})\]</span></p>
<p>and,</p>
<p>For the moving average coefficients, we adopt an uninformative truncated normal prior for the MA coefficient <span class="math inline">\(\psi\)</span>:</p>
<p><span class="math display">\[
\psi \sim \mathcal{N}(\psi_0, V_\psi) \mathbb{1}(|\psi|&lt;1)
\]</span></p>
<p>and we set <span class="math inline">\(\psi_0 = 0\)</span> and <span class="math inline">\(V_\psi = 1\)</span> so that the prior centers around 0 with a relatively large variance and has support within (-1,1). Further more, we assume independent priors for <span class="math inline">\(\sigma^2_h\)</span> and <span class="math inline">\(\rho\)</span>:</p>
<p><span class="math display">\[\sigma_h^2 \sim \mathcal{IG}(\nu_{h_0}, s_{h_0})\]</span></p>
<p><span class="math display">\[\rho \sim \mathcal{N}(\rho_0, V_\rho) \mathbb{1}(|\rho|&lt;1)\]</span></p>
<p>We set the hyperparameters <span class="math inline">\(\nu_{h_0} = 5\)</span>, <span class="math inline">\(s_{h_0} = 0.04\)</span>, <span class="math inline">\(\rho_0 = 0.9\)</span> and <span class="math inline">\(V_\rho = 0.04\)</span> so that the prior mean of <span class="math inline">\(\sigma_h^2\)</span> is 0.01 and <span class="math inline">\(\rho\)</span> is centered at 0.9.</p>
</section>
<section id="posterior-distribution" class="level3">
<h3 class="anchored" data-anchor-id="posterior-distribution">3.4.3 Posterior Distribution</h3>
<p>The posterior distribution specified above has the following form:</p>
<p><span class="math display">\[p(Y|A, \Sigma) = (2\pi)^{-\frac{Tn}{2}}|\Sigma|^{-\frac{T}{2}}|\Omega|^{-\frac{N}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T\Omega^{-1}(Y-XA))\]</span></p>
<p>and the joint posterior distribution</p>
<p><span class="math display">\[p(A, \Sigma \mid Y, \Omega)  = \frac{p(A,\Sigma,Y, \Omega)}{p(Y, \Omega)}\]</span> <span class="math display">\[\propto p(A, \Sigma, Y, \Omega) \propto p(Y \mid A,\Sigma, \Omega)\times p(A,\Sigma, \Omega)\]</span> <span class="math display">\[=  p(Y \mid A,\Sigma, \Omega) p(A, \Sigma) p(\Omega) = p(Y \mid A,\Sigma, \Omega) p(A \mid \Sigma) p(\Sigma) p(\Omega)\]</span></p>
<p><span class="math display">\[\propto p(Y \mid A,\Sigma, \Omega) p(A \mid \Sigma) p(\Sigma)\]</span></p>
<p><span class="math display">\[\propto |\Sigma|^{-\frac{T}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T\Omega^{-1}(Y-XA)) \times\]</span></p>
<p><span class="math display">\[\mid \Sigma \mid^{-\frac{\nu_0+N+K}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}S_o))exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\]</span></p>
<p><span class="math display">\[\propto |\Sigma|^{-\frac{T+N+K+\nu_0+1}{2}} \times \exp (-\frac{1}{2}tr(\Sigma^{-1}S_0)) \times \]</span> <span class="math display">\[exp(-\frac{1}{2}tr(\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\Omega^{-1}Y-\bar{A}^T\bar{V}^{-1}\bar{A})\times\]</span> <span class="math display">\[exp(-\frac{1}{2}tr(\Sigma^{-1}(A-\bar{A})^T\bar{V}^{-1}(A-\bar{A})))\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[p(A, \Sigma \mid Y, X) = p(A \mid Y, X, \Sigma) p(\Sigma \mid Y, X)\]</span></p>
<p><span class="math display">\[p(A \mid Y, X, \Sigma, \Omega) = \mathcal{MN}_{k \times N}(\bar{A}, \Sigma, \bar{V})\]</span></p>
<p><span class="math display">\[p(\Sigma \mid Y, X) = \mathcal{IW}_N(\bar{S}, \bar{\nu})\]</span></p>
<p>where</p>
<p><span class="math display">\[\bar{V} = (X^T\Omega^{-1}X + V_A^{-1})^{-1}\]</span></p>
<p><span class="math display">\[\bar{A} = \bar{V}(X^T\Omega^{-1}Y + V_A^{-1}A_0)\]</span></p>
<p><span class="math display">\[\bar{\nu} = T + \nu_0\]</span></p>
<p><span class="math display">\[\bar{S} = S_0 + Y^T\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \bar{A}^T\bar{V}^{-1}\bar{A}\]</span></p>
</section>
<section id="estimation-procedure-2" class="level3">
<h3 class="anchored" data-anchor-id="estimation-procedure-2">3.4.4 Estimation Procedure</h3>
<p>In this setting, we have</p>
<ul>
<li><span class="math inline">\((A,\Sigma) \sim \mathcal{NIW}_{K \times N} (A_0, V_A, S_0, \nu_0)\)</span></li>
<li><span class="math inline">\(p(A, \Sigma, \Omega) = p(A, \Sigma) \times p(\Omega)\)</span></li>
</ul>
<p>then, prior draws can be sampled from</p>
<ul>
<li><p><span class="math inline">\(\Sigma \sim \mathcal{IW}(S_0, \nu_0)\)</span></p></li>
<li><p><span class="math inline">\(A|\Sigma \sim \mathcal{MN}_{K \times N}(A_0, \Sigma, V_A)\)</span></p></li>
<li><p><span class="math inline">\(\Omega = \left(
\begin{array}{cccc}
(1 + \psi_1^2) e^{h_1} &amp; \psi_1 e^{h_1} &amp; \cdots &amp; 0 \\
\psi_1 e^{h_1} &amp; \psi_1^2 e^{h_1} + e^{h_2} &amp; \cdots &amp; \vdots \\
0 &amp; \cdots &amp; \ddots &amp; \vdots \\
\vdots &amp; \cdots &amp; \psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} &amp; \psi_1 e^{h_{T-1}} \\
0 &amp; \cdots &amp; \psi_1 e^{h_{T-1}} &amp; \psi_1^2 e^{h_{T-1}} + e^{h_T}
\end{array}
\right)\)</span></p></li>
<li><p><span class="math inline">\(\epsilon_t = u_t + \psi_1 u_{t-1}\)</span></p></li>
<li><p><span class="math inline">\(\psi \sim \mathcal{N}(\psi_0, V_\psi) \mathbb{1}(|\psi|&lt;1)\)</span></p></li>
<li><p><span class="math inline">\(u_t \sim \mathcal{N}(0,e^{h_t} \Sigma)\)</span></p></li>
<li><p><span class="math inline">\(h_t = \rho h_{t-1} + u_t^h\)</span></p></li>
<li><p><span class="math inline">\(\rho \sim \mathcal{N}(\rho_0, V_\rho) \mathbb{1}(|\rho|&lt;1)\)</span></p></li>
<li><p><span class="math inline">\(u_t^h \sim N(0,\sigma_h^2)\)</span></p></li>
<li><p><span class="math inline">\(\sigma_h^2 \sim \mathcal{IG}(\nu_{h_0}, s_{h_0})\)</span></p></li>
</ul>
<p>To sample <span class="math inline">\(S_1+S_2\)</span> draws of <span class="math inline">\(\left\{\Omega^{(s)}\right\}_{s=1}^{S_1+S_2}\)</span></p>
<ol type="1">
<li>Sample <span class="math inline">\(S_1+S_2\)</span> draws of <span class="math inline">\(\left\{\sigma^{2,(s)}_h\right\}_{s=1}^{S_1+S_2}\)</span> from <span class="math inline">\(\mathcal{IG}(v_{h_0}, s_{h_0})\)</span></li>
<li>Sample <span class="math inline">\(S_1+S_2\)</span> draws of <span class="math inline">\(\left\{\rho^{(s)}\right\}_{s=1}^{S_1+S_2}\)</span> from <span class="math inline">\(\mathcal{N}(\rho_0, V_\rho) \mathbb{1}(|\rho|&lt;1)\)</span></li>
<li>For each <span class="math inline">\(\sigma^{2,(s)}_h\)</span>, sample <span class="math inline">\(\left\{u_t^{h,(s)}\right\}_{t=1}^T\)</span> from <span class="math inline">\(N(0,\sigma_h^{2,(s)})\)</span></li>
<li>For t = 1,‚Ä¶,T and s = 1,‚Ä¶, <span class="math inline">\(S_1+S_2\)</span>, compute <span class="math inline">\(h_t^{(s)} = \rho h_{t-1}^{(s)} + u_t^{h,(s)}\)</span></li>
<li>Sample <span class="math inline">\(S_1+S_2\)</span> draws of <span class="math inline">\(\left\{u_t^{(s)}\right\}_{s=1}^{S_1+S_2}\)</span> from <span class="math inline">\(u_t \sim \mathcal{N}(0,e^{h_t^{(s)}} \Sigma)\)</span> for t = 1,‚Ä¶,T</li>
<li>Sample <span class="math inline">\(S_1+S_2\)</span> draws of <span class="math inline">\(\left\{\psi^{(s)}\right\}_{s=1}^{S_1+S_2}\)</span> from <span class="math inline">\(\mathcal{N}(\psi_0, V_\psi) \mathbb{1}(|\psi|&lt;1)\)</span></li>
<li>for each t = 1,‚Ä¶,T and s = 1,‚Ä¶<span class="math inline">\(S_1+S_2\)</span>, compute <span class="math inline">\(\epsilon_t^{(s)} = u_t^{(s)} + \psi^{(s)}u_{t-1}^{(s)}\)</span></li>
</ol>
<p>After we have obtained <span class="math inline">\(\left\{\Omega^{(s)}\right\}_{s=1}^{S_1+S_2}\)</span>, we can use the following Gibb‚Äôs Sampler algorithm to sample from the posterior distribution <span class="math inline">\(p(A \mid Y, X, \Sigma, \Omega)\)</span>:</p>
<ul>
<li>Initialize <span class="math inline">\(\Sigma\)</span> at <span class="math inline">\(\Sigma^0\)</span></li>
<li>For <span class="math inline">\(s = 1,...,S_1+S_2\)</span>
<ol type="1">
<li>Draw a sample <span class="math inline">\(A^{(s)}\)</span> from <span class="math inline">\(p(A \mid Y, X, \Omega^{(s)}, \Sigma^{(s-1)}) \sim \mathcal{MN}_{k \times N}(\bar{A}, \Sigma, \bar{V})\)</span></li>
<li>Draw a sample <span class="math inline">\(\Sigma^{(s)}\)</span> from <span class="math inline">\(p(\Sigma \mid Y, X, A^{(s)}, \Omega^{(s)}) = \mathcal{IW}_N(\bar{S}, \bar{\nu})\)</span></li>
</ol></li>
</ul>
<p>We discard the first <span class="math inline">\(S_1\)</span> sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain <span class="math inline">\(S_2\)</span> sampled draws from the joint posterior distribution.</p>
<p><span class="math display">\[\left\{A^{(s)}, \Sigma^{(s)}\right\}_{s=S_1+1}^{S_1+S_2}\]</span></p>
<p>Sampling from the joint predictive density is the same as before.</p>
</section>
</section>
</section>
<section id="model-estimation" class="level1">
<h1>4. Model Estimation</h1>
<section id="standard-bayesian-var" class="level2">
<h2 class="anchored" data-anchor-id="standard-bayesian-var">4.1 Standard Bayesian VAR</h2>
<section id="model-building-code-and-validation" class="level3">
<h3 class="anchored" data-anchor-id="model-building-code-and-validation">4.1.1 Model Building Code and Validation</h3>
<p>We verify that our model can replicate the true parameter of the data generate process by: 1. Generate artificial data containing 1000 observations simulated from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2. That is,</p>
<p><span class="math display">\[\mathbf{y_t} = \begin{pmatrix} y_t,1 \\ y_t,2\end{pmatrix} = \mathbf{y_{t-1}} + \mathbf{\epsilon_t} = \begin{pmatrix}y_{t-1,1}\\y_{t-1, 2}\end{pmatrix} + \begin{pmatrix}\epsilon_{t,1}\\ \epsilon_{t, 2}\end{pmatrix}\]</span> and</p>
<p><span class="math display">\[\mathbf{\epsilon} \sim iid \mathcal{N}(\mathbf{0}, \mathbf{I_2} )\]</span></p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="1152"></p>
</figure>
</div>
</div>
</div>
<p>Then we estimate a model with a constant term and 1 lag using the simulated data, show that the posterior mean of the autoregressive and the covariance matrices are close to an identity matrix and that the posterior mean of the constant term is close to a vector of zeros.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior mean of autoregressive parameter:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>       [,1]  [,2]
[1,]  0.068 0.165
[2,]  1.000 0.001
[3,] -0.003 0.995</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior standard deviation of autoregressive parameter:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>      [,1]  [,2]
[1,] 0.074 0.071
[2,] 0.003 0.002
[3,] 0.002 0.002</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior mean of covariance matrix:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>      [,1]  [,2]
[1,] 1.051 0.067
[2,] 0.067 0.949</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior standard deviation of covariance matrix:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>      [,1]  [,2]
[1,] 0.048 0.032
[2,] 0.032 0.042</code></pre>
</div>
</div>
</section>
<section id="empirical-result" class="level3">
<h3 class="anchored" data-anchor-id="empirical-result">4.1.2 Empirical Result</h3>
<p><strong>1. Fitted Model Parameters </strong></p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior mean of autoregressive parameter:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]
 [1,] -0.899  0.116 -1.487 -0.685 -0.092  0.305 -0.003 -0.070  5.525  0.136
 [2,]  0.421  0.049 -0.191 -0.034 -0.004  0.071  0.000 -0.002  0.275 -0.006
 [3,]  0.098  0.315  0.157 -0.061 -0.001  0.066  0.000  0.004  0.251 -0.007
 [4,]  0.067 -0.126  0.639  0.032 -0.007  0.016  0.002  0.004  0.510  0.016
 [5,] -0.010  0.055 -0.145  0.518  0.002 -0.046 -0.004 -0.009  0.012 -0.002
 [6,] -0.006 -0.002 -0.004 -0.007  0.998  0.005  0.000 -0.001  0.150  0.003
 [7,]  0.116 -0.112  0.029  0.025  0.007  0.881 -0.003 -0.009 -0.307 -0.005
 [8,] -0.016  0.033  0.022  0.022 -0.002  0.025  1.001  0.004  0.205  0.000
 [9,]  0.002  0.081 -0.004  0.000  0.003  0.020  0.001  1.001 -0.230 -0.009
[10,]  0.025 -0.003 -0.011 -0.003 -0.001 -0.008  0.000 -0.004  1.180  0.002
[11,] -0.010  0.043  0.023  0.018  0.000 -0.022  0.001  0.006  0.182  0.982
[12,] -0.013 -0.066  0.031  0.020 -0.004  0.005 -0.001  0.003  0.362  0.010
[13,]  0.049 -0.026 -0.041 -0.047 -0.001 -0.032 -0.001 -0.009  0.131  0.003
[14,]  0.021 -0.103  0.001  0.052  0.004 -0.055 -0.003 -0.007 -0.296  0.000
[15,] -0.019  0.083 -0.074  0.002 -0.003 -0.003  0.000 -0.004  0.228  0.005
[16,]  0.003  0.026  0.022  0.015 -0.003  0.010  0.000  0.002  0.347  0.005
[17,] -0.001  0.005 -0.003 -0.001  0.000  0.000  0.000  0.000  0.028  0.001
[18,] -0.050  0.020  0.019 -0.003 -0.004  0.028  0.001  0.006  0.302  0.000
[19,] -0.006  0.008  0.007  0.005 -0.001  0.007  0.000  0.001  0.054  0.000
[20,] -0.003  0.011  0.003  0.004  0.000  0.009  0.000  0.001  0.002 -0.001
[21,] -0.022 -0.011  0.010  0.002  0.003  0.014  0.000  0.004 -0.448 -0.002
[22,] -0.001  0.005  0.010  0.001  0.000 -0.006  0.000  0.001  0.042 -0.008
[23,] -0.006 -0.009  0.006  0.003  0.000  0.001  0.000  0.002  0.052  0.002
[24,] -0.021  0.007  0.017  0.013 -0.001  0.025  0.000  0.001  0.187  0.001
[25,] -0.005  0.027 -0.076 -0.029  0.000 -0.023  0.000 -0.001 -0.155  0.000
[26,] -0.009  0.017  0.046  0.014  0.000  0.004  0.000 -0.001  0.087 -0.002
[27,] -0.010 -0.020  0.010  0.005 -0.001  0.001  0.000 -0.001  0.049  0.002
[28,] -0.001 -0.001 -0.001  0.000  0.000  0.001  0.000  0.000  0.009  0.000
[29,]  0.009  0.047  0.009 -0.002  0.000  0.005  0.000  0.001 -0.060  0.001
[30,] -0.002  0.005  0.002  0.002  0.000  0.003  0.000  0.000  0.016  0.000
[31,]  0.001  0.006  0.002  0.001  0.000  0.002  0.000  0.000 -0.013  0.000
[32,]  0.006  0.016  0.012  0.001 -0.003 -0.014  0.000 -0.001  0.398  0.001
[33,]  0.002  0.004  0.006  0.003  0.000 -0.002  0.000  0.000  0.019 -0.003
[34,] -0.002  0.000  0.001  0.001  0.000  0.000  0.000  0.001  0.016  0.000
[35,]  0.021 -0.037 -0.002 -0.005 -0.001 -0.013  0.000 -0.002  0.070  0.000
[36,] -0.012  0.132 -0.004  0.002  0.000  0.007  0.000  0.000  0.000 -0.001
[37,]  0.013 -0.039  0.020  0.007 -0.001  0.001  0.000 -0.001  0.090  0.000
[38,]  0.008  0.000 -0.004  0.006  0.000  0.004  0.000  0.001 -0.026  0.000
[39,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.009  0.000
[40,] -0.011 -0.007 -0.001  0.006 -0.001  0.019  0.000  0.001  0.051  0.001
[41,] -0.001  0.002  0.001  0.002  0.000  0.002  0.000  0.000  0.012  0.000
[42,] -0.001  0.001  0.000  0.001  0.000  0.003  0.000  0.000  0.007  0.000
[43,]  0.001 -0.021 -0.006  0.000  0.001  0.013  0.000  0.002 -0.147  0.001
[44,]  0.000  0.002  0.002  0.002  0.000 -0.001  0.000  0.000  0.005 -0.002
[45,] -0.001  0.002  0.001  0.001  0.000  0.000  0.000  0.000  0.003  0.000
[46,] -0.014  0.009 -0.004 -0.001  0.000  0.006  0.000  0.001  0.039  0.000
[47,]  0.004 -0.029  0.015 -0.005  0.000  0.003  0.000  0.000  0.069  0.001
[48,]  0.002 -0.016  0.002  0.008 -0.001  0.001  0.000  0.001  0.064  0.001
[49,] -0.003  0.003 -0.002  0.004  0.000 -0.002  0.000  0.001  0.027  0.000
[50,]  0.000 -0.001  0.000  0.000  0.000  0.000  0.000  0.000  0.006  0.000
[51,]  0.006 -0.010  0.000  0.001  0.000 -0.008  0.000 -0.001 -0.018  0.000
[52,] -0.001  0.001  0.001  0.001  0.000  0.001  0.000  0.000  0.005  0.000
[53,]  0.000  0.000  0.000  0.000  0.000  0.001  0.000  0.000  0.001  0.000
[54,] -0.004  0.007 -0.002  0.001  0.001 -0.006  0.000 -0.001 -0.097 -0.001
[55,]  0.000  0.001  0.002  0.001  0.000 -0.001  0.000  0.000 -0.003 -0.002
[56,]  0.000  0.001  0.001  0.001  0.000  0.000  0.000  0.000  0.002  0.000
       [,11]
 [1,]  0.154
 [2,]  0.016
 [3,] -0.003
 [4,]  0.004
 [5,]  0.009
 [6,]  0.002
 [7,] -0.004
 [8,] -0.002
 [9,] -0.001
[10,]  0.003
[11,] -0.009
[12,]  0.996
[13,]  0.007
[14,]  0.001
[15,]  0.007
[16,]  0.000
[17,]  0.001
[18,] -0.002
[19,] -0.001
[20,]  0.001
[21,] -0.003
[22,] -0.002
[23,] -0.002
[24,]  0.001
[25,]  0.002
[26,]  0.002
[27,]  0.001
[28,]  0.000
[29,] -0.001
[30,]  0.000
[31,]  0.000
[32,]  0.000
[33,] -0.001
[34,] -0.001
[35,]  0.001
[36,] -0.001
[37,]  0.000
[38,] -0.002
[39,]  0.000
[40,]  0.000
[41,]  0.000
[42,]  0.000
[43,]  0.000
[44,]  0.000
[45,] -0.001
[46,]  0.000
[47,]  0.000
[48,] -0.001
[49,] -0.001
[50,]  0.000
[51,]  0.000
[52,]  0.000
[53,]  0.000
[54,]  0.001
[55,]  0.000
[56,]  0.000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior standard deviation of autoregressive parameter:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]   [,9] [,10] [,11]
 [1,] 2.914 4.910 3.068 3.203 0.507 1.487 0.467 0.541 22.079 0.668 0.548
 [2,] 0.086 0.145 0.091 0.093 0.015 0.044 0.014 0.016  0.651 0.020 0.016
 [3,] 0.052 0.087 0.054 0.057 0.009 0.026 0.008 0.010  0.393 0.012 0.010
 [4,] 0.077 0.130 0.082 0.084 0.014 0.039 0.012 0.014  0.585 0.018 0.014
 [5,] 0.079 0.133 0.083 0.085 0.014 0.040 0.013 0.015  0.601 0.018 0.015
 [6,] 0.115 0.193 0.122 0.126 0.020 0.059 0.018 0.021  0.873 0.026 0.022
 [7,] 0.089 0.149 0.093 0.096 0.015 0.045 0.014 0.016  0.673 0.020 0.017
 [8,] 0.114 0.193 0.121 0.125 0.020 0.058 0.018 0.021  0.864 0.026 0.021
 [9,] 0.111 0.186 0.116 0.121 0.019 0.056 0.018 0.020  0.842 0.025 0.021
[10,] 0.012 0.020 0.013 0.013 0.002 0.006 0.002 0.002  0.091 0.003 0.002
[11,] 0.099 0.167 0.103 0.108 0.017 0.050 0.016 0.018  0.751 0.023 0.018
[12,] 0.115 0.192 0.120 0.124 0.020 0.058 0.018 0.021  0.871 0.026 0.021
[13,] 0.053 0.090 0.056 0.058 0.009 0.027 0.008 0.010  0.407 0.012 0.010
[14,] 0.040 0.068 0.042 0.044 0.007 0.020 0.006 0.007  0.306 0.009 0.007
[15,] 0.051 0.086 0.054 0.055 0.009 0.026 0.008 0.009  0.387 0.012 0.009
[16,] 0.051 0.086 0.054 0.056 0.009 0.026 0.008 0.009  0.392 0.012 0.010
[17,] 0.059 0.100 0.062 0.064 0.010 0.030 0.009 0.011  0.448 0.014 0.011
[18,] 0.054 0.092 0.057 0.059 0.009 0.027 0.009 0.010  0.413 0.012 0.010
[19,] 0.059 0.099 0.062 0.064 0.010 0.030 0.009 0.011  0.449 0.014 0.011
[20,] 0.059 0.099 0.061 0.064 0.010 0.030 0.009 0.011  0.444 0.013 0.011
[21,] 0.018 0.030 0.018 0.019 0.003 0.009 0.003 0.003  0.133 0.004 0.003
[22,] 0.057 0.096 0.060 0.062 0.010 0.029 0.009 0.010  0.435 0.013 0.011
[23,] 0.059 0.099 0.062 0.064 0.010 0.030 0.009 0.011  0.448 0.014 0.011
[24,] 0.038 0.064 0.040 0.041 0.007 0.019 0.006 0.007  0.287 0.009 0.007
[25,] 0.031 0.053 0.033 0.034 0.006 0.016 0.005 0.006  0.238 0.007 0.006
[26,] 0.037 0.062 0.039 0.040 0.006 0.019 0.006 0.007  0.278 0.008 0.007
[27,] 0.037 0.062 0.039 0.040 0.006 0.019 0.006 0.007  0.280 0.008 0.007
[28,] 0.040 0.067 0.042 0.043 0.007 0.020 0.006 0.007  0.299 0.009 0.007
[29,] 0.038 0.064 0.040 0.042 0.007 0.019 0.006 0.007  0.289 0.009 0.007
[30,] 0.040 0.067 0.042 0.043 0.007 0.020 0.006 0.007  0.301 0.009 0.007
[31,] 0.040 0.067 0.041 0.043 0.007 0.020 0.006 0.007  0.299 0.009 0.007
[32,] 0.016 0.028 0.017 0.018 0.003 0.008 0.003 0.003  0.124 0.004 0.003
[33,] 0.039 0.065 0.041 0.042 0.007 0.020 0.006 0.007  0.295 0.009 0.007
[34,] 0.039 0.066 0.041 0.043 0.007 0.020 0.006 0.007  0.299 0.009 0.007
[35,] 0.029 0.049 0.030 0.031 0.005 0.015 0.005 0.005  0.218 0.007 0.005
[36,] 0.026 0.043 0.027 0.028 0.004 0.013 0.004 0.005  0.194 0.006 0.005
[37,] 0.028 0.048 0.030 0.031 0.005 0.014 0.004 0.005  0.214 0.006 0.005
[38,] 0.029 0.049 0.030 0.031 0.005 0.014 0.005 0.005  0.217 0.007 0.005
[39,] 0.030 0.050 0.031 0.032 0.005 0.015 0.005 0.005  0.226 0.007 0.006
[40,] 0.029 0.049 0.031 0.032 0.005 0.015 0.005 0.005  0.220 0.007 0.005
[41,] 0.030 0.050 0.031 0.032 0.005 0.015 0.005 0.005  0.227 0.007 0.006
[42,] 0.030 0.050 0.031 0.032 0.005 0.015 0.005 0.005  0.226 0.007 0.006
[43,] 0.015 0.026 0.016 0.017 0.003 0.008 0.002 0.003  0.116 0.004 0.003
[44,] 0.029 0.050 0.031 0.032 0.005 0.015 0.005 0.005  0.223 0.007 0.005
[45,] 0.030 0.051 0.031 0.032 0.005 0.015 0.005 0.005  0.226 0.007 0.006
[46,] 0.023 0.039 0.025 0.025 0.004 0.012 0.004 0.004  0.178 0.005 0.004
[47,] 0.022 0.037 0.023 0.024 0.004 0.011 0.003 0.004  0.166 0.005 0.004
[48,] 0.023 0.039 0.024 0.025 0.004 0.012 0.004 0.004  0.176 0.005 0.004
[49,] 0.023 0.039 0.024 0.025 0.004 0.012 0.004 0.004  0.176 0.005 0.004
[50,] 0.024 0.040 0.025 0.026 0.004 0.012 0.004 0.004  0.181 0.005 0.004
[51,] 0.024 0.040 0.025 0.026 0.004 0.012 0.004 0.004  0.178 0.005 0.004
[52,] 0.024 0.040 0.025 0.026 0.004 0.012 0.004 0.004  0.181 0.005 0.004
[53,] 0.024 0.040 0.025 0.026 0.004 0.012 0.004 0.004  0.181 0.005 0.004
[54,] 0.011 0.018 0.011 0.012 0.002 0.005 0.002 0.002  0.081 0.002 0.002
[55,] 0.024 0.040 0.025 0.026 0.004 0.012 0.004 0.004  0.179 0.005 0.004
[56,] 0.024 0.040 0.025 0.026 0.004 0.012 0.004 0.004  0.181 0.005 0.004</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior mean of covariance matrix:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]
 [1,]  0.355 -0.051  0.086  0.038  0.001 -0.058 0.000  0.001 -0.039  0.005
 [2,] -0.051  1.011  0.031  0.093  0.008  0.021 0.004  0.012 -0.647 -0.007
 [3,]  0.086  0.031  0.393  0.169 -0.001 -0.002 0.003  0.012  0.433 -0.010
 [4,]  0.038  0.093  0.169  0.421  0.000  0.016 0.003  0.012  0.178 -0.005
 [5,]  0.001  0.008 -0.001  0.000  0.011 -0.003 0.000  0.000 -0.189 -0.002
 [6,] -0.058  0.021 -0.002  0.016 -0.003  0.092 0.002  0.007  0.208  0.000
 [7,]  0.000  0.004  0.003  0.003  0.000  0.002 0.009  0.000  0.012  0.000
 [8,]  0.001  0.012  0.012  0.012  0.000  0.007 0.000  0.012  0.004 -0.001
 [9,] -0.039 -0.647  0.433  0.178 -0.189  0.208 0.012  0.004 20.485  0.134
[10,]  0.005 -0.007 -0.010 -0.005 -0.002  0.000 0.000 -0.001  0.134  0.019
[11,] -0.008 -0.012 -0.014 -0.009  0.000  0.000 0.000 -0.003  0.045  0.001
       [,11]
 [1,] -0.008
 [2,] -0.012
 [3,] -0.014
 [4,] -0.009
 [5,]  0.000
 [6,]  0.000
 [7,]  0.000
 [8,] -0.003
 [9,]  0.045
[10,]  0.001
[11,]  0.012</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior standard deviation of covariance matrix:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]
 [1,] 0.048 0.056 0.036 0.037 0.006 0.018 0.005 0.006 0.255 0.008 0.006
 [2,] 0.056 0.134 0.060 0.062 0.010 0.029 0.009 0.010 0.433 0.013 0.011
 [3,] 0.036 0.060 0.052 0.042 0.006 0.018 0.006 0.007 0.270 0.008 0.007
 [4,] 0.037 0.062 0.042 0.056 0.006 0.018 0.006 0.007 0.277 0.008 0.007
 [5,] 0.006 0.010 0.006 0.006 0.001 0.003 0.001 0.001 0.048 0.001 0.001
 [6,] 0.018 0.029 0.018 0.018 0.003 0.012 0.003 0.003 0.130 0.004 0.003
 [7,] 0.005 0.009 0.006 0.006 0.001 0.003 0.001 0.001 0.040 0.001 0.001
 [8,] 0.006 0.010 0.007 0.007 0.001 0.003 0.001 0.002 0.047 0.001 0.001
 [9,] 0.255 0.433 0.270 0.277 0.048 0.130 0.040 0.047 2.746 0.059 0.048
[10,] 0.008 0.013 0.008 0.008 0.001 0.004 0.001 0.001 0.059 0.002 0.001
[11,] 0.006 0.011 0.007 0.007 0.001 0.003 0.001 0.001 0.048 0.001 0.002</code></pre>
</div>
</div>
<p><strong>2. Prediction Plots</strong></p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Running prediction for 50000 iterations took: 131.527000 seconds"</code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="1152"></p>
</figure>
</div>
</div>
</div>
<p><strong>3. Test for Granger Causality</strong> We first perform Granger Causality test for each country, we test whether the macroeconomic factor of one country has some predictive power on the macroeconomic factors of another. Specifically, we test the following hypothesis:</p>
<ol type="1">
<li>$H_0: $ The consumer price index, interest rate, foreign direct investment, balance of payments, gross domestic product, exchange rate against the dollar of country A has no impact on these variables in the country B$</li>
</ol>
<p>The Baye‚Äôs factor in this case is defined as:</p>
<p><span class="math display">\[B_H = \frac{p_0(Y)}{p_1(Y)} = \frac{p_1(A_{ij} = 0|Y)}{p_1(A_{ij} = 0)}=\frac{\int p_1(A_{ij} = 0|Y, X,\Sigma)d \Sigma}{\int p_1(A_{ij}=0, \Sigma)d\Sigma}\]</span> <span class="math display">\[p_1(A_{ij} = 0|Y, X,\Sigma) \sim \mathcal{MN}_{K \times N}(\bar{A_{ij}}, \Sigma_{ij}, \bar{V}_{ij})\]</span> <span class="math display">\[p_1(A_{ij} = 0, \Sigma) \sim \mathcal{NIW}_{K \times N}(A_{0, ij}, V_{A, ij}, S_{0, ij}, \nu_{0, ij})\]</span> <span class="math display">\[p_1(A_{ij} = 0, \Sigma) = p(A_{ij}|\Sigma)\times p(\Sigma)\]</span> <span class="math display">\[A_{ij}|\Sigma \sim \mathcal{MN}_{K \times N} (A_{0,ij}, \Sigma, V_{A, ij}) \]</span> <span class="math display">\[\Sigma \sim \mathcal{IW}(S_0, \nu_0)\]</span> and we test the bi-lateral relationship between each of these countries.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Running Bayes factor calculation for 500 iterations took: 671.543000 seconds"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>             CN           US           JP          AU
CN -318685.2027     1070.657     1051.494    1112.069
US    1079.5498 -1618774.333     1348.506    1358.446
JP     932.3779     1159.568 -1602272.595    1284.957
AU    1150.6094     1301.027     1326.103 -945759.463</code></pre>
</div>
</div>
<table class="table">
<colgroup>
<col style="width: 13%">
<col style="width: 16%">
<col style="width: 20%">
<col style="width: 21%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>China</th>
<th>United States</th>
<th>Japan</th>
<th>Australia</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>China</strong></td>
<td>Strong Evidence</td>
<td>No Evidence</td>
<td>No Evidence</td>
<td>No Evidence</td>
</tr>
<tr class="even">
<td><strong>US</strong></td>
<td>No Evidence</td>
<td>Strong Evidence</td>
<td>No Evidence</td>
<td>No Evidence</td>
</tr>
<tr class="odd">
<td><strong>Japan</strong></td>
<td>No Evidence</td>
<td>No Evidence</td>
<td>Strong Evidence</td>
<td>No Evidence</td>
</tr>
<tr class="even">
<td><strong>Australia</strong></td>
<td>No Evidence</td>
<td>No Evidence</td>
<td>No Evidence</td>
<td>Strong Evidence</td>
</tr>
</tbody>
</table>
<p>Testing our null hypothesis using the Savage-Dickey ratio indicates that with p=5 lags, no country Granger-causes the economic indicators of another country. However, the lags of each country do Granger-cause their own economic indicators. We will test p = 4 lags next.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Running Bayes factor calculation for 500 iterations took: 352.230000 seconds"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>             CN           US           JP          AU
CN -272960.5910     1042.926     1056.942    1099.940
US    1188.0282 -1638597.076     1458.310    1478.405
JP     918.1063     1287.356 -1616055.722    1359.913
AU    1316.9405     1333.768     1459.371 -966899.803</code></pre>
</div>
</div>
<table class="table">
<colgroup>
<col style="width: 13%">
<col style="width: 16%">
<col style="width: 20%">
<col style="width: 21%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>China</th>
<th>United States</th>
<th>Japan</th>
<th>Australia</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>China</strong></td>
<td>Strong Evidence</td>
<td>No Evidence</td>
<td>No Evidence</td>
<td>No Evidence</td>
</tr>
<tr class="even">
<td><strong>US</strong></td>
<td>No Evidence</td>
<td>Strong Evidence</td>
<td>No Evidence</td>
<td>No Evidence</td>
</tr>
<tr class="odd">
<td><strong>Japan</strong></td>
<td>No Evidence</td>
<td>No Evidence</td>
<td>Strong Evidence</td>
<td>No Evidence</td>
</tr>
<tr class="even">
<td><strong>Australia</strong></td>
<td>No Evidence</td>
<td>No Evidence</td>
<td>No Evidence</td>
<td>Strong Evidence</td>
</tr>
</tbody>
</table>
<p>Testing our null hypothesis using the Savage-Dickey ratio indicates that with p=4 lags, no country Granger-causes the economic indicators of another country. However, the lags of each country do Granger-cause their own economic indicators. We will test the model for pre-covid and post_covid period next.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Running Bayes factor calculation for 500 iterations took: 671.645000 seconds"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>             CN           US           JP          AU
CN -314680.5342     1057.100     1049.716    1110.877
US    1052.6074 -1619446.107     1365.653    1418.698
JP     950.7971     1143.669 -1609776.548    1277.014
AU    1197.0614     1295.412     1365.178 -944295.423</code></pre>
</div>
</div>
<table class="table">
<colgroup>
<col style="width: 13%">
<col style="width: 16%">
<col style="width: 20%">
<col style="width: 21%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>China</th>
<th>United States</th>
<th>Japan</th>
<th>Australia</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>China</strong></td>
<td>Strong Evidence</td>
<td>No Evidence</td>
<td>No Evidence</td>
<td>No Evidence</td>
</tr>
<tr class="even">
<td><strong>US</strong></td>
<td>No Evidence</td>
<td>Strong Evidence</td>
<td>No Evidence</td>
<td>No Evidence</td>
</tr>
<tr class="odd">
<td><strong>Japan</strong></td>
<td>No Evidence</td>
<td>No Evidence</td>
<td>Strong Evidence</td>
<td>No Evidence</td>
</tr>
<tr class="even">
<td><strong>Australia</strong></td>
<td>No Evidence</td>
<td>No Evidence</td>
<td>No Evidence</td>
<td>Strong Evidence</td>
</tr>
</tbody>
</table>
<p>Testing our null hypothesis using the Savage-Dickey ratio indicates that with p=4 lags, no country Granger-causes the economic indicators of another country in the pre-Covid period from 1994 Q1 to 2019 Q4. Next, we test the hypothesis on data from Q1 2004, two years after China joined the World Trade Organization.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Running Bayes factor calculation for 500 iterations took: 672.771000 seconds"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>             CN           US           JP          AU
CN -319453.0737     1113.804     1047.330    1094.916
US    1095.5090 -1613266.393     1357.081    1372.285
JP     945.0979     1137.518 -1602505.247    1253.436
AU    1180.1254     1271.541     1346.508 -961943.542</code></pre>
</div>
</div>
</section>
</section>
<section id="large-bvar-model-with-ma1-gaussian-innovations" class="level2">
<h2 class="anchored" data-anchor-id="large-bvar-model-with-ma1-gaussian-innovations">4.2 Large BVAR model with MA(1) Gaussian Innovations</h2>
<section id="model-building-code-and-validation-1" class="level3">
<h3 class="anchored" data-anchor-id="model-building-code-and-validation-1">4.2.1 Model Building Code and Validation</h3>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "acceptance rate of the Metropolis Hasting Step: "
[1] 0.1785</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Fit MA BVAR on 1000 bi-variate random walk for 2000 iterations took: 3887.012000 seconds"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior mean of autoregressive parameter:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>       [,1]  [,2]
[1,]  0.068 0.175
[2,]  0.999 0.002
[3,] -0.003 0.995</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior standard deviation of autoregressive parameter:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>      [,1]  [,2]
[1,] 0.073 0.074
[2,] 0.003 0.003
[3,] 0.002 0.002</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior mean of row specific covariance matrix:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>      [,1]  [,2]
[1,] 1.150 0.063
[2,] 0.063 1.048</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior standard deviation of row specific covariance matrix:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>      [,1]  [,2]
[1,] 0.053 0.035
[2,] 0.035 0.045</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior mean of column specific covariance matrix (first 10 rows and columns):"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]
 [1,]  1.001 -0.028  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000
 [2,] -0.028  1.001 -0.028  0.000  0.000  0.000  0.000  0.000  0.000  0.000
 [3,]  0.000 -0.028  1.001 -0.028  0.000  0.000  0.000  0.000  0.000  0.000
 [4,]  0.000  0.000 -0.028  1.001 -0.028  0.000  0.000  0.000  0.000  0.000
 [5,]  0.000  0.000  0.000 -0.028  1.001 -0.028  0.000  0.000  0.000  0.000
 [6,]  0.000  0.000  0.000  0.000 -0.028  1.001 -0.028  0.000  0.000  0.000
 [7,]  0.000  0.000  0.000  0.000  0.000 -0.028  1.001 -0.028  0.000  0.000
 [8,]  0.000  0.000  0.000  0.000  0.000  0.000 -0.028  1.001 -0.028  0.000
 [9,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.028  1.001 -0.028
[10,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.028  1.001</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]
 [1,] 0.001 0.022 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
 [2,] 0.022 0.001 0.022 0.000 0.000 0.000 0.000 0.000 0.000 0.000
 [3,] 0.000 0.022 0.001 0.022 0.000 0.000 0.000 0.000 0.000 0.000
 [4,] 0.000 0.000 0.022 0.001 0.022 0.000 0.000 0.000 0.000 0.000
 [5,] 0.000 0.000 0.000 0.022 0.001 0.022 0.000 0.000 0.000 0.000
 [6,] 0.000 0.000 0.000 0.000 0.022 0.001 0.022 0.000 0.000 0.000
 [7,] 0.000 0.000 0.000 0.000 0.000 0.022 0.001 0.022 0.000 0.000
 [8,] 0.000 0.000 0.000 0.000 0.000 0.000 0.022 0.001 0.022 0.000
 [9,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.022 0.001 0.022
[10,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.022 0.001</code></pre>
</div>
</div>
</section>
<section id="empirical-results" class="level3">
<h3 class="anchored" data-anchor-id="empirical-results">4.2.1 Empirical Results</h3>
<p><strong>1. Fitted Model Parameters</strong></p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "acceptance rate of the Metropolis Hasting Step: "
[1] 0.2496</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Fit MA BVAR on data for 10000 iterations took: 3887.012000 seconds"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior mean of autoregressive parameter:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]
 [1,]  0.104 -0.248  0.043 -0.127 -0.059  0.075 0.003  0.023  5.413 -0.204
 [2,]  0.992  0.000 -0.008 -0.002  0.000  0.000 0.000  0.000  0.020  0.000
 [3,]  0.008  0.964  0.008 -0.005  0.000  0.003 0.000  0.000  0.020 -0.001
 [4,]  0.002 -0.006  0.988 -0.001  0.000 -0.001 0.000  0.000  0.039  0.000
 [5,]  0.001 -0.002 -0.006  0.990  0.000 -0.001 0.000  0.000  0.034  0.000
 [6,]  0.000  0.001  0.000  0.000  1.000  0.000 0.000  0.000  0.001  0.000
 [7,]  0.002  0.001  0.000  0.002  0.000  0.998 0.000  0.000 -0.004  0.002
 [8,]  0.000  0.001  0.000  0.001  0.000  0.000 1.000  0.000  0.003  0.000
 [9,]  0.000  0.002  0.000  0.001  0.000  0.000 0.000  1.000 -0.004  0.000
[10,]  0.003 -0.001 -0.006 -0.002 -0.001 -0.001 0.000 -0.001  1.146  0.002
[11,]  0.000 -0.003  0.000 -0.001  0.000  0.000 0.000  0.001  0.005  0.998
[12,]  0.000  0.000  0.001  0.000  0.000  0.000 0.000  0.000  0.004  0.000
[13,]  0.000  0.000 -0.001 -0.001  0.000  0.000 0.000  0.000  0.007  0.000
[14,]  0.001 -0.007 -0.002 -0.001  0.000 -0.001 0.000  0.000  0.000  0.000
[15,] -0.001  0.002 -0.003 -0.001  0.000  0.000 0.000  0.000  0.009  0.000
[16,]  0.000  0.000 -0.001 -0.001  0.000  0.000 0.000  0.000  0.009  0.000
[17,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000
[18,] -0.001  0.002  0.000  0.001  0.000  0.000 0.000  0.000  0.002  0.000
[19,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000
[20,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[21,] -0.003  0.002  0.003  0.001  0.001  0.001 0.000  0.001 -0.092  0.000
[22,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.001 -0.001
[23,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000
[24,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.004  0.000
[25,] -0.001  0.000 -0.002 -0.001  0.000 -0.001 0.000  0.000  0.000  0.000
[26,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.005  0.000
[27,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.003  0.000
[28,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[29,]  0.000  0.001  0.000  0.000  0.000  0.000 0.000  0.000 -0.001  0.000
[30,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[31,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[32,]  0.000  0.000  0.003  0.001  0.000  0.000 0.000  0.000 -0.037  0.000
[33,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[34,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000
[35,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000
[36,]  0.000  0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.003  0.000
[37,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.003  0.000
[38,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000
[39,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000
[40,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000
[41,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[42,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[43,]  0.000 -0.001  0.000  0.001  0.000  0.000 0.000  0.000 -0.034  0.000
[44,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[45,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[46,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[47,]  0.000 -0.002  0.001  0.000  0.000  0.000 0.000  0.000  0.004  0.000
[48,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000
[49,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000
[50,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000
[51,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[52,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[53,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[54,]  0.000  0.001  0.000  0.001  0.000  0.000 0.000  0.000 -0.030  0.000
[55,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
[56,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000
       [,11]
 [1,] -0.012
 [2,]  0.001
 [3,]  0.000
 [4,]  0.000
 [5,]  0.000
 [6,]  0.000
 [7,]  0.000
 [8,]  0.000
 [9,]  0.000
[10,]  0.001
[11,] -0.001
[12,]  1.000
[13,]  0.000
[14,]  0.000
[15,]  0.000
[16,]  0.000
[17,]  0.000
[18,]  0.000
[19,]  0.000
[20,]  0.000
[21,] -0.001
[22,]  0.000
[23,]  0.000
[24,]  0.000
[25,]  0.000
[26,]  0.000
[27,]  0.000
[28,]  0.000
[29,]  0.000
[30,]  0.000
[31,]  0.000
[32,]  0.000
[33,]  0.000
[34,]  0.000
[35,]  0.000
[36,]  0.000
[37,]  0.000
[38,]  0.000
[39,]  0.000
[40,]  0.000
[41,]  0.000
[42,]  0.000
[43,]  0.000
[44,]  0.000
[45,]  0.000
[46,]  0.000
[47,]  0.000
[48,]  0.000
[49,]  0.000
[50,]  0.000
[51,]  0.000
[52,]  0.000
[53,]  0.000
[54,]  0.000
[55,]  0.000
[56,]  0.000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior standard deviation of autoregressive parameter:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]
 [1,] 0.563 1.211 0.651 0.609 0.082 0.266 0.080 0.083 4.126 0.120 0.097
 [2,] 0.014 0.029 0.016 0.015 0.002 0.006 0.002 0.002 0.099 0.003 0.002
 [3,] 0.013 0.028 0.015 0.014 0.002 0.006 0.002 0.002 0.096 0.003 0.002
 [4,] 0.014 0.028 0.015 0.014 0.002 0.006 0.002 0.002 0.098 0.003 0.002
 [5,] 0.014 0.029 0.016 0.015 0.002 0.006 0.002 0.002 0.099 0.003 0.002
 [6,] 0.014 0.028 0.015 0.015 0.002 0.006 0.002 0.002 0.101 0.003 0.002
 [7,] 0.013 0.028 0.015 0.014 0.002 0.006 0.002 0.002 0.095 0.003 0.002
 [8,] 0.014 0.029 0.015 0.015 0.002 0.006 0.002 0.002 0.100 0.003 0.002
 [9,] 0.014 0.028 0.015 0.014 0.002 0.006 0.002 0.002 0.098 0.003 0.002
[10,] 0.006 0.012 0.007 0.006 0.001 0.003 0.001 0.001 0.047 0.001 0.001
[11,] 0.013 0.028 0.015 0.014 0.002 0.006 0.002 0.002 0.098 0.003 0.002
[12,] 0.014 0.029 0.015 0.015 0.002 0.007 0.002 0.002 0.100 0.003 0.002
[13,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001
[14,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001
[15,] 0.007 0.015 0.008 0.007 0.001 0.003 0.001 0.001 0.051 0.001 0.001
[16,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001
[17,] 0.007 0.015 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001
[18,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001
[19,] 0.007 0.015 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001
[20,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001
[21,] 0.006 0.012 0.007 0.006 0.001 0.003 0.001 0.001 0.044 0.001 0.001
[22,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.051 0.001 0.001
[23,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001
[24,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001
[25,] 0.004 0.009 0.005 0.005 0.001 0.002 0.001 0.001 0.033 0.001 0.001
[26,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001
[27,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001
[28,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001
[29,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.033 0.001 0.001
[30,] 0.005 0.009 0.005 0.005 0.001 0.002 0.001 0.001 0.033 0.001 0.001
[31,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001
[32,] 0.004 0.009 0.005 0.004 0.001 0.002 0.001 0.001 0.030 0.001 0.001
[33,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001
[34,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001
[35,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001
[36,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001
[37,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.001 0.025 0.001 0.001
[38,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001
[39,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001
[40,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001
[41,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.001 0.025 0.001 0.001
[42,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.001 0.025 0.001 0.001
[43,] 0.003 0.007 0.004 0.003 0.000 0.001 0.000 0.000 0.023 0.001 0.001
[44,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001
[45,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001
[46,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000
[47,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000
[48,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000
[49,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000
[50,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000
[51,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000
[52,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000
[53,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000
[54,] 0.002 0.005 0.003 0.003 0.000 0.001 0.000 0.000 0.018 0.001 0.000
[55,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000
[56,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior mean of row specific covariance matrix:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]
 [1,]  0.376 -0.159  0.154  0.049  0.001 -0.036 0.001  0.005  0.142  0.003
 [2,] -0.159  1.502 -0.073  0.099  0.006  0.003 0.005  0.017 -0.727 -0.001
 [3,]  0.154 -0.073  0.571  0.192  0.001  0.026 0.004  0.018  0.243 -0.017
 [4,]  0.049  0.099  0.192  0.421  0.001  0.014 0.004  0.012 -0.023 -0.006
 [5,]  0.001  0.006  0.001  0.001  0.011 -0.002 0.000  0.001 -0.263 -0.002
 [6,] -0.036  0.003  0.026  0.014 -0.002  0.086 0.002  0.007  0.133 -0.002
 [7,]  0.001  0.005  0.004  0.004  0.000  0.002 0.009  0.000  0.017  0.000
 [8,]  0.005  0.017  0.018  0.012  0.001  0.007 0.000  0.014 -0.054 -0.002
 [9,]  0.142 -0.727  0.243 -0.023 -0.263  0.133 0.017 -0.054 30.696  0.198
[10,]  0.003 -0.001 -0.017 -0.006 -0.002 -0.002 0.000 -0.002  0.198  0.026
[11,] -0.012 -0.014 -0.024 -0.011 -0.001 -0.002 0.000 -0.005  0.115  0.002
       [,11]
 [1,] -0.012
 [2,] -0.014
 [3,] -0.024
 [4,] -0.011
 [5,] -0.001
 [6,] -0.002
 [7,]  0.000
 [8,] -0.005
 [9,]  0.115
[10,]  0.002
[11,]  0.015</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior standard deviation of row specific covariance matrix:"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]
 [1,] 0.052 0.073 0.046 0.038 0.006 0.017 0.005 0.007 0.320 0.009 0.007
 [2,] 0.073 0.206 0.089 0.076 0.012 0.034 0.011 0.013 0.649 0.019 0.015
 [3,] 0.046 0.089 0.078 0.049 0.008 0.021 0.007 0.009 0.401 0.011 0.009
 [4,] 0.038 0.076 0.049 0.057 0.007 0.018 0.006 0.007 0.339 0.010 0.008
 [5,] 0.006 0.012 0.008 0.007 0.002 0.003 0.001 0.001 0.061 0.002 0.001
 [6,] 0.017 0.034 0.021 0.018 0.003 0.012 0.003 0.003 0.156 0.004 0.003
 [7,] 0.005 0.011 0.007 0.006 0.001 0.003 0.001 0.001 0.049 0.001 0.001
 [8,] 0.007 0.013 0.009 0.007 0.001 0.003 0.001 0.002 0.061 0.002 0.001
 [9,] 0.320 0.649 0.401 0.339 0.061 0.156 0.049 0.061 4.236 0.086 0.066
[10,] 0.009 0.019 0.011 0.010 0.002 0.004 0.001 0.002 0.086 0.004 0.002
[11,] 0.007 0.015 0.009 0.008 0.001 0.003 0.001 0.001 0.066 0.002 0.002</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior mean of column specific covariance matrix (first 10 rows and columns):"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]
 [1,]  1.192 -0.521  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000
 [2,] -0.521  1.230 -0.436  0.000  0.000  0.000  0.000  0.000  0.000  0.000
 [3,]  0.000 -0.436  1.192 -0.436  0.000  0.000  0.000  0.000  0.000  0.000
 [4,]  0.000  0.000 -0.436  1.192 -0.436  0.000  0.000  0.000  0.000  0.000
 [5,]  0.000  0.000  0.000 -0.436  1.192 -0.436  0.000  0.000  0.000  0.000
 [6,]  0.000  0.000  0.000  0.000 -0.436  1.192 -0.436  0.000  0.000  0.000
 [7,]  0.000  0.000  0.000  0.000  0.000 -0.436  1.192 -0.436  0.000  0.000
 [8,]  0.000  0.000  0.000  0.000  0.000  0.000 -0.436  1.192 -0.436  0.000
 [9,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.436  1.192 -0.436
[10,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.436  1.192</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]
 [1,] 0.038 0.070 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
 [2,] 0.070 0.053 0.045 0.000 0.000 0.000 0.000 0.000 0.000 0.000
 [3,] 0.000 0.045 0.038 0.045 0.000 0.000 0.000 0.000 0.000 0.000
 [4,] 0.000 0.000 0.045 0.038 0.045 0.000 0.000 0.000 0.000 0.000
 [5,] 0.000 0.000 0.000 0.045 0.038 0.045 0.000 0.000 0.000 0.000
 [6,] 0.000 0.000 0.000 0.000 0.045 0.038 0.045 0.000 0.000 0.000
 [7,] 0.000 0.000 0.000 0.000 0.000 0.045 0.038 0.045 0.000 0.000
 [8,] 0.000 0.000 0.000 0.000 0.000 0.000 0.045 0.038 0.045 0.000
 [9,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.045 0.038 0.045
[10,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.045 0.038</code></pre>
</div>
</div>
<p><strong>2. Prediction Plots </strong></p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Predict 8 periods ahead MA BVAR on data for 10000 iterations took: 285.658000 seconds"</code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-29-1.png" class="img-fluid figure-img" width="1152"></p>
</figure>
</div>
</div>
</div>
<p><strong>2. Forecasts Model Parameters</strong></p>
<p><strong>3. Test for Granger Causality</strong></p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Running Bayes factor calculation for 500 iterations took: 674.808000 seconds"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>             CN           US           JP           AU
CN -102804.7576     376.2762     327.0783     379.1168
US     375.6391 -528073.5117     352.3693     379.6552
JP     378.5526     378.8469 -522652.2917     374.8028
AU     376.2018     382.5976     323.0241 -302290.8092</code></pre>
</div>
</div>
</section>
</section>
<section id="large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility" class="level2">
<h2 class="anchored" data-anchor-id="large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility">4.4 Large BVAR model with MA(1) Gaussian Innovations and Common Stochastic Volatility</h2>
<section id="model-building-code-and-validation-2" class="level3">
<h3 class="anchored" data-anchor-id="model-building-code-and-validation-2">4.4.1 Model Building Code and Validation</h3>
</section>
<section id="empirical-result-1" class="level3">
<h3 class="anchored" data-anchor-id="empirical-result-1">4.4.2 Empirical Result</h3>
<p>```</p>
<!-- Furthermore, a non-Gaussian framework allows the modeling of asymmetries in how economies react to positive versus negative shocks. Traditional BVAR models with Gaussian assumptions is often unable to capture such nuances, while flexible error structures can incorporate the differing impacts of shocks of different magnitudes or directions. 
economic variables are often characterized by distributions that are far from Gaussian, often featuring fat tails and skewness. Secondly, -->



</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Chan_2015" class="csl-entry" role="listitem">
Chan, Joshua C. C. 2015. <span>‚ÄúLarge Bayesian VARs: A Flexible Kronecker Error Covariance Structure.‚Äù</span> <em>SSRN Electronic Journal</em>.
</div>
<div id="ref-Chib1995" class="csl-entry" role="listitem">
Chib, Siddhartha, and Edward Greenberg. 1995. <span>‚ÄúUnderstanding the Metropolis-Hastings Algorithm.‚Äù</span> <em>The American Statistician</em> 49 (4): 327‚Äì35.
</div>
<div id="ref-Granger_1969" class="csl-entry" role="listitem">
Granger, C. W. J. 1969. <span>‚ÄúInvestigating Causal Relations by Econometric Models and Cross-Spectral Methods.‚Äù</span> <em>Econometrica</em>.
</div>
<div id="ref-wozniakBsvarsBayesianEstimation2022" class="csl-entry" role="listitem">
Wo≈∫niak, Tomasz. 2022. <em>Bsvars: Bayesian Estimation of Structural Vector Autoregressive Models</em>. R Package. <a href="https://cran.r-project.org/package=bsvars">https://cran.r-project.org/package=bsvars</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>